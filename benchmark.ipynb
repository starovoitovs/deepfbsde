{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Consider\n",
    "\n",
    "$$\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "W \\in \\R ^m, N \\in \\R^\\ell\\\\\n",
    "X\\in \\R^d, Y\\in \\R^n, Z\\in \\R^{n\\times m}, R \\in \\R ^{n\\times \\ell}\\\\\n",
    "\\gamma \\in \\R^{d \\times \\ell}, \\sigma \\in \\R^{d\\times m} \\\\\n",
    "f\\in C^2(\\R^d, E\\subset\\R^n)\\\\\n",
    "$$\n",
    "\n",
    "For the control problem with jumps, we would have the FBSDE of the form (assuming zero drift on $X$)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\sigma_t dW_t + \\gamma_t dN_t\\\\\n",
    "dY_t &= (\\dots)dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assume that $Y=f(X)$. Necessarily, by Ito's lemma (Oksendal, p.9, 1.2.8; for k-th column):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_t &= (D f(X_t))^T \\cdot \\sigma_t \\\\\n",
    "R^{(\\cdot k)}_t &= f\\left(X_t+\\gamma ^{(\\cdot k)}_t\\right) - f(X_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assuming that $D f(X) D f(X)^T$ is never singular and $f^{-1}\\in C^2(E, \\R^d)$ exists, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_t &= \\left(D f(X_t) D f(X_t) ^T \\right) ^{-1} D f(X_t) Z =: \\beta(X_t) Z_t\\\\\n",
    "\\gamma^{(\\cdot k)}_t &= f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for $\\beta(X) = \\left(D f(X) D f(X) ^T \\right) ^{-1} D f(X)$. Thus we obtain by Ito's lemma (note that $D^2 f$ is a 3d-tensor indexed by $i,j,k$ with $k\\in 1,\\dots, n$)\n",
    "\n",
    "$$\n",
    "(\\dots)dt = \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt\n",
    "$$\n",
    "\n",
    "Altogether, we can rewrite\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\beta(X_t) Z_t dW_t + \\left(f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(X)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= \\delta_{ij} \\exp(X^{(i)}) =\\operatorname{diag}(\\exp(X))\\\\\n",
    "D^2 f(X)^{(ijk)} &= \\delta_{ijk} \\exp(X^{(i)})\\\\\n",
    "\\beta(X) &= \\delta_{ij}\\exp(-X^{(i)}) = \\operatorname{diag}(\\exp(-X))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\operatorname{diag}(\\exp(-X)) Z_t dW_t + \\left(\\log\\left(\\exp(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\operatorname{diag} ( \\operatorname{diag}(\\exp(-X)) Z_t( \\operatorname{diag}(\\exp(-X)) Z_t)^T) \\exp(X_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(X_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1a\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(aX)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\frac 1a \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= a\\delta_{ij} \\exp(aX^{(i)}) = a\\operatorname{diag}(\\exp(aX))\\\\\n",
    "D^2 f(X)^{(ijk)} &= a^2 \\delta_{ijk} \\exp(aX^{(i)})\\\\\n",
    "\\beta(X) &= \\frac 1a \\delta_{ij}\\exp(-aX^{(i)}) = \\frac 1a \\operatorname{diag}(\\exp(-aX))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\frac 1a \\operatorname{diag}(\\exp(-aX)) Z_t dW_t + \\left(\\frac 1a \\log\\left(\\exp(aX_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac {1}{2} \\operatorname{diag} ( \\operatorname{diag}(\\exp(-aX)) Z_t( \\operatorname{diag}(\\exp(-aX)) Z_t)^T) \\exp(aX_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(X_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/h5py/__init__.py:39: UserWarning: h5py is running against HDF5 1.10.8 when it was built against 1.10.7, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer, BatchNormalization, Add\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.metrics import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gpu_utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(pick_gpu_lowest_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=11, linewidth=90, formatter=dict(float=lambda x: \"%7.5g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{timestamp}\"\n",
    "tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "output_dir = f\"_output/models/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_paths = 2 ** 8\n",
    "n_timesteps = 16\n",
    "time_horizon = 1.\n",
    "n_x_dimensions = 2\n",
    "n_y_dimensions = 2\n",
    "n_diffusion_factors = 2\n",
    "n_jump_factors = 2\n",
    "intensity = 1.\n",
    "alpha = 2.\n",
    "stddev = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = time_horizon / n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    return tf.zeros((n_x_dimensions,))\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z) / alpha\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    # floor the log argument\n",
    "    res = tf.maximum(tf.exp(alpha * x) + r, 1e-2)\n",
    "    return (tf.math.log(res) / alpha - x)\n",
    "\n",
    "def f(t, x, y, z, r):\n",
    "    res = tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z)\n",
    "    return 0.5 * tf.einsum('ij,j', tf.linalg.matmul(res, res, transpose_b=True), tf.exp(alpha * x))\n",
    "\n",
    "def g(x):\n",
    "    return tf.exp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layers and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Y0Callback(Callback):\n",
    "    \n",
    "    def __init__(self, filepath=None):\n",
    "        super(Y0Callback, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.y0s = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y0 = self.model.get_layer('y_0').y0.numpy()\n",
    "        self.y0s += [y0[0]]\n",
    "        print(f\"{y0}\\n\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.filepath is not None:\n",
    "            pd.DataFrame(self.y0s).to_csv(self.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSave(Callback):\n",
    "    def __init__(self, directory):\n",
    "        self.batch = 1\n",
    "        self.epoch = 1\n",
    "        self.directory = directory\n",
    "        shutil.rmtree(directory, ignore_errors=True)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        self.epoch += 1\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        filename = os.path.join(self.directory, f\"weights_{self.epoch:03}_{self.batch:03}.h5\")\n",
    "        self.model.save_weights(filename)\n",
    "        self.batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_x_dimensions, n_y_dimensions, n_diffusion_factors, n_jump_factors, n_timesteps, time_horizon):\n",
    "\n",
    "    dt = time_horizon / n_timesteps\n",
    "\n",
    "    def dX(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', s(t, x, y, z, r), dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', v(t, x, y, z, r), dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    def dY(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', z, dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', r, dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))        \n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    @tf.function\n",
    "    def hx(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return x + dX(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    @tf.function\n",
    "    def hy(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return y + dY(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    n_hidden_units = n_x_dimensions + n_diffusion_factors + n_jump_factors + 10\n",
    "\n",
    "    inputs_x0 = Input(shape=(n_x_dimensions))\n",
    "    inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "    inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "\n",
    "    # constant x0\n",
    "\n",
    "    x0 = tf.Variable([[1. for _ in range(n_x_dimensions)]], trainable=False)\n",
    "    y0 = tf.Variable([[0. for _ in range(n_y_dimensions)]], trainable=True)\n",
    "\n",
    "    x = InitialValue(x0, trainable=False, name='x_0')(inputs_dW)\n",
    "    y = InitialValue(y0, trainable=True, name='y_0')(inputs_dW)\n",
    "\n",
    "    # adjoints\n",
    "\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z1_0')(z)\n",
    "    z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z2_0')(z)\n",
    "    z = BatchNormalization(name='zbn_0')(z)\n",
    "    z = Reshape((n_y_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r1_0')(r)\n",
    "    r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r2_0')(r)\n",
    "    r = BatchNormalization(name='rbn_0')(r)\n",
    "    r = Reshape((n_y_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "\n",
    "    # pre-compile lambda layers\n",
    "    \n",
    "    for i in range(n_timesteps):\n",
    "\n",
    "        step = InitialValue(tf.Variable(i, dtype=tf.float32, trainable=False))(inputs_dW)\n",
    "\n",
    "        dW = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dW, step])\n",
    "        dN = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dN, step])\n",
    "\n",
    "        x, y = (\n",
    "            Lambda(hx, name=f'x_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "            Lambda(hy, name=f'y_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "        )\n",
    "\n",
    "        # we don't train z for the last time step; keep for consistency\n",
    "        z = concatenate([x, y])\n",
    "        z = Dense(n_hidden_units, activation='relu', name=f'z1_{i+1}')(z)\n",
    "        z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "        z = Reshape((n_y_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "        z = BatchNormalization(name=f'zbn_{i+1}')(z)\n",
    "\n",
    "        # we don't train r for the last time step; keep for consistency\n",
    "        r = concatenate([x, y])\n",
    "        r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r1_{i+1}')(r)\n",
    "        r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r2_{i+1}')(r)\n",
    "        r = Reshape((n_y_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "        r = BatchNormalization(name=f'rbn_{i+1}')(r)\n",
    "\n",
    "        paths += [[x, y, z, r]]\n",
    "\n",
    "    outputs_loss = Lambda(lambda r: r[1] - tf.vectorized_map(g, r[0]))([x, y])\n",
    "    \n",
    "    # remember that z and r are matrices\n",
    "    outputs_paths = tf.stack(\n",
    "        [tf.stack([p[0] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "        [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "    model_loss = Model([inputs_x0, inputs_dW, inputs_dN], outputs_loss)\n",
    "\n",
    "    # (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "    # skips the first time step\n",
    "    model_paths = Model([inputs_x0, inputs_dW, inputs_dN], outputs_paths)\n",
    "\n",
    "    return model_loss, model_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "dt = time_horizon / n_timesteps\n",
    "model_loss, model_paths = build_model(n_x_dimensions=n_x_dimensions,\n",
    "                                      n_y_dimensions=n_y_dimensions,\n",
    "                                      n_diffusion_factors=n_diffusion_factors,\n",
    "                                      n_jump_factors=n_jump_factors,\n",
    "                                      n_timesteps=n_timesteps,\n",
    "                                      time_horizon=time_horizon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant(np.full((n_paths, n_x_dimensions), 1.), dtype=tf.float32)\n",
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([intensity for _ in range(n_jump_factors)])))\n",
    "target = tf.zeros((n_paths, n_y_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "callbacks = []\n",
    "\n",
    "callbacks += [Y0Callback(os.path.join(output_dir, \"y0.csv\"))]\n",
    "# callbacks += [BatchSave(os.path.join(output_dir, \"weights\"))]\n",
    "callbacks += [ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)]\n",
    "callbacks += [tf.keras.callbacks.TerminateOnNaN()]\n",
    "callbacks += [tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=30)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # print(\"\\n\", y_pred.numpy().flatten(), \"\\r\")\n",
    "    return(tf.keras.metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "adam = Adam(learning_rate=1e-2) \n",
    "model_loss.compile(loss=mse_loss, optimizer=adam, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "64/64 [==============================] - 53s 831ms/step - loss: 4.7102\n",
      "[[0.48166 0.55793]]\n",
      "\n",
      "Epoch 2/1000\n",
      "64/64 [==============================] - 51s 794ms/step - loss: 2.7492\n",
      "[[0.74327 0.82623]]\n",
      "\n",
      "Epoch 3/1000\n",
      "64/64 [==============================] - 53s 836ms/step - loss: 1.8404\n",
      "[[0.95117 0.98319]]\n",
      "\n",
      "Epoch 4/1000\n",
      "14/64 [=====>........................] - ETA: 35s - loss: 1.7387"
     ]
    }
   ],
   "source": [
    "history = model_loss.fit([x0, dW, dN], target, batch_size=4, initial_epoch=0, epochs=1000, callbacks=callbacks, shuffle=False)\n",
    "df_loss = pd.DataFrame(history.history['loss'])\n",
    "df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
