{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Consider\n",
    "\n",
    "$$\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "W \\in \\R ^m, N \\in \\R^\\ell\\\\\n",
    "X\\in \\R^d, Y\\in \\R^n, Z\\in \\R^{n\\times m}, R \\in \\R ^{n\\times \\ell}\\\\\n",
    "\\gamma \\in \\R^{d \\times \\ell}, \\sigma \\in \\R^{d\\times m} \\\\\n",
    "f\\in C^2(\\R^d, E\\subset\\R^n)\\\\\n",
    "$$\n",
    "\n",
    "For the control problem with jumps, we would have the FBSDE of the form (assuming zero drift on $X$)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\sigma_t dW_t + \\gamma_t dN_t\\\\\n",
    "dY_t &= (\\dots)dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assume that $Y=f(X)$. Necessarily, by Ito's lemma (Oksendal, p.9, 1.2.8; for k-th column):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_t &= (D f(X_t))^T \\cdot \\sigma_t \\\\\n",
    "R^{(\\cdot k)}_t &= f\\left(X_t+\\gamma ^{(\\cdot k)}_t\\right) - f(X_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assuming that $D f(X) D f(X)^T$ is never singular and $f^{-1}\\in C^2(E, \\R^d)$ exists, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_t &= \\left(D f(X_t) D f(X_t) ^T \\right) ^{-1} D f(X_t) Z =: \\beta(X_t) Z_t\\\\\n",
    "\\gamma^{(\\cdot k)}_t &= f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for $\\beta(X) = \\left(D f(X) D f(X) ^T \\right) ^{-1} D f(X)$. Thus we obtain by Ito's lemma (note that $D^2 f$ is a 3d-tensor indexed by $i,j,k$ with $k\\in 1,\\dots, n$)\n",
    "\n",
    "$$\n",
    "(\\dots)dt = \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt\n",
    "$$\n",
    "\n",
    "Altogether, we can rewrite\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\beta(X_t) Z_t dW_t + \\left(f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(X)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= \\delta_{ij} \\exp(X^{(i)}) =\\operatorname{diag}(\\exp(X))\\\\\n",
    "D^2 f(X)^{(ijk)} &= \\delta_{ijk} \\exp(X^{(i)})\\\\\n",
    "\\beta(X) &= \\delta_{ij}\\exp(-X^{(i)}) = \\operatorname{diag}(\\exp(-X))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\operatorname{diag}(\\exp(-X)) Z_t dW_t + \\left(\\log\\left(\\exp(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\operatorname{diag} ( \\operatorname{diag}(\\exp(-X)) Z_t( \\operatorname{diag}(\\exp(-X)) Z_t)^T) \\exp(X_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(X_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1a\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(aX)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\frac 1a \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= a\\delta_{ij} \\exp(aX^{(i)}) = a\\operatorname{diag}(\\exp(aX))\\\\\n",
    "D^2 f(X)^{(ijk)} &= a^2 \\delta_{ijk} \\exp(aX^{(i)})\\\\\n",
    "\\beta(X) &= \\frac 1a \\delta_{ij}\\exp(-aX^{(i)}) = \\frac 1a \\operatorname{diag}(\\exp(-aX))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\frac 1a \\operatorname{diag}(\\exp(-aX)) Z_t dW_t + \\left(\\frac 1a \\log\\left(\\exp(aX_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac {1}{2} \\operatorname{diag} ( \\operatorname{diag}(\\exp(-aX)) Z_t( \\operatorname{diag}(\\exp(-aX)) Z_t)^T) \\exp(aX_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(aX_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/h5py/__init__.py:39: UserWarning: h5py is running against HDF5 1.10.8 when it was built against 1.10.7, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer, BatchNormalization, Add\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.metrics import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gpu_utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(pick_gpu_lowest_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=11, linewidth=90, formatter=dict(float=lambda x: \"%7.5g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{timestamp}\"\n",
    "tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "output_dir = f\"_output/models/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_paths = 2 ** 10\n",
    "n_timesteps = 16\n",
    "time_horizon = 1.\n",
    "n_x_dimensions = 2\n",
    "n_y_dimensions = 2\n",
    "n_diffusion_factors = 2\n",
    "n_jump_factors = 2\n",
    "intensity = 5.\n",
    "alpha = 4.\n",
    "stddev = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = time_horizon / n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    return tf.zeros((n_x_dimensions,))\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z) / alpha\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    # floor the log argument\n",
    "    res = tf.maximum(tf.exp(alpha * x) + r, 1e-2)\n",
    "    return (tf.math.log(res) / alpha - x)\n",
    "\n",
    "def f(t, x, y, z, r):\n",
    "    res = tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z)\n",
    "    return 0.5 * tf.einsum('ij,j', tf.linalg.matmul(res, res, transpose_b=True), tf.exp(alpha * x))\n",
    "\n",
    "def g(x):\n",
    "    return tf.exp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layers and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Y0Callback(Callback):\n",
    "    \n",
    "    def __init__(self, filepath=None):\n",
    "        super(Y0Callback, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.y0s = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y0 = self.model.get_layer('y_0').y0.numpy()\n",
    "        self.y0s += [y0[0]]\n",
    "        print(f\"{y0}\\n\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.filepath is not None:\n",
    "            pd.DataFrame(self.y0s).to_csv(self.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSave(Callback):\n",
    "    def __init__(self, directory):\n",
    "        self.batch = 1\n",
    "        self.epoch = 1\n",
    "        self.directory = directory\n",
    "        shutil.rmtree(directory, ignore_errors=True)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        self.epoch += 1\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        filename = os.path.join(self.directory, f\"weights_{self.epoch:03}_{self.batch:03}.h5\")\n",
    "        self.model.save_weights(filename)\n",
    "        self.batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_x_dimensions, n_y_dimensions, n_diffusion_factors, n_jump_factors, n_timesteps, time_horizon):\n",
    "\n",
    "    dt = time_horizon / n_timesteps\n",
    "\n",
    "    def dX(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', s(t, x, y, z, r), dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', v(t, x, y, z, r), dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    def dY(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', z, dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', r, dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))        \n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    @tf.function\n",
    "    def hx(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return x + dX(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    @tf.function\n",
    "    def hy(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return y + dY(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    n_hidden_units = n_x_dimensions + n_diffusion_factors + n_jump_factors + 10\n",
    "\n",
    "    inputs_x0 = Input(shape=(n_x_dimensions))\n",
    "    inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "    inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "\n",
    "    # constant x0\n",
    "\n",
    "    x0 = tf.Variable([[1. for _ in range(n_x_dimensions)]], trainable=False)\n",
    "    y0 = tf.Variable([[0. for _ in range(n_y_dimensions)]], trainable=True)\n",
    "\n",
    "    x = InitialValue(x0, trainable=False, name='x_0')(inputs_dW)\n",
    "    y = InitialValue(y0, trainable=True, name='y_0')(inputs_dW)\n",
    "\n",
    "    # adjoints\n",
    "\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z1_0')(z)\n",
    "    z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z2_0')(z)\n",
    "    z = BatchNormalization(name='zbn_0')(z)\n",
    "    z = Reshape((n_y_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r1_0')(r)\n",
    "    r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r2_0')(r)\n",
    "    r = BatchNormalization(name='rbn_0')(r)\n",
    "    r = Reshape((n_y_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "\n",
    "    # pre-compile lambda layers\n",
    "    \n",
    "    for i in range(n_timesteps):\n",
    "\n",
    "        step = InitialValue(tf.Variable(i, dtype=tf.float32, trainable=False))(inputs_dW)\n",
    "\n",
    "        dW = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dW, step])\n",
    "        dN = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dN, step])\n",
    "\n",
    "        x, y = (\n",
    "            Lambda(hx, name=f'x_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "            Lambda(hy, name=f'y_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "        )\n",
    "\n",
    "        # we don't train z for the last time step; keep for consistency\n",
    "        z = concatenate([x, y])\n",
    "        z = Dense(n_hidden_units, activation='relu', name=f'z1_{i+1}')(z)\n",
    "        z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "        z = Reshape((n_y_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "        z = BatchNormalization(name=f'zbn_{i+1}')(z)\n",
    "\n",
    "        # we don't train r for the last time step; keep for consistency\n",
    "        r = concatenate([x, y])\n",
    "        r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r1_{i+1}')(r)\n",
    "        r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r2_{i+1}')(r)\n",
    "        r = Reshape((n_y_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "        r = BatchNormalization(name=f'rbn_{i+1}')(r)\n",
    "\n",
    "        paths += [[x, y, z, r]]\n",
    "\n",
    "    outputs_loss = Lambda(lambda r: r[1] - tf.vectorized_map(g, r[0]))([x, y])\n",
    "    \n",
    "    # remember that z and r are matrices\n",
    "    outputs_paths = tf.stack(\n",
    "        [tf.stack([p[0] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "        [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "    model_loss = Model([inputs_x0, inputs_dW, inputs_dN], outputs_loss)\n",
    "\n",
    "    # (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "    # skips the first time step\n",
    "    model_paths = Model([inputs_x0, inputs_dW, inputs_dN], outputs_paths)\n",
    "\n",
    "    return model_loss, model_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "dt = time_horizon / n_timesteps\n",
    "model_loss, model_paths = build_model(n_x_dimensions=n_x_dimensions,\n",
    "                                      n_y_dimensions=n_y_dimensions,\n",
    "                                      n_diffusion_factors=n_diffusion_factors,\n",
    "                                      n_jump_factors=n_jump_factors,\n",
    "                                      n_timesteps=n_timesteps,\n",
    "                                      time_horizon=time_horizon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant(np.full((n_paths, n_x_dimensions), 1.), dtype=tf.float32)\n",
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([intensity for _ in range(n_jump_factors)])))\n",
    "target = tf.zeros((n_paths, n_y_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "callbacks = []\n",
    "\n",
    "callbacks += [Y0Callback(os.path.join(output_dir, \"y0.csv\"))]\n",
    "# callbacks += [BatchSave(os.path.join(output_dir, \"weights\"))]\n",
    "callbacks += [ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)]\n",
    "callbacks += [tf.keras.callbacks.TerminateOnNaN()]\n",
    "callbacks += [tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=30)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # print(\"\\n\", y_pred.numpy().flatten(), \"\\r\")\n",
    "    return(tf.keras.metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "adam = Adam(learning_rate=1e-2) \n",
    "model_loss.compile(loss=mse_loss, optimizer=adam, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (x_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[      0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (y_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[      0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function build_model.<locals>.hx at 0x7f81f043bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function build_model.<locals>.hy at 0x7f81f043b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function build_model.<locals>.hx at 0x7f81f043bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function build_model.<locals>.hy at 0x7f81f043b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " 4/32 [==>...........................] - ETA: 14s - loss: 7.9492WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7f81ec338e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " 5/32 [===>..........................] - ETA: 14s - loss: 7.7092WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7f81ec3389d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "32/32 [==============================] - 32s 626ms/step - loss: 3.7912\n",
      "[[0.22623 0.21172]]\n",
      "\n",
      "Epoch 2/1000\n",
      "32/32 [==============================] - 21s 658ms/step - loss: 1.2192\n",
      "[[0.25807 0.25573]]\n",
      "\n",
      "Epoch 3/1000\n",
      "32/32 [==============================] - 23s 711ms/step - loss: 0.8822\n",
      "[[0.29497 0.27819]]\n",
      "\n",
      "Epoch 4/1000\n",
      "32/32 [==============================] - 21s 661ms/step - loss: 0.6281\n",
      "[[0.34899 0.31359]]\n",
      "\n",
      "Epoch 5/1000\n",
      "32/32 [==============================] - 19s 590ms/step - loss: 0.5339\n",
      "[[0.38783 0.35408]]\n",
      "\n",
      "Epoch 6/1000\n",
      "32/32 [==============================] - 21s 651ms/step - loss: 0.4746\n",
      "[[0.43289 0.40667]]\n",
      "\n",
      "Epoch 7/1000\n",
      "32/32 [==============================] - 22s 689ms/step - loss: 0.4092\n",
      "[[0.48398 0.45688]]\n",
      "\n",
      "Epoch 8/1000\n",
      "32/32 [==============================] - 22s 679ms/step - loss: 0.3810\n",
      "[[0.54704 0.50384]]\n",
      "\n",
      "Epoch 9/1000\n",
      "32/32 [==============================] - 17s 529ms/step - loss: 0.3347\n",
      "[[0.58727  0.5727]]\n",
      "\n",
      "Epoch 10/1000\n",
      "32/32 [==============================] - 21s 646ms/step - loss: 0.3070\n",
      "[[0.63061 0.63631]]\n",
      "\n",
      "Epoch 11/1000\n",
      "32/32 [==============================] - 21s 667ms/step - loss: 0.2848\n",
      "[[0.68773 0.69356]]\n",
      "\n",
      "Epoch 12/1000\n",
      "32/32 [==============================] - 22s 699ms/step - loss: 0.2646\n",
      "[[0.73941 0.75419]]\n",
      "\n",
      "Epoch 13/1000\n",
      "32/32 [==============================] - 21s 665ms/step - loss: 0.2457\n",
      "[[0.78985 0.81309]]\n",
      "\n",
      "Epoch 14/1000\n",
      "32/32 [==============================] - 17s 518ms/step - loss: 0.2289\n",
      "[[0.84151 0.87517]]\n",
      "\n",
      "Epoch 15/1000\n",
      "32/32 [==============================] - 23s 701ms/step - loss: 0.2175\n",
      "[[0.90205 0.93633]]\n",
      "\n",
      "Epoch 16/1000\n",
      "32/32 [==============================] - 22s 694ms/step - loss: 0.2066\n",
      "[[0.96394 0.98567]]\n",
      "\n",
      "Epoch 17/1000\n",
      "32/32 [==============================] - 22s 703ms/step - loss: 0.1934\n",
      "[[ 1.0242  1.0426]]\n",
      "\n",
      "Epoch 18/1000\n",
      "32/32 [==============================] - 18s 557ms/step - loss: 0.1783\n",
      "[[ 1.0862  1.1023]]\n",
      "\n",
      "Epoch 19/1000\n",
      "32/32 [==============================] - 19s 604ms/step - loss: 0.1614\n",
      "[[ 1.1474  1.1581]]\n",
      "\n",
      "Epoch 20/1000\n",
      "32/32 [==============================] - 23s 719ms/step - loss: 0.1497\n",
      "[[ 1.2002  1.2207]]\n",
      "\n",
      "Epoch 21/1000\n",
      "32/32 [==============================] - 23s 713ms/step - loss: 0.1393\n",
      "[[ 1.2541  1.2849]]\n",
      "\n",
      "Epoch 22/1000\n",
      "32/32 [==============================] - 21s 650ms/step - loss: 0.1268\n",
      "[[ 1.3092  1.3435]]\n",
      "\n",
      "Epoch 23/1000\n",
      "32/32 [==============================] - 21s 664ms/step - loss: 0.1145\n",
      "[[ 1.3624  1.4009]]\n",
      "\n",
      "Epoch 24/1000\n",
      "32/32 [==============================] - 23s 707ms/step - loss: 0.1086\n",
      "[[ 1.4186  1.4551]]\n",
      "\n",
      "Epoch 25/1000\n",
      "32/32 [==============================] - 22s 684ms/step - loss: 0.1004\n",
      "[[ 1.4797  1.5046]]\n",
      "\n",
      "Epoch 26/1000\n",
      "32/32 [==============================] - 22s 674ms/step - loss: 0.0890\n",
      "[[ 1.5324  1.5606]]\n",
      "\n",
      "Epoch 27/1000\n",
      "32/32 [==============================] - 21s 664ms/step - loss: 0.0835\n",
      "[[ 1.5812  1.6168]]\n",
      "\n",
      "Epoch 28/1000\n",
      "32/32 [==============================] - 22s 697ms/step - loss: 0.0747\n",
      "[[ 1.6329  1.6691]]\n",
      "\n",
      "Epoch 29/1000\n",
      "32/32 [==============================] - 23s 705ms/step - loss: 0.0678\n",
      "[[ 1.6821  1.7201]]\n",
      "\n",
      "Epoch 30/1000\n",
      "32/32 [==============================] - 21s 652ms/step - loss: 0.0645\n",
      "[[  1.732  1.7715]]\n",
      "\n",
      "Epoch 31/1000\n",
      "32/32 [==============================] - 21s 652ms/step - loss: 0.0587\n",
      "[[ 1.7865  1.8199]]\n",
      "\n",
      "Epoch 32/1000\n",
      "32/32 [==============================] - 25s 780ms/step - loss: 0.0521\n",
      "[[ 1.8387  1.8676]]\n",
      "\n",
      "Epoch 33/1000\n",
      "32/32 [==============================] - 24s 758ms/step - loss: 0.0463\n",
      "[[ 1.8885  1.9141]]\n",
      "\n",
      "Epoch 34/1000\n",
      "32/32 [==============================] - 24s 748ms/step - loss: 0.0415\n",
      "[[ 1.9362  1.9596]]\n",
      "\n",
      "Epoch 35/1000\n",
      "32/32 [==============================] - 24s 741ms/step - loss: 0.0379\n",
      "[[  1.983  2.0038]]\n",
      "\n",
      "Epoch 36/1000\n",
      "32/32 [==============================] - 25s 794ms/step - loss: 0.0327\n",
      "[[ 2.0281  2.0452]]\n",
      "\n",
      "Epoch 37/1000\n",
      "32/32 [==============================] - 22s 674ms/step - loss: 0.0295\n",
      "[[ 2.0704  2.0865]]\n",
      "\n",
      "Epoch 38/1000\n",
      "32/32 [==============================] - 18s 563ms/step - loss: 0.0252\n",
      "[[ 2.1111  2.1267]]\n",
      "\n",
      "Epoch 39/1000\n",
      "32/32 [==============================] - 22s 699ms/step - loss: 0.0222\n",
      "[[ 2.1498  2.1639]]\n",
      "\n",
      "Epoch 40/1000\n",
      "32/32 [==============================] - 22s 690ms/step - loss: 0.0195\n",
      "[[ 2.1872  2.1998]]\n",
      "\n",
      "Epoch 41/1000\n",
      "32/32 [==============================] - 22s 701ms/step - loss: 0.0171\n",
      "[[ 2.2222  2.2344]]\n",
      "\n",
      "Epoch 42/1000\n",
      "32/32 [==============================] - 23s 711ms/step - loss: 0.0148\n",
      "[[ 2.2562  2.2666]]\n",
      "\n",
      "Epoch 43/1000\n",
      "32/32 [==============================] - 17s 516ms/step - loss: 0.0129\n",
      "[[  2.288  2.2981]]\n",
      "\n",
      "Epoch 44/1000\n",
      "32/32 [==============================] - 21s 668ms/step - loss: 0.0112\n",
      "[[ 2.3182  2.3279]]\n",
      "\n",
      "Epoch 45/1000\n",
      "32/32 [==============================] - 23s 703ms/step - loss: 0.0098\n",
      "[[ 2.3472  2.3571]]\n",
      "\n",
      "Epoch 46/1000\n",
      "32/32 [==============================] - 23s 705ms/step - loss: 0.0085\n",
      "[[ 2.3752  2.3842]]\n",
      "\n",
      "Epoch 47/1000\n",
      "32/32 [==============================] - 17s 530ms/step - loss: 0.0072\n",
      "[[ 2.4018  2.4101]]\n",
      "\n",
      "Epoch 48/1000\n",
      "32/32 [==============================] - 20s 614ms/step - loss: 0.0061\n",
      "[[  2.427  2.4343]]\n",
      "\n",
      "Epoch 49/1000\n",
      "32/32 [==============================] - 23s 720ms/step - loss: 0.0052\n",
      "[[ 2.4507   2.457]]\n",
      "\n",
      "Epoch 50/1000\n",
      "32/32 [==============================] - 24s 757ms/step - loss: 0.0044\n",
      "[[ 2.4725  2.4788]]\n",
      "\n",
      "Epoch 51/1000\n",
      "32/32 [==============================] - 22s 668ms/step - loss: 0.0037\n",
      "[[ 2.4935  2.4986]]\n",
      "\n",
      "Epoch 52/1000\n",
      "32/32 [==============================] - 17s 536ms/step - loss: 0.0031\n",
      "[[ 2.5125  2.5176]]\n",
      "\n",
      "Epoch 53/1000\n",
      "32/32 [==============================] - 23s 724ms/step - loss: 0.0026\n",
      "[[ 2.5313  2.5341]]\n",
      "\n",
      "Epoch 54/1000\n",
      "32/32 [==============================] - 25s 778ms/step - loss: 0.0021\n",
      "[[ 2.5478   2.551]]\n",
      "\n",
      "Epoch 55/1000\n",
      "32/32 [==============================] - 24s 731ms/step - loss: 0.0018\n",
      "[[ 2.5637  2.5657]]\n",
      "\n",
      "Epoch 56/1000\n",
      "32/32 [==============================] - 22s 692ms/step - loss: 0.0015\n",
      "[[  2.578  2.5799]]\n",
      "\n",
      "Epoch 57/1000\n",
      "32/32 [==============================] - 20s 630ms/step - loss: 0.0012\n",
      "[[ 2.5911  2.5932]]\n",
      "\n",
      "Epoch 58/1000\n",
      "32/32 [==============================] - 22s 694ms/step - loss: 0.0010\n",
      "[[ 2.6037  2.6049]]\n",
      "\n",
      "Epoch 59/1000\n",
      "32/32 [==============================] - 22s 679ms/step - loss: 8.0983e-04\n",
      "[[ 2.6148  2.6162]]\n",
      "\n",
      "Epoch 60/1000\n",
      "32/32 [==============================] - 23s 720ms/step - loss: 6.5745e-04\n",
      "[[ 2.6251  2.6265]]\n",
      "\n",
      "Epoch 61/1000\n",
      "32/32 [==============================] - 24s 739ms/step - loss: 5.3959e-04\n",
      "[[ 2.6348  2.6357]]\n",
      "\n",
      "Epoch 62/1000\n",
      "32/32 [==============================] - 17s 530ms/step - loss: 4.4840e-04\n",
      "[[ 2.6445  2.6437]]\n",
      "\n",
      "Epoch 63/1000\n",
      "32/32 [==============================] - 18s 562ms/step - loss: 3.5424e-04\n",
      "[[ 2.6524   2.652]]\n",
      "\n",
      "Epoch 64/1000\n",
      "32/32 [==============================] - 21s 651ms/step - loss: 2.7927e-04\n",
      "[[ 2.6598  2.6596]]\n",
      "\n",
      "Epoch 65/1000\n",
      "32/32 [==============================] - 21s 652ms/step - loss: 2.2527e-04\n",
      "[[ 2.6667  2.6659]]\n",
      "\n",
      "Epoch 66/1000\n",
      " 3/32 [=>............................] - ETA: 20s - loss: 2.1175e-04"
     ]
    }
   ],
   "source": [
    "history = model_loss.fit([x0, dW, dN], target, batch_size=32, initial_epoch=0, epochs=1000, callbacks=callbacks, shuffle=False)\n",
    "df_loss = pd.DataFrame(history.history['loss'])\n",
    "df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "* Alpha has to grow with number of dimensions to avoid explosion\n",
    "* Number of samples has to grow with number of dimensions to converge to Y_0, loss will be close to zero if the number of samples is too small. What is a good number of samples?\n",
    "* Too little samples leads to plain memorizing overfitting. Could this be a theorem?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
