{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Consider\n",
    "\n",
    "$$\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "W \\in \\R ^m, N \\in \\R^\\ell\\\\\n",
    "X\\in \\R^d, Y\\in \\R^n, Z\\in \\R^{n\\times m}, R \\in \\R ^{n\\times \\ell}\\\\\n",
    "\\gamma \\in \\R^{d \\times \\ell}, \\sigma \\in \\R^{d\\times m} \\\\\n",
    "f\\in C^2(\\R^d, E\\subset\\R^n)\\\\\n",
    "$$\n",
    "\n",
    "For the control problem with jumps, we would have the FBSDE of the form (assuming zero drift on $X$)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\sigma_t dW_t + \\gamma_t dN_t\\\\\n",
    "dY_t &= (\\dots)dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assume that $Y=f(X)$. Necessarily, by Ito's lemma (Oksendal, p.9, 1.2.8; for k-th column):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_t &= (D f(X_t))^T \\cdot \\sigma_t \\\\\n",
    "R^{(\\cdot k)}_t &= f\\left(X_t+\\gamma ^{(\\cdot k)}_t\\right) - f(X_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assuming that $D f(X) D f(X)^T$ is never singular and $f^{-1}\\in C^2(E, \\R^d)$ exists, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_t &= \\left(D f(X_t) D f(X_t) ^T \\right) ^{-1} D f(X_t) Z =: \\beta(X_t) Z_t\\\\\n",
    "\\gamma^{(\\cdot k)}_t &= f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for $\\beta(X) = \\left(D f(X) D f(X) ^T \\right) ^{-1} D f(X)$. Thus we obtain by Ito's lemma (note that $D^2 f$ is a 3d-tensor indexed by $i,j,k$ with $k\\in 1,\\dots, n$)\n",
    "\n",
    "$$\n",
    "(\\dots)dt = \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt\n",
    "$$\n",
    "\n",
    "Altogether, we can rewrite\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\beta(X_t) Z_t dW_t + \\left(f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(X)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= \\delta_{ij} \\exp(X^{(i)}) =\\operatorname{diag}(\\exp(X))\\\\\n",
    "D^2 f(X)^{(ijk)} &= \\delta_{ijk} \\exp(X^{(i)})\\\\\n",
    "\\beta(X) &= \\delta_{ij}\\exp(-X^{(i)}) = \\operatorname{diag}(\\exp(-X))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\operatorname{diag}(\\exp(-X)) Z_t dW_t + \\left(\\log\\left(\\exp(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\operatorname{diag} ( \\operatorname{diag}(\\exp(-X)) Z_t( \\operatorname{diag}(\\exp(-X)) Z_t)^T) \\exp(X_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(X_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1a\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(aX)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\frac 1a \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= a\\delta_{ij} \\exp(aX^{(i)}) = a\\operatorname{diag}(\\exp(aX))\\\\\n",
    "D^2 f(X)^{(ijk)} &= a^2 \\delta_{ijk} \\exp(aX^{(i)})\\\\\n",
    "\\beta(X) &= \\frac 1a \\delta_{ij}\\exp(-aX^{(i)}) = \\frac 1a \\operatorname{diag}(\\exp(-aX))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\frac 1a \\operatorname{diag}(\\exp(-aX)) Z_t dW_t + \\left(\\frac 1a \\log\\left(\\exp(aX_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac {1}{2} \\operatorname{diag} ( \\operatorname{diag}(\\exp(-aX)) Z_t( \\operatorname{diag}(\\exp(-aX)) Z_t)^T) \\exp(aX_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(aX_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/h5py/__init__.py:39: UserWarning: h5py is running against HDF5 1.10.8 when it was built against 1.10.7, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer, BatchNormalization, Add\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.metrics import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gpu_utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(pick_gpu_lowest_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=11, linewidth=90, formatter=dict(float=lambda x: \"%7.5g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{timestamp}\"\n",
    "tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "output_dir = f\"_output/models/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_paths = 2 ** 16\n",
    "n_timesteps = 16\n",
    "time_horizon = 1.\n",
    "n_x_dimensions = 20\n",
    "n_y_dimensions = 20\n",
    "n_diffusion_factors = 20\n",
    "n_jump_factors = 20\n",
    "intensity = 5.\n",
    "alpha = 4.\n",
    "stddev = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = time_horizon / n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    return tf.zeros((n_x_dimensions,))\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z) / alpha\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    # floor the log argument\n",
    "    res = tf.maximum(tf.exp(alpha * x) + r, 1e-2)\n",
    "    return (tf.math.log(res) / alpha - x)\n",
    "\n",
    "def f(t, x, y, z, r):\n",
    "    res = tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z)\n",
    "    return 0.5 * tf.einsum('ij,j', tf.linalg.matmul(res, res, transpose_b=True), tf.exp(alpha * x))\n",
    "\n",
    "def g(x):\n",
    "    return tf.exp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layers and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Y0Callback(Callback):\n",
    "    \n",
    "    def __init__(self, filepath=None, freq=32):\n",
    "        super(Y0Callback, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.batch = 1\n",
    "        self.freq = freq\n",
    "    \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        y0 = self.model.get_layer('y_0').y0.numpy()\n",
    "        if self.batch % self.freq == 0:\n",
    "            print(f\"\\n{y0}\\n\")\n",
    "        self.batch += 1\n",
    "\n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        self.batch = 1\n",
    "        \n",
    "#     def on_train_end(self, logs=None):\n",
    "#         if self.filepath is not None:\n",
    "#             pd.DataFrame(self.y0s).to_csv(self.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSave(Callback):\n",
    "    def __init__(self, directory):\n",
    "        self.batch = 1\n",
    "        self.epoch = 1\n",
    "        self.directory = directory\n",
    "        shutil.rmtree(directory, ignore_errors=True)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        self.epoch += 1\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        filename = os.path.join(self.directory, f\"weights_{self.epoch:03}_{self.batch:03}.h5\")\n",
    "        self.model.save_weights(filename)\n",
    "        self.batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_x_dimensions, n_y_dimensions, n_diffusion_factors, n_jump_factors, n_timesteps, time_horizon):\n",
    "\n",
    "    dt = time_horizon / n_timesteps\n",
    "\n",
    "    def dX(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', s(t, x, y, z, r), dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', v(t, x, y, z, r), dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    def dY(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', z, dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', r, dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))        \n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    @tf.function\n",
    "    def hx(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return x + dX(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    @tf.function\n",
    "    def hy(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return y + dY(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    #n_hidden_units = n_x_dimensions + n_diffusion_factors + n_jump_factors + 10\n",
    "    n_hidden_units = 5\n",
    "\n",
    "    inputs_x0 = Input(shape=(n_x_dimensions))\n",
    "    inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "    inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "\n",
    "    # constant x0\n",
    "\n",
    "    x0 = tf.Variable([[1. for _ in range(n_x_dimensions)]], trainable=False)\n",
    "    y0 = tf.Variable([[0. for _ in range(n_y_dimensions)]], trainable=True)\n",
    "\n",
    "    x = InitialValue(x0, trainable=False, name='x_0')(inputs_dW)\n",
    "    y = InitialValue(y0, trainable=True, name='y_0')(inputs_dW)\n",
    "\n",
    "    # adjoints\n",
    "\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z1_0')(z)\n",
    "    z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z2_0')(z)\n",
    "    z = BatchNormalization(name='zbn_0')(z)\n",
    "    z = Reshape((n_y_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r1_0')(r)\n",
    "    r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r2_0')(r)\n",
    "    r = BatchNormalization(name='rbn_0')(r)\n",
    "    r = Reshape((n_y_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "\n",
    "    # pre-compile lambda layers\n",
    "    \n",
    "    for i in range(n_timesteps):\n",
    "\n",
    "        step = InitialValue(tf.Variable(i, dtype=tf.float32, trainable=False))(inputs_dW)\n",
    "\n",
    "        dW = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dW, step])\n",
    "        dN = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dN, step])\n",
    "\n",
    "        x, y = (\n",
    "            Lambda(hx, name=f'x_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "            Lambda(hy, name=f'y_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "        )\n",
    "\n",
    "        # we don't train z for the last time step; keep for consistency\n",
    "        z = concatenate([x, y])\n",
    "        z = Dense(n_hidden_units, activation='relu', name=f'z1_{i+1}')(z)\n",
    "        z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "        z = Reshape((n_y_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "        z = BatchNormalization(name=f'zbn_{i+1}')(z)\n",
    "\n",
    "        # we don't train r for the last time step; keep for consistency\n",
    "        r = concatenate([x, y])\n",
    "        r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r1_{i+1}')(r)\n",
    "        r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r2_{i+1}')(r)\n",
    "        r = Reshape((n_y_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "        r = BatchNormalization(name=f'rbn_{i+1}')(r)\n",
    "\n",
    "        paths += [[x, y, z, r]]\n",
    "\n",
    "    outputs_loss = Lambda(lambda r: r[1] - tf.vectorized_map(g, r[0]))([x, y])\n",
    "    \n",
    "    # remember that z and r are matrices\n",
    "    outputs_paths = tf.stack(\n",
    "        [tf.stack([p[0] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "        [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "    model_loss = Model([inputs_x0, inputs_dW, inputs_dN], outputs_loss)\n",
    "\n",
    "    # (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "    # skips the first time step\n",
    "    model_paths = Model([inputs_x0, inputs_dW, inputs_dN], outputs_paths)\n",
    "\n",
    "    return model_loss, model_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "dt = time_horizon / n_timesteps\n",
    "model_loss, model_paths = build_model(n_x_dimensions=n_x_dimensions,\n",
    "                                      n_y_dimensions=n_y_dimensions,\n",
    "                                      n_diffusion_factors=n_diffusion_factors,\n",
    "                                      n_jump_factors=n_jump_factors,\n",
    "                                      n_timesteps=n_timesteps,\n",
    "                                      time_horizon=time_horizon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant(np.full((n_paths, n_x_dimensions), 1.), dtype=tf.float32)\n",
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([intensity for _ in range(n_jump_factors)])))\n",
    "target = tf.zeros((n_paths, n_y_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "callbacks = []\n",
    "\n",
    "callbacks += [Y0Callback(os.path.join(output_dir, \"y0.csv\"))]\n",
    "# callbacks += [BatchSave(os.path.join(output_dir, \"weights\"))]\n",
    "callbacks += [ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)]\n",
    "callbacks += [tf.keras.callbacks.TerminateOnNaN()]\n",
    "callbacks += [tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=30)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # print(\"\\n\", y_pred.numpy().flatten(), \"\\r\")\n",
    "    return(tf.keras.metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "adam = Adam(learning_rate=1e-2) \n",
    "model_loss.compile(loss=mse_loss, optimizer=adam, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (x_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 20) dtype=float32, numpy=\n",
      "array([[      0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (x_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 20) dtype=float32, numpy=\n",
      "array([[      0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (y_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 20) dtype=float32, numpy=\n",
      "array([[      0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (y_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 20) dtype=float32, numpy=\n",
      "array([[      0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32/2048 [..............................] - ETA: 21:00 - loss: 26.7701\n",
      "[[0.0034396 -0.00018156 0.015271 0.022059 -0.041922 0.10288 0.060548 0.057387 -0.036833\n",
      "  0.081858 -0.069396 -0.0057992 0.14145 -0.10794 -0.042393 -0.07326 -0.025318 -0.038901\n",
      "  0.066175 -0.044001]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 20:42 - loss: 20.6459\n",
      "[[0.0074945 0.0044151 0.0037754 0.029503 -0.050241 0.10783 0.052805 0.054267 -0.023193\n",
      "  0.080053 -0.056851 -0.0081929 0.14555 -0.091452 -0.032763 -0.085385 -0.013655 -0.03922\n",
      "  0.082727 -0.020998]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 20:50 - loss: 16.0805\n",
      "[[-0.011748 0.014293 -0.0031429 0.032666 -0.039228 0.10253 0.044039 0.070241 -0.015925\n",
      "  0.10144 -0.049741 -0.04091 0.12904 -0.071163 -0.021239 -0.094892 -0.0078957 -0.037426\n",
      "  0.089894 -0.0027611]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 19:57 - loss: 13.0677\n",
      "[[-0.021281 0.025468 0.014117 0.021354 -0.015192 0.09906 0.078645 0.056417 -0.0053173\n",
      "  0.13965 -0.035803 -0.12961 0.11285 -0.06442 0.0042151 -0.029702 -0.0018738 -0.03233\n",
      "  0.099561 -0.0075386]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 19:07 - loss: 10.8824\n",
      "[[-0.013143 0.029888 0.027537 0.031285 -0.0073589 0.084751 0.091852 0.057352 0.019001\n",
      "  0.10566 -0.019607 -0.15587 0.094096 -0.035842 0.013385 -0.0040961 0.016393 -0.011864\n",
      "  0.10083 0.0066209]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 19:03 - loss: 9.2552\n",
      "[[-0.017237 0.032404 0.022154 0.045537 0.019966 0.064219 0.098826 0.06047 0.039177\n",
      "  0.096876 0.0063964 -0.16802 0.080627 -0.010323 0.026663 0.016168 0.028963 0.0043827\n",
      "  0.10142 0.017824]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 18:56 - loss: 8.0169\n",
      "[[-0.020861 0.036452 0.022515 0.049333 0.035448 0.049175  0.1022 0.055646 0.036037\n",
      "  0.099554 0.030636 -0.16789 0.066666 -0.0068804 0.038699 0.028673 0.04577 0.029523\n",
      "  0.10464 0.026067]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 18:55 - loss: 7.0583\n",
      "[[-0.012413 0.04103 0.032165 0.047968 0.044224 0.052198   0.105 0.059799 0.040437\n",
      "  0.095679 0.041629 -0.15633 0.069947 0.0006271 0.039694 0.033133 0.057508 0.042311\n",
      "  0.10183 0.039842]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 18:39 - loss: 6.2976\n",
      "[[-0.0014795 0.047384 0.041923 0.054289 0.053617 0.053147 0.10959 0.058459 0.043747\n",
      "  0.097755 0.050812 -0.15008 0.077258 0.012634 0.044374 0.038258 0.067451 0.049371\n",
      "  0.10455 0.050141]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 18:05 - loss: 5.6828\n",
      "[[0.010161 0.049228 0.048969 0.059313 0.060581 0.059579 0.11919 0.060048 0.051146\n",
      "  0.10023 0.058165 -0.13982 0.082652 0.014626 0.047824 0.045317 0.074969 0.055506\n",
      "  0.10719 0.057885]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 17:26 - loss: 5.1768\n",
      "[[0.019464 0.052348 0.050786 0.062982 0.069002 0.063839 0.12318 0.067401 0.057304\n",
      "   0.1056 0.066597 -0.12683 0.087207 0.021444 0.052056 0.048897 0.082954 0.061519\n",
      "  0.11243 0.066364]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 17:05 - loss: 4.7538\n",
      "[[0.024846 0.058245 0.055676 0.067192 0.073552 0.066794 0.12486 0.075977 0.064294\n",
      "  0.11446 0.075438 -0.11178 0.092944 0.026899 0.056718 0.054223 0.089669 0.069432\n",
      "  0.11937 0.072307]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 16:50 - loss: 4.3952\n",
      "[[0.030877 0.062953 0.060004 0.070978 0.079391 0.071732 0.13157 0.083906 0.070946\n",
      "  0.11991 0.084172 -0.09459 0.10057 0.032776 0.061744 0.062136 0.094988 0.076458 0.12424\n",
      "  0.079433]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 16:33 - loss: 4.0880\n",
      "[[0.034269 0.068342 0.065977 0.076345 0.087599 0.075077 0.13875 0.090187 0.078776\n",
      "   0.1264 0.095809 -0.081093 0.10783 0.037122 0.067314 0.071838 0.10503 0.082213 0.12883\n",
      "  0.085239]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 16:18 - loss: 3.8207\n",
      "[[0.04119 0.073867 0.072751 0.082705 0.094478 0.080958 0.14555 0.096714 0.085396 0.13286\n",
      "  0.10483 -0.068034 0.11491 0.045337 0.072715 0.080057 0.11252 0.090147 0.13616 0.092545]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 15:58 - loss: 3.5873\n",
      "[[0.048524 0.081521 0.080279 0.089394 0.099461 0.086124 0.15299 0.10414 0.092573 0.13943\n",
      "   0.1129 -0.053512  0.1226 0.051273 0.078878 0.08823 0.12069 0.098293 0.14194 0.10032]]\n",
      "\n",
      " 544/2048 [======>.......................] - ETA: 15:44 - loss: 3.3818\n",
      "[[0.058369 0.089805 0.090469 0.095256 0.10874 0.091256 0.16177 0.11358 0.10183 0.14927\n",
      "  0.12599 -0.040189 0.13271 0.058675 0.085184 0.097206 0.13206 0.10679 0.14935 0.10992]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 15:31 - loss: 3.1996\n",
      "[[0.067139 0.096426 0.10106 0.10247 0.11779 0.09645 0.16881 0.12307 0.11003  0.1598\n",
      "  0.13772 -0.02826 0.14232 0.065536 0.091258 0.10783 0.14244   0.115 0.15544 0.11863]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 15:15 - loss: 3.0363\n",
      "[[0.073246 0.10336  0.1095 0.10907 0.12692 0.10423 0.17591 0.13324 0.11894 0.16873\n",
      "  0.14965 -0.017705 0.15277 0.075539 0.098949 0.11721 0.15307 0.12418 0.16326 0.12649]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 14:57 - loss: 2.8888\n",
      "[[0.080946 0.11057 0.11979 0.11635 0.13597 0.11058 0.18547 0.14347 0.12805 0.17852\n",
      "  0.16207 -0.012224 0.16381 0.084272 0.10555 0.13015 0.16428 0.13364 0.17216  0.1367]]\n",
      "\n",
      " 672/2048 [========>.....................] - ETA: 14:33 - loss: 2.7550\n",
      "[[0.091984 0.11661 0.12962 0.12507 0.14586 0.11496  0.1945 0.15401 0.13704 0.18904\n",
      "   0.1742 -0.005902 0.17467 0.092924 0.11347 0.14118 0.17519 0.14384 0.18095 0.14612]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 14:06 - loss: 2.6339\n",
      "[[0.10299 0.12389 0.14033 0.13258 0.15481   0.125 0.20248 0.16363 0.14656 0.19828\n",
      "   0.1853 0.0047159 0.18473 0.10251 0.12113 0.15062 0.18487 0.15248 0.18914   0.155]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 13:48 - loss: 2.5228\n",
      "[[0.11318 0.13157 0.15031 0.14101  0.1652 0.12963 0.21264 0.17455 0.15691 0.20919\n",
      "  0.19831 0.01206 0.19626 0.11112 0.12721 0.15986 0.19671 0.16278 0.19811 0.16506]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 13:30 - loss: 2.4213\n",
      "[[0.12041 0.14159 0.15984 0.15058 0.17683 0.13834 0.22243 0.18644 0.16698 0.22107\n",
      "  0.21192 0.022078 0.20867  0.1201 0.13567 0.16936 0.20962 0.17438 0.20772 0.17624]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 13:12 - loss: 2.3276\n",
      "[[ 0.1301 0.15071 0.17035 0.15873  0.1875  0.1444  0.2312 0.19732 0.17609 0.23149\n",
      "  0.22417 0.031251 0.21925 0.12922 0.14488 0.18271 0.22075 0.18586 0.21616 0.18732]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 12:55 - loss: 2.2418\n",
      "[[0.13859 0.15795 0.18175 0.17087 0.19626 0.15427 0.24266 0.20644 0.18792 0.24029\n",
      "   0.2351 0.047476 0.22864 0.14709 0.15325  0.1973 0.23062 0.19672 0.22654 0.19761]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 12:32 - loss: 2.1625\n",
      "[[0.15887 0.17139 0.19599 0.18509 0.20151 0.16976 0.25036 0.21185 0.19634  0.2445\n",
      "  0.24205 0.070809 0.23386 0.16553 0.16848 0.21169 0.23655 0.20984 0.23417 0.20942]]\n",
      "\n",
      " 896/2048 [============>.................] - ETA: 12:06 - loss: 2.0888\n",
      "[[0.16672 0.17862 0.20057 0.18942 0.21532 0.17383 0.26631  0.2267 0.21125 0.25969\n",
      "   0.2589 0.077192 0.24957 0.16853 0.17291 0.21833 0.25219 0.21811 0.24938 0.21683]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 11:43 - loss: 2.0202\n",
      "[[0.17996  0.1906 0.21801 0.20025 0.22763 0.18386 0.27818  0.2387 0.22374 0.27135\n",
      "  0.27089 0.090811 0.26145 0.18413 0.18368 0.23271 0.26382 0.23198 0.26172 0.23053]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 11:23 - loss: 1.9559\n",
      "[[0.18668 0.19651 0.22273 0.20907 0.24081 0.19304 0.29398  0.2529 0.23868  0.2855\n",
      "  0.28736 0.099161 0.27656 0.18944 0.19182 0.24065 0.27891 0.24292 0.27582 0.24176]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:04 - loss: 1.8957\n",
      "[[0.19644 0.20823 0.23793 0.22091 0.25388 0.20332 0.30867 0.26627 0.25277 0.29872\n",
      "  0.30146 0.10989 0.29013 0.20187 0.20146 0.25197 0.29312  0.2552  0.2897 0.25399]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 10:45 - loss: 1.8395\n",
      "[[0.20777 0.21999 0.24937 0.23194 0.26622 0.21441 0.32152 0.27898 0.26605 0.31126\n",
      "  0.31471  0.1232 0.30279 0.21127 0.21211 0.26278 0.30549 0.26841 0.30247 0.26708]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 10:29 - loss: 1.7867\n",
      "[[0.21684 0.22914 0.26017 0.24447 0.27777 0.22539 0.33355 0.29123 0.27879 0.32272\n",
      "  0.32738 0.14125 0.31517 0.22658 0.22377 0.27482 0.31761 0.28267 0.31437  0.2805]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:06 - loss: 1.7370\n",
      "[[0.22967  0.2407 0.27421 0.25422 0.29256 0.23267 0.35001 0.30657 0.29409  0.3386\n",
      "  0.34354 0.14837 0.33126 0.23252 0.23414 0.29055 0.33365 0.29517 0.32993 0.29228]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 9:42 - loss: 1.6899\n",
      "[[0.23823 0.25095 0.28736 0.26497 0.30692 0.24261 0.36403 0.32159 0.30922 0.35254\n",
      "  0.35829    0.16 0.34575 0.23974 0.24503 0.30215 0.34797 0.30764 0.34439 0.30464]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 9:22 - loss: 1.6456\n",
      "[[0.25146 0.26206 0.29779 0.27865 0.31846 0.25467 0.37739 0.33389 0.32273 0.36461\n",
      "   0.3724 0.17616 0.35833  0.2548 0.25791 0.31554 0.36076 0.32188 0.35649 0.31894]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:02 - loss: 1.6034\n",
      "[[0.26679 0.27264  0.3127 0.28768 0.33147 0.26165 0.39176 0.34764 0.33735 0.37828\n",
      "  0.38725 0.19212 0.37218 0.26347 0.26701 0.32714 0.37485 0.33214 0.37029 0.32888]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 8:42 - loss: 1.5637\n",
      "[[0.27534 0.28585 0.32305 0.29829  0.3442 0.27111 0.40391 0.36038 0.35069 0.39008\n",
      "  0.39998  0.2055 0.38438 0.27302 0.27814 0.33813 0.38714 0.34551 0.38276 0.34322]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 8:24 - loss: 1.5259\n",
      "[[0.28902 0.29642 0.33802 0.31217 0.35602 0.28313 0.41644 0.37247 0.36353 0.40174\n",
      "   0.4132 0.21711 0.39622 0.29002  0.2895 0.35119 0.39933 0.35609 0.39429 0.35423]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:05 - loss: 1.4899\n",
      "[[0.29738 0.30972 0.34753 0.32147 0.37127 0.29598 0.43175 0.38797 0.37909 0.41766\n",
      "  0.42831 0.23086 0.41174 0.29382 0.30342 0.36645 0.41486 0.37232 0.41002 0.36969]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 7:43 - loss: 1.4556\n",
      "[[ 0.3146 0.32316 0.36399 0.33216 0.38657 0.30515 0.44721 0.40324 0.39489 0.43279\n",
      "  0.44418 0.24087 0.42765 0.31041 0.31559 0.38026 0.43041 0.38588 0.42494 0.38289]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 7:22 - loss: 1.4229\n",
      "[[ 0.3209 0.33506 0.37565 0.34614 0.40202 0.31668 0.46378 0.41909  0.4108 0.44871\n",
      "  0.46054 0.25542 0.44385 0.32398 0.32703 0.39082 0.44646 0.40096 0.44061  0.3973]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:02 - loss: 1.3918\n",
      "[[0.33566 0.34695  0.3901 0.35706 0.41622 0.32735 0.47866 0.43369 0.42655 0.46268\n",
      "  0.47562 0.26981 0.45766  0.3337 0.34054 0.40455 0.46091 0.41585  0.4548  0.4123]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 6:42 - loss: 1.3622\n",
      "[[0.35201 0.36699 0.40201 0.36973 0.43172 0.33797 0.49416 0.44975 0.44282  0.4781\n",
      "  0.49156 0.28876 0.47407 0.34755 0.35456  0.4207 0.47628 0.42987 0.47057 0.42632]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:22 - loss: 1.3341\n",
      "[[0.36568 0.37654 0.41213 0.38174 0.44527 0.35314 0.50704 0.46344  0.4567 0.49051\n",
      "  0.50504 0.30536 0.48722 0.36273 0.36899 0.43613 0.48986  0.4451 0.48375 0.44168]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:03 - loss: 1.3072\n",
      "[[0.37683 0.38986 0.42779 0.39379 0.46132 0.36249 0.52613 0.48067 0.47531  0.5083\n",
      "  0.52387 0.31845 0.50515 0.37478 0.38116   0.451 0.50754 0.46186 0.50167 0.45723]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 5:42 - loss: 1.2813\n",
      "[[0.39175  0.4045 0.44411   0.407 0.47561 0.37739 0.54069 0.49538 0.49057 0.52234\n",
      "  0.53857 0.33175 0.51992 0.38714 0.39692 0.46474 0.52203 0.47732 0.51647 0.47318]]\n",
      "\n",
      "1536/2048 [=====================>........] - ETA: 5:21 - loss: 1.2564\n",
      "[[0.40251 0.41732 0.45408 0.42038 0.49088  0.3948 0.55447 0.50958 0.50524 0.53698\n",
      "  0.55216 0.34775 0.53383 0.40058 0.41218 0.47782 0.53634 0.49163 0.53118 0.48792]]\n",
      "\n",
      "1568/2048 [=====================>........] - ETA: 5:01 - loss: 1.2324\n",
      "[[0.41909 0.42945 0.46838  0.4329 0.50741 0.40327 0.57078   0.526 0.52219 0.55322\n",
      "  0.56934   0.361  0.5496 0.41924 0.42517  0.4982 0.55284  0.5062 0.54748 0.50305]]\n",
      "\n",
      "1600/2048 [======================>.......] - ETA: 4:41 - loss: 1.2095\n",
      "[[0.43169 0.45053 0.48564  0.4482 0.52171  0.4241 0.58499 0.54121  0.5375 0.56733\n",
      "  0.58347 0.38237 0.56413 0.43813 0.44135 0.50777 0.56749 0.52422 0.56195 0.52045]]\n",
      "\n",
      "1632/2048 [======================>.......] - ETA: 4:21 - loss: 1.1874\n",
      "[[0.44972 0.46364 0.50379 0.46652 0.53805 0.43381 0.59987  0.5574 0.55353 0.58343\n",
      "  0.59898 0.39568    0.58 0.45409 0.45437 0.53252 0.58295 0.53797  0.5781 0.53405]]\n",
      "\n",
      "1664/2048 [=======================>......] - ETA: 4:01 - loss: 1.1660\n",
      "[[0.46515 0.47699 0.51459 0.48202 0.55471 0.44937  0.6163  0.5739 0.57048 0.59993\n",
      "  0.61527 0.40977 0.59604 0.47823 0.47193 0.55105 0.59904 0.55558 0.59441 0.55193]]\n",
      "\n",
      "1696/2048 [=======================>......] - ETA: 3:41 - loss: 1.1458\n",
      "[[0.48194 0.49238 0.52971 0.49342 0.57078 0.46293 0.63055  0.5898 0.58615 0.61485\n",
      "  0.62985 0.43185 0.61149 0.49655 0.48628 0.56134 0.61428  0.5681 0.61001 0.56465]]\n",
      "\n",
      "1728/2048 [========================>.....] - ETA: 3:21 - loss: 1.1264\n",
      "[[0.49533 0.50714 0.54509 0.50881 0.58613 0.47943 0.64753 0.60569 0.60358 0.63082\n",
      "  0.64654 0.44641 0.62702 0.50965 0.50226 0.57748 0.63073 0.58639 0.62599 0.58236]]\n",
      "\n",
      "1760/2048 [========================>.....] - ETA: 3:00 - loss: 1.1077\n",
      "[[0.51546 0.51798 0.56371 0.52672 0.60205 0.49373 0.66352 0.62275 0.62027 0.64619\n",
      "  0.66272 0.46066 0.64409 0.52876 0.51909 0.59095 0.64671 0.60103  0.6416 0.59702]]\n",
      "\n",
      "1792/2048 [=========================>....] - ETA: 2:40 - loss: 1.0898\n",
      "[[0.54246 0.54258 0.58638  0.5503 0.61465 0.51744 0.67184 0.63369 0.63062 0.65564\n",
      "  0.67105 0.49016 0.65485  0.5554 0.53996 0.61485  0.6574 0.61662 0.65146 0.61264]]\n",
      "\n",
      "1824/2048 [=========================>....] - ETA: 2:20 - loss: 1.0720\n",
      "[[ 0.5562 0.55077 0.59668 0.56092 0.63394 0.52755 0.69322 0.65361  0.6509 0.67718\n",
      "  0.69365 0.50596 0.67571 0.56758 0.55584 0.62432  0.6783 0.63595 0.67192 0.63063]]\n",
      "\n",
      "1856/2048 [==========================>...] - ETA: 2:00 - loss: 1.0549\n",
      "[[0.57141 0.58176 0.61611 0.57781 0.65156 0.54091 0.70862  0.6704 0.66729 0.69324\n",
      "  0.70825 0.51864 0.69185 0.58531  0.5696 0.63771 0.69479 0.65146 0.68833  0.6474]]\n",
      "\n",
      "1888/2048 [==========================>...] - ETA: 1:40 - loss: 1.0384\n",
      "[[0.59289 0.59239 0.63805 0.59447 0.66918 0.55314 0.72782 0.68748 0.68617  0.7117\n",
      "  0.72717 0.53384 0.70955 0.60021 0.58699 0.65936 0.71343 0.66651  0.7065 0.66306]]\n",
      "\n",
      "1920/2048 [===========================>..] - ETA: 1:20 - loss: 1.0224\n",
      "[[0.61391 0.61252 0.65409 0.61218 0.68658 0.57194 0.74349 0.70553 0.70335 0.72809\n",
      "  0.74256  0.5532 0.72634  0.6196  0.6052 0.67703 0.72877 0.68533 0.72333 0.68157]]\n",
      "\n",
      "1952/2048 [===========================>..] - ETA: 1:00 - loss: 1.0069\n",
      "[[0.63203  0.6291  0.6728 0.62633 0.70455 0.58798 0.76059 0.72254 0.72072 0.74532\n",
      "  0.75991 0.57318 0.74363 0.63503 0.62019 0.68937 0.74615 0.69922 0.74028 0.69575]]\n",
      "\n",
      "1984/2048 [============================>.] - ETA: 40s - loss: 0.9918\n",
      "[[0.64748 0.64437 0.68791 0.64236 0.72093 0.60333 0.77658  0.7383  0.7366  0.7609\n",
      "  0.77619  0.5884 0.75929 0.65013 0.63748 0.70738 0.76236 0.71656 0.75643 0.71221]]\n",
      "\n",
      "2016/2048 [============================>.] - ETA: 20s - loss: 0.9773\n",
      "[[ 0.6658 0.65964 0.70529 0.65798 0.73787 0.61987 0.79199 0.75516 0.75357 0.77773\n",
      "  0.79187 0.60313 0.77532  0.6677 0.65452  0.7268 0.77787 0.73312 0.77299 0.72963]]\n",
      "\n",
      "2048/2048 [==============================] - ETA: 0s - loss: 0.9631\n",
      "[[0.67954 0.67583 0.72173 0.67513 0.75485 0.63502 0.80932 0.77297 0.77107 0.79453\n",
      "  0.80826 0.62105 0.79261 0.68673  0.6698 0.73507 0.79574 0.75108 0.78969 0.74694]]\n",
      "\n",
      "2048/2048 [==============================] - 1306s 631ms/step - loss: 0.9631\n",
      "Epoch 2/1000\n",
      "  32/2048 [..............................] - ETA: 23:02 - loss: 0.0671\n",
      "[[0.70281 0.69458 0.73811  0.6894 0.77296 0.65096 0.82615 0.79011 0.78867 0.81174\n",
      "  0.82598 0.64158 0.80963 0.70126 0.68703 0.75668 0.81295 0.76431 0.80707 0.76111]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 21:51 - loss: 0.0651\n",
      "[[0.71813 0.71229 0.74906 0.70653 0.79156 0.66626 0.84464 0.80887 0.80773 0.83003\n",
      "  0.84493 0.65295 0.82801 0.71784 0.70666 0.77619 0.83135 0.78518 0.82535 0.78103]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 20:29 - loss: 0.0641\n",
      "[[0.73704 0.73131 0.76645 0.72264 0.80876 0.68386 0.86064 0.82553 0.82504 0.84619\n",
      "  0.86008 0.66965 0.84467 0.73702 0.72533 0.79036 0.84782 0.79986 0.84153 0.79615]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 20:27 - loss: 0.0637\n",
      "[[0.75377 0.75065 0.78384 0.73892 0.82552 0.70105 0.87656 0.84234 0.84213 0.86239\n",
      "  0.87624 0.69002  0.8604 0.75392 0.74037 0.80889  0.8633 0.81557 0.85824 0.81256]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 20:23 - loss: 0.0640\n",
      "[[0.77354 0.76713  0.8024 0.75763 0.84329 0.72184 0.89195 0.85904 0.85863 0.87927\n",
      "  0.89158 0.71058 0.87713 0.77597 0.76074 0.82786  0.8791  0.8338 0.87502 0.83052]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 20:23 - loss: 0.0650\n",
      "[[0.78971 0.78837 0.81752 0.77431 0.86197 0.74111 0.91096 0.87736  0.8776 0.89764\n",
      "  0.91171 0.72983 0.89608 0.79335 0.77831 0.84009 0.89847 0.85259 0.89343 0.84909]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 20:18 - loss: 0.0646\n",
      "[[0.80796 0.80446 0.83538 0.78992 0.88051 0.76123 0.92974 0.89641 0.89597 0.91616\n",
      "   0.9297 0.74385 0.91445 0.80975 0.79636 0.86032 0.91677 0.87208 0.91206 0.86773]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 20:17 - loss: 0.0649\n",
      "[[0.82326 0.82254 0.85642 0.81236 0.89868   0.779 0.94742 0.91367 0.91466 0.93454\n",
      "  0.94769 0.76409 0.93184 0.83093 0.81613 0.87772  0.9348 0.89035 0.92983 0.88688]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 19:42 - loss: 0.0658\n",
      "[[0.84545 0.84428 0.87664 0.82992 0.91735 0.80122 0.96469 0.93259 0.93264   0.952\n",
      "  0.96411 0.78501 0.95016 0.84924 0.83384 0.89516 0.95241 0.90878 0.94772 0.90484]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 19:14 - loss: 0.0663\n",
      "[[0.86463 0.86919 0.89556 0.84651 0.93599 0.81813 0.98368 0.95137 0.95158 0.97083\n",
      "  0.98249 0.80273 0.96874 0.86687 0.85212 0.91712 0.97165 0.92422 0.96651 0.92095]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 19:02 - loss: 0.0665\n",
      "[[0.88424 0.88845 0.91424 0.86722 0.95435 0.83882 0.99803 0.96878  0.9684 0.98718\n",
      "  0.99811 0.82498 0.98522 0.88657 0.87003 0.93067 0.98754 0.94484 0.98324 0.94113]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 19:01 - loss: 0.0669\n",
      "[[0.89903  0.9102 0.92973 0.88246  0.9704 0.85851  1.0141 0.98515 0.98509  1.0026\n",
      "   1.0138 0.84079  1.0007 0.90351 0.88895 0.94713  1.0036 0.95964 0.99905 0.95601]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 18:55 - loss: 0.0664\n",
      "[[ 0.9183 0.93117 0.94927 0.90242 0.98905  0.8768  1.0339  1.0032  1.0037  1.0213\n",
      "   1.0326 0.86028  1.0201  0.9228  0.9075 0.96753  1.0227 0.97716  1.0173 0.97459]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 18:41 - loss: 0.0664\n",
      "[[  0.932 0.95016 0.97086 0.92301  1.0084 0.89657  1.0517  1.0213  1.0225    1.04\n",
      "   1.0499 0.88124  1.0381 0.94252 0.92707 0.98389  1.0408 0.99536  1.0363 0.99298]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 18:25 - loss: 0.0657\n",
      "[[0.95095 0.97035 0.98557 0.94025  1.0258 0.91843  1.0674  1.0391  1.0392  1.0562\n",
      "   1.0666  0.9002  1.0553 0.95964 0.94644  1.0015  1.0572  1.0147  1.0523  1.0114]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 17:47 - loss: 0.0649\n",
      "[[0.97132 0.99289  1.0031 0.95799  1.0439 0.93638  1.0872  1.0577  1.0586  1.0746\n",
      "   1.0861 0.91941  1.0741 0.97773 0.96291  1.0193  1.0769  1.0339   1.071  1.0309]]\n",
      "\n",
      " 544/2048 [======>.......................] - ETA: 17:08 - loss: 0.0650\n",
      "[[0.99703  1.0157  1.0314 0.98374   1.066 0.96239  1.1075  1.0787  1.0791  1.0961\n",
      "    1.107 0.94233  1.0945 0.99973 0.98751  1.0433  1.0967  1.0547  1.0922  1.0523]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 16:37 - loss: 0.0652\n",
      "[[ 1.0154  1.0372  1.0525  1.0019  1.0879  0.9815  1.1307  1.1015  1.1021  1.1193\n",
      "   1.1302 0.96126  1.1177  1.0198   1.006  1.0629  1.1192  1.0742  1.1153  1.0717]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 16:12 - loss: 0.0652\n",
      "[[ 1.0379  1.0599  1.0687  1.0234  1.1068  1.0086  1.1487  1.1203  1.1207  1.1371\n",
      "   1.1482 0.98598  1.1365  1.0413  1.0304  1.0833  1.1388  1.0954  1.1341  1.0928]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 15:49 - loss: 0.0653\n",
      "[[ 1.0597   1.078  1.0894  1.0389   1.127  1.0241  1.1689  1.1406  1.1408  1.1573\n",
      "   1.1674 0.99879  1.1567  1.0542   1.047  1.1086  1.1588  1.1099   1.154   1.108]]\n",
      "\n",
      " 672/2048 [========>.....................] - ETA: 15:28 - loss: 0.0648\n",
      "[[ 1.0779  1.0976  1.1112  1.0599  1.1452  1.0466  1.1858  1.1579  1.1587  1.1746\n",
      "   1.1842  1.0188  1.1731  1.0745   1.067  1.1259  1.1748  1.1307  1.1714  1.1277]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 15:06 - loss: 0.0647\n",
      "[[  1.097  1.1182  1.1316  1.0779  1.1659  1.0677  1.2041  1.1788   1.178  1.1943\n",
      "   1.2038  1.0396   1.193  1.0962  1.0885  1.1448   1.195  1.1526   1.191   1.149]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 14:36 - loss: 0.0643\n",
      "[[ 1.1191  1.1401  1.1533  1.0968  1.1875  1.0851  1.2248  1.1989  1.1985  1.2151\n",
      "   1.2238  1.0608  1.2099  1.1144  1.1068  1.1637  1.2153  1.1698  1.2115  1.1666]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 14:09 - loss: 0.0641\n",
      "[[  1.141  1.1612  1.1718  1.1235  1.2081  1.1111  1.2436  1.2185  1.2186   1.235\n",
      "   1.2435   1.087  1.2228  1.1404  1.1321  1.1831   1.225  1.1952  1.2311  1.1908]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 13:46 - loss: 0.0636\n",
      "[[ 1.1577  1.1798  1.1895    1.14  1.2266  1.1314  1.2614  1.2376  1.2365  1.2523\n",
      "   1.2608  1.1082  1.2354  1.1561    1.15  1.2021  1.2382  1.2122  1.2487  1.2084]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 13:25 - loss: 0.0659\n",
      "[[ 1.1743   1.207     1.2  1.1451  1.2301  1.1366  1.2764  1.2429   1.252  1.2537\n",
      "   1.2769  1.1196  1.2434  1.1943  1.1654  1.2149  1.2879  1.2401  1.2442  1.2095]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 13:05 - loss: 0.0655\n",
      "[[ 1.1903  1.2201  1.2106   1.173    1.25  1.1628  1.2839  1.2605  1.2652  1.2719\n",
      "   1.2879  1.1404  1.2564  1.1935  1.1839  1.2313  1.2708  1.2479  1.2733  1.2394]]\n",
      "\n",
      " 896/2048 [============>.................] - ETA: 12:48 - loss: 0.0649\n",
      "[[ 1.2108  1.2373  1.2325  1.1914  1.2714  1.1835   1.303  1.2806   1.285  1.2943\n",
      "   1.3057  1.1583  1.2724   1.207  1.2035  1.2499  1.2792   1.266  1.2933  1.2558]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 12:26 - loss: 0.0644\n",
      "[[ 1.2268  1.2603  1.2509  1.2088  1.2912  1.2018  1.3231  1.3005  1.3047  1.3141\n",
      "   1.3262  1.1814   1.293  1.2284  1.2233  1.2653  1.2989  1.2859   1.312  1.2768]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 12:01 - loss: 0.0640\n",
      "[[ 1.2482  1.2768  1.2724  1.2316  1.3114  1.2219   1.345  1.3223  1.3255  1.3339\n",
      "   1.3469  1.2005  1.3086  1.2453  1.2411  1.2832  1.3134  1.3057  1.3322  1.2987]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:41 - loss: 0.0636\n",
      "[[ 1.2697  1.2996  1.2976  1.2561  1.3337  1.2445  1.3655  1.3422  1.3456  1.3555\n",
      "    1.366  1.2249  1.3281  1.2723   1.264  1.3067  1.3335  1.3251  1.3535  1.3184]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 11:20 - loss: 0.0633\n",
      "[[ 1.2844  1.3179  1.3122  1.2736  1.3529  1.2669  1.3846  1.3634  1.3652  1.3749\n",
      "   1.3857  1.2449  1.3448  1.2905  1.2844  1.3255  1.3498  1.3456  1.3736  1.3388]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 10:59 - loss: 0.0630\n",
      "[[ 1.3074  1.3384  1.3377  1.2975   1.371  1.2839  1.4034  1.3819  1.3838  1.3928\n",
      "    1.404  1.2666  1.3651  1.3129  1.3043  1.3455  1.3694  1.3658   1.391  1.3593]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:39 - loss: 0.0626\n",
      "[[  1.329  1.3583  1.3558  1.3134  1.3919  1.3037  1.4236  1.4024  1.4038  1.4131\n",
      "   1.4237  1.2836  1.3851  1.3275   1.325  1.3646  1.3872  1.3835  1.4109  1.3784]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 10:19 - loss: 0.0624\n",
      "[[ 1.3464  1.3732  1.3771  1.3352  1.4103  1.3212  1.4425  1.4222   1.423  1.4311\n",
      "   1.4422  1.2984  1.3977  1.3426  1.3422  1.3829  1.3998   1.401  1.4298  1.3957]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 9:56 - loss: 0.0622\n",
      "[[ 1.3613  1.3895   1.393  1.3543  1.4267  1.3409  1.4588   1.439  1.4411  1.4479\n",
      "    1.459  1.3169  1.4122  1.3611  1.3613  1.4016  1.4133  1.4172  1.4476  1.4135]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:33 - loss: 0.0617\n",
      "[[ 1.3876  1.4118   1.414  1.3733  1.4473  1.3612  1.4775  1.4586  1.4598  1.4675\n",
      "   1.4782  1.3384  1.4341  1.3824  1.3822  1.4232  1.4348  1.4368  1.4671  1.4326]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 9:13 - loss: 0.0614\n",
      "[[ 1.4088  1.4271  1.4332  1.3929   1.464  1.3822  1.4935  1.4748  1.4759  1.4833\n",
      "   1.4935  1.3588   1.447   1.399  1.4027  1.4413  1.4481  1.4534  1.4823  1.4507]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 8:54 - loss: 0.0611\n",
      "[[  1.422  1.4455  1.4489  1.4094  1.4818  1.4004   1.507  1.4904  1.4911  1.4998\n",
      "   1.5066  1.3821  1.4631  1.4191  1.4176  1.4543  1.4651  1.4695  1.4979  1.4664]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:33 - loss: 0.0607\n",
      "[[  1.441  1.4635  1.4652  1.4291  1.5002   1.422  1.5253  1.5104  1.5094   1.518\n",
      "   1.5248  1.4023   1.483  1.4395  1.4391  1.4727  1.4838  1.4896  1.5168  1.4849]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 8:12 - loss: 0.0602\n",
      "[[ 1.4624  1.4804  1.4854  1.4503  1.5173  1.4409  1.5434  1.5275  1.5274   1.535\n",
      "   1.5443  1.4189  1.4996  1.4585  1.4599  1.4925     1.5  1.5094  1.5338  1.5057]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 7:50 - loss: 0.0597\n",
      "[[ 1.4825  1.5021  1.5062    1.47  1.5396  1.4585  1.5656  1.5492  1.5491  1.5572\n",
      "   1.5661  1.4394  1.5195  1.4768  1.4776  1.5107  1.5208  1.5265  1.5555  1.5232]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:27 - loss: 0.0591\n",
      "[[ 1.5022    1.52  1.5259  1.4902  1.5572  1.4785   1.583  1.5671  1.5672  1.5743\n",
      "   1.5826  1.4603  1.5366  1.4952  1.4966  1.5269  1.5375  1.5444  1.5726   1.543]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 7:06 - loss: 0.0584\n",
      "[[ 1.5234   1.541   1.543  1.5085  1.5763  1.5002  1.6006  1.5857  1.5853  1.5927\n",
      "   1.6001  1.4826  1.5582  1.5152  1.5178  1.5468  1.5576  1.5623  1.5916  1.5611]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:46 - loss: 0.0580\n",
      "[[ 1.5397  1.5577  1.5598  1.5286  1.5916  1.5204   1.615  1.5999  1.6007  1.6066\n",
      "   1.6146  1.5036  1.5743   1.536  1.5365  1.5655  1.5738  1.5808  1.6061  1.5788]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:25 - loss: 0.0576\n",
      "[[ 1.5546  1.5735  1.5741  1.5443  1.6091   1.538   1.632  1.6178  1.6183  1.6242\n",
      "   1.6317  1.5185  1.5892  1.5496  1.5538  1.5809  1.5894   1.598  1.6236  1.5954]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 6:05 - loss: 0.0572\n",
      "[[  1.575  1.5923  1.5909  1.5608  1.6269  1.5602   1.648   1.635   1.635  1.6413\n",
      "   1.6481   1.541   1.608  1.5699   1.575  1.5999  1.6079   1.616  1.6414  1.6145]]\n",
      "\n",
      "1536/2048 [=====================>........] - ETA: 5:43 - loss: 0.0567\n",
      "[[ 1.5958  1.6145  1.6136  1.5828  1.6453  1.5781  1.6664  1.6526  1.6531  1.6592\n",
      "   1.6661   1.561  1.6292  1.5912  1.5934  1.6188  1.6295  1.6337  1.6589  1.6333]]\n",
      "\n",
      "1568/2048 [=====================>........] - ETA: 5:20 - loss: 0.0567\n",
      "[[ 1.6105  1.6318  1.6309  1.5975  1.6662  1.5935  1.6861  1.6733  1.6728  1.6792\n",
      "   1.6856  1.5751  1.6474  1.6035  1.6088  1.6385  1.6479  1.6492  1.6791  1.6476]]\n",
      "\n",
      "1600/2048 [======================>.......] - ETA: 4:58 - loss: 0.0569\n",
      "[[ 1.6265  1.6494    1.65  1.6169  1.6805  1.6121  1.7011  1.6891  1.6883  1.6939\n",
      "   1.7012  1.5963  1.6646  1.6214   1.624  1.6509  1.6645   1.668  1.6932  1.6661]]\n",
      "\n",
      "1632/2048 [======================>.......] - ETA: 4:37 - loss: 0.0572\n",
      "[[ 1.6474  1.6679    1.67  1.6374  1.6994  1.6308  1.7212  1.7069  1.7088  1.7138\n",
      "   1.7216   1.611  1.6817  1.6397   1.645  1.6752  1.6809  1.6861  1.7127  1.6854]]\n",
      "\n",
      "1664/2048 [=======================>......] - ETA: 4:16 - loss: 0.0571\n",
      "[[ 1.6637  1.6868  1.6931  1.6627  1.7203  1.6545  1.7398  1.7275  1.7273  1.7333\n",
      "    1.739  1.6317  1.7002  1.6625   1.667  1.6949  1.7009  1.7079  1.7324  1.7062]]\n",
      "\n",
      "1696/2048 [=======================>......] - ETA: 3:54 - loss: 0.0570\n",
      "[[ 1.6834  1.7072  1.7058  1.6761  1.7388  1.6714  1.7578  1.7467  1.7459  1.7517\n",
      "   1.7584  1.6516  1.7211  1.6782  1.6851  1.7119   1.721  1.7247  1.7512  1.7221]]\n",
      "\n",
      "1728/2048 [========================>.....] - ETA: 3:34 - loss: 0.0570\n",
      "[[ 1.6946  1.7277  1.7226  1.6926  1.7573  1.6864  1.7775   1.766   1.767   1.772\n",
      "   1.7782   1.669  1.7408  1.6988  1.7008  1.7255  1.7403   1.743  1.7712   1.741]]\n",
      "\n",
      "1760/2048 [========================>.....] - ETA: 3:11 - loss: 0.0570\n",
      "[[  1.714  1.7439  1.7397  1.7066  1.7734     1.7   1.793  1.7813  1.7814  1.7868\n",
      "   1.7937  1.6829  1.7567  1.7129  1.7178  1.7453  1.7569  1.7541  1.7852  1.7531]]\n",
      "\n",
      "1792/2048 [=========================>....] - ETA: 2:49 - loss: 0.0566\n",
      "[[ 1.7297  1.7609  1.7576  1.7267  1.7889  1.7212  1.8062   1.795  1.7959  1.8011\n",
      "   1.8072  1.7053  1.7739  1.7325  1.7368  1.7618  1.7735  1.7721  1.7997  1.7709]]\n",
      "\n",
      "1824/2048 [=========================>....] - ETA: 2:28 - loss: 0.0561\n",
      "[[   1.75  1.7783  1.7723  1.7429  1.8041  1.7394  1.8206  1.8112  1.8113  1.8161\n",
      "   1.8219  1.7235  1.7911  1.7513  1.7545  1.7784  1.7907  1.7879  1.8147  1.7865]]\n",
      "\n",
      "1856/2048 [==========================>...] - ETA: 2:07 - loss: 0.0555\n",
      "[[ 1.7672  1.7961  1.7908  1.7642  1.8218  1.7553  1.8372   1.828  1.8286  1.8334\n",
      "   1.8383  1.7426  1.8065  1.7697  1.7715  1.7941   1.807  1.8061   1.832  1.8056]]\n",
      "\n",
      "1888/2048 [==========================>...] - ETA: 1:46 - loss: 0.0550\n",
      "[[ 1.7868  1.8116  1.8095  1.7812  1.8375   1.771   1.853  1.8436  1.8445  1.8488\n",
      "    1.853  1.7588  1.8228  1.7848  1.7885  1.8116  1.8232  1.8214  1.8482  1.8211]]\n",
      "\n",
      "1920/2048 [===========================>..] - ETA: 1:24 - loss: 0.0546\n",
      "[[ 1.8055  1.8283  1.8249  1.7973  1.8539  1.7924  1.8692  1.8605  1.8606  1.8646\n",
      "   1.8695  1.7787  1.8402  1.8016  1.8068  1.8278    1.84  1.8399  1.8643  1.8378]]\n",
      "\n",
      "1952/2048 [===========================>..] - ETA: 1:03 - loss: 0.0541\n",
      "[[ 1.8219  1.8426  1.8417   1.814  1.8679  1.8076   1.883  1.8736  1.8749  1.8786\n",
      "   1.8836  1.7946  1.8532  1.8175  1.8214  1.8426   1.853  1.8537  1.8775  1.8518]]\n",
      "\n",
      "1984/2048 [============================>.] - ETA: 42s - loss: 0.0536\n",
      "[[ 1.8372  1.8592  1.8563   1.831  1.8861  1.8259  1.8992  1.8914  1.8909  1.8955\n",
      "   1.8995  1.8117  1.8702  1.8339  1.8407  1.8583  1.8695  1.8718  1.8947  1.8702]]\n",
      "\n",
      "2016/2048 [============================>.] - ETA: 21s - loss: 0.0531\n",
      "[[ 1.8542  1.8741  1.8708  1.8466   1.899  1.8439  1.9113   1.904  1.9038  1.9082\n",
      "   1.9123  1.8292  1.8836  1.8496   1.858  1.8728  1.8835  1.8868  1.9074  1.8857]]\n",
      "\n",
      "2048/2048 [==============================] - ETA: 0s - loss: 0.0528\n",
      "[[ 1.8665  1.8899  1.8863  1.8634  1.9148  1.8577   1.928  1.9211  1.9197   1.924\n",
      "   1.9279  1.8457  1.9001  1.8658  1.8716  1.8868  1.8991  1.9013  1.9232  1.9001]]\n",
      "\n",
      "2048/2048 [==============================] - 1349s 659ms/step - loss: 0.0528\n",
      "Epoch 3/1000\n",
      "  32/2048 [..............................] - ETA: 21:19 - loss: 0.0414\n",
      "[[ 1.8887  1.9081   1.903   1.878  1.9322  1.8738   1.946  1.9375  1.9377  1.9418\n",
      "   1.9465  1.8618  1.9171  1.8798   1.888  1.9048   1.918  1.9163  1.9404  1.9143]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 21:40 - loss: 0.0406\n",
      "[[ 1.9047  1.9237  1.9222  1.8948  1.9491  1.8908  1.9618  1.9537  1.9545  1.9586\n",
      "   1.9624   1.875  1.9329   1.898  1.9045  1.9258  1.9334  1.9312  1.9564  1.9301]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 21:52 - loss: 0.0373\n",
      "[[ 1.9175  1.9411  1.9366  1.9095  1.9624  1.9062  1.9749  1.9681  1.9681  1.9715\n",
      "   1.9747  1.8899  1.9504  1.9157  1.9203  1.9371  1.9505  1.9468  1.9702  1.9458]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 20:43 - loss: 0.0341\n",
      "[[  1.933  1.9558  1.9517  1.9259  1.9767  1.9226  1.9893  1.9822  1.9822  1.9851\n",
      "   1.9886  1.9095  1.9656   1.931  1.9345  1.9544  1.9654  1.9599  1.9843  1.9595]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 19:54 - loss: 0.0321\n",
      "[[ 1.9501    1.97  1.9653   1.944  1.9896  1.9418  1.9999  1.9941  1.9935   1.997\n",
      "   1.9993  1.9306  1.9794  1.9501  1.9531  1.9687   1.979  1.9767  1.9959  1.9762]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 19:40 - loss: 0.0315\n",
      "[[ 1.9645  1.9854  1.9777   1.956  2.0062  1.9564  2.0174  2.0112  2.0107  2.0137\n",
      "    2.017  1.9433  1.9948  1.9619  1.9686  1.9832   1.995  1.9933   2.013  1.9927]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 19:09 - loss: 0.0308\n",
      "[[ 1.9796  2.0016  1.9908  1.9705    2.02  1.9738  2.0322  2.0253  2.0254   2.028\n",
      "   2.0312    1.96  2.0097  1.9782  1.9844  1.9976    2.01  2.0076  2.0271  2.0068]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 18:45 - loss: 0.0298\n",
      "[[ 1.9931   2.016  2.0105  1.9897  2.0361  1.9877  2.0469  2.0406  2.0403   2.044\n",
      "   2.0466  1.9738  2.0245  1.9944  1.9996  2.0148  2.0247  2.0219  2.0432  2.0213]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 18:42 - loss: 0.0288\n",
      "[[ 2.0085  2.0304  2.0232  2.0054  2.0506  2.0039  2.0605  2.0548  2.0539  2.0575\n",
      "   2.0595  1.9913  2.0385   2.009  2.0151  2.0264  2.0382  2.0386  2.0567  2.0373]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 18:22 - loss: 0.0281\n",
      "[[ 2.0226  2.0453  2.0412  2.0213  2.0666  2.0162  2.0758  2.0698  2.0694  2.0726\n",
      "   2.0755  2.0035  2.0533  2.0259  2.0291  2.0456  2.0532  2.0521  2.0724  2.0511]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 17:46 - loss: 0.0272\n",
      "[[ 2.0387  2.0592  2.0541  2.0349  2.0797  2.0314  2.0876  2.0825  2.0826  2.0855\n",
      "   2.0881   2.019  2.0654  2.0384  2.0424  2.0559  2.0654  2.0662  2.0852  2.0653]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 17:20 - loss: 0.0265\n",
      "[[ 2.0544  2.0735  2.0673  2.0473   2.093  2.0457  2.1021  2.0969   2.097  2.0991\n",
      "   2.1021  2.0345  2.0798  2.0519  2.0557  2.0699  2.0802  2.0784  2.0989  2.0784]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 17:06 - loss: 0.0259\n",
      "[[  2.067   2.089  2.0792  2.0606  2.1061  2.0591  2.1149  2.1091  2.1095  2.1121\n",
      "   2.1146  2.0512  2.0963  2.0672  2.0706  2.0832   2.096  2.0913  2.1117  2.0912]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 16:47 - loss: 0.0255\n",
      "[[ 2.0799  2.1037  2.0941  2.0777  2.1224  2.0766  2.1309  2.1253  2.1253  2.1283\n",
      "   2.1301  2.0687  2.1104   2.084   2.087  2.0991  2.1107  2.1091   2.128  2.1088]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 16:34 - loss: 0.0251\n",
      "[[ 2.0931  2.1142  2.1055  2.0879  2.1342  2.0907  2.1434  2.1384  2.1385  2.1402\n",
      "   2.1428  2.0785  2.1205  2.0924  2.1007  2.1127  2.1202  2.1203  2.1403  2.1193]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 16:15 - loss: 0.0251\n",
      "[[ 2.1048  2.1292  2.1201  2.1037  2.1494  2.1042  2.1586  2.1538  2.1533  2.1549\n",
      "   2.1577  2.0935  2.1364  2.1079   2.114  2.1245   2.136  2.1362  2.1555  2.1353]]\n",
      "\n",
      " 544/2048 [======>.......................] - ETA: 15:47 - loss: 0.0256\n",
      "[[ 2.1249  2.1459  2.1385  2.1209  2.1661  2.1253  2.1749  2.1701  2.1686  2.1714\n",
      "   2.1736  2.1113  2.1544   2.124  2.1345  2.1449   2.153   2.153  2.1717  2.1515]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 15:21 - loss: 0.0265\n",
      "[[ 2.1394  2.1626  2.1537   2.133  2.1806  2.1355  2.1896   2.185   2.184  2.1868\n",
      "   2.1896  2.1208  2.1706  2.1352   2.146  2.1571  2.1698  2.1646  2.1864  2.1631]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 15:05 - loss: 0.0275\n",
      "[[ 2.1578   2.178  2.1661  2.1449  2.1953  2.1491  2.2049  2.1986  2.1984  2.2011\n",
      "   2.2046  2.1356  2.1858   2.145  2.1576  2.1711   2.185  2.1741  2.2003  2.1733]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 14:49 - loss: 0.0278\n",
      "[[ 2.1712  2.1868  2.1772   2.157  2.2085  2.1609  2.2158  2.2104  2.2105  2.2138\n",
      "   2.2164  2.1462  2.1933  2.1559  2.1715  2.1846  2.1936  2.1871  2.2125  2.1857]]\n",
      "\n",
      " 672/2048 [========>.....................] - ETA: 14:30 - loss: 0.0276\n",
      "[[ 2.1829  2.1988   2.188  2.1678  2.2212  2.1747  2.2272  2.2233  2.2228  2.2257\n",
      "   2.2276  2.1595  2.2048  2.1673  2.1844  2.1974   2.205  2.2002  2.2244  2.1987]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 14:13 - loss: 0.0276\n",
      "[[ 2.1916  2.2108  2.1987  2.1811  2.2335  2.1867  2.2403  2.2364  2.2363   2.239\n",
      "   2.2405  2.1722  2.2161  2.1831  2.1983  2.2077  2.2159  2.2173  2.2372  2.2154]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 13:52 - loss: 0.0277\n",
      "[[ 2.2045  2.2258  2.2184  2.1986  2.2481  2.2011  2.2541  2.2496  2.2491   2.253\n",
      "   2.2537   2.186  2.2307  2.2001  2.2106  2.2233  2.2313  2.2283  2.2514  2.2276]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 13:34 - loss: 0.0276\n",
      "[[ 2.2173  2.2389  2.2327   2.215  2.2599  2.2133  2.2644  2.2609  2.2605  2.2632\n",
      "   2.2639  2.2027  2.2446  2.2178  2.2241  2.2369  2.2445  2.2424  2.2629  2.2412]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 13:17 - loss: 0.0275\n",
      "[[ 2.2262   2.245  2.2389  2.2212   2.266  2.2199  2.2728  2.2681   2.269  2.2705\n",
      "   2.2723  2.2105  2.2501  2.2232  2.2284  2.2439  2.2501  2.2487    2.27  2.2495]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 12:53 - loss: 0.0277\n",
      "[[   2.24  2.2571  2.2519  2.2359  2.2782  2.2356  2.2834  2.2797    2.28   2.282\n",
      "   2.2828  2.2253  2.2623  2.2342  2.2435  2.2559  2.2619  2.2616  2.2814  2.2613]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 12:28 - loss: 0.0284\n",
      "[[ 2.2472  2.2682  2.2585  2.2449  2.2896  2.2452  2.2922  2.2899  2.2895  2.2916\n",
      "   2.2917  2.2348  2.2728  2.2409  2.2536  2.2593  2.2724  2.2726  2.2911  2.2706]]\n",
      "\n",
      " 896/2048 [============>.................] - ETA: 12:08 - loss: 0.0288\n",
      "[[   2.26  2.2821  2.2681  2.2508  2.3008  2.2539  2.3036  2.3015  2.3004  2.3034\n",
      "   2.3026   2.246  2.2876  2.2497  2.2606   2.268  2.2862  2.2811  2.3025  2.2796]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 11:52 - loss: 0.0292\n",
      "[[ 2.2616  2.2948  2.2744  2.2564  2.3137  2.2587  2.3182  2.3154  2.3152  2.3172\n",
      "   2.3178  2.2508  2.2989  2.2583  2.2685  2.2754  2.2979  2.2928  2.3165   2.291]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 11:34 - loss: 0.0292\n",
      "[[ 2.2754  2.2996  2.2839  2.2673  2.3217  2.2664  2.3264   2.324  2.3241  2.3252\n",
      "   2.3265  2.2545  2.3027  2.2661  2.2798   2.285  2.3028  2.3016  2.3243  2.3006]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:16 - loss: 0.0294\n",
      "[[ 2.2892  2.3125  2.2978  2.2809  2.3323   2.282  2.3374  2.3351  2.3349  2.3365\n",
      "   2.3379  2.2695  2.3161  2.2809  2.2926  2.2986  2.3169  2.3122  2.3357  2.3117]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 10:56 - loss: 0.0293\n",
      "[[ 2.3022  2.3233  2.3107  2.2961   2.343   2.296  2.3476  2.3455  2.3452  2.3466\n",
      "   2.3472  2.2856  2.3259   2.295  2.3066  2.3115   2.327  2.3267  2.3466  2.3249]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 10:37 - loss: 0.0292\n",
      "[[ 2.3085  2.3283  2.3203  2.3077  2.3503  2.3041  2.3543  2.3516  2.3518   2.353\n",
      "   2.3542  2.2983  2.3311  2.3065  2.3153  2.3196  2.3321  2.3367  2.3531   2.335]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:11 - loss: 0.0290\n",
      "[[ 2.3187   2.338  2.3306  2.3158   2.358  2.3105  2.3609  2.3592  2.3585  2.3602\n",
      "   2.3603  2.3041  2.3413  2.3138  2.3229  2.3287  2.3409  2.3412  2.3601  2.3396]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 9:49 - loss: 0.0289\n",
      "[[  2.325   2.346  2.3406  2.3241  2.3671  2.3164  2.3702  2.3689  2.3679  2.3687\n",
      "   2.3694   2.309  2.3492  2.3201  2.3289  2.3371  2.3494  2.3477  2.3692   2.346]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 9:29 - loss: 0.0287\n",
      "[[ 2.3328  2.3541  2.3467   2.333  2.3757  2.3261  2.3807  2.3783  2.3778  2.3784\n",
      "   2.3798  2.3189   2.357  2.3293  2.3393   2.345  2.3576  2.3596  2.3787  2.3582]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:10 - loss: 0.0285\n",
      "[[ 2.3466  2.3655  2.3561  2.3426  2.3885  2.3398  2.3921  2.3891   2.389  2.3907\n",
      "   2.3914  2.3288  2.3681  2.3395  2.3531  2.3587  2.3687  2.3709  2.3906  2.3688]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 8:51 - loss: 0.0282\n",
      "[[ 2.3615  2.3757  2.3684  2.3535  2.3976  2.3537  2.4014  2.3983  2.3983  2.3994\n",
      "   2.4009  2.3435  2.3788  2.3502  2.3658  2.3707  2.3792  2.3802  2.3993   2.379]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 8:31 - loss: 0.0278\n",
      "[[ 2.3754  2.3856  2.3809  2.3649  2.4054   2.367  2.4078  2.4058  2.4055  2.4073\n",
      "    2.407  2.3594  2.3878  2.3631  2.3766  2.3813  2.3879  2.3884  2.4062  2.3882]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:11 - loss: 0.0274\n",
      "[[ 2.3812  2.3972    2.39  2.3769  2.4169  2.3798  2.4192   2.418  2.4167  2.4185\n",
      "   2.4185  2.3737  2.4009  2.3778  2.3871  2.3895  2.3999  2.4021  2.4182  2.4015]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 7:47 - loss: 0.0270\n",
      "[[ 2.3926  2.4055  2.4021  2.3889  2.4248  2.3908  2.4279  2.4263  2.4251  2.4268\n",
      "   2.4277  2.3815  2.4091  2.3886  2.3991  2.4037  2.4082  2.4125  2.4261  2.4119]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 7:25 - loss: 0.0267\n",
      "[[ 2.4049  2.4196  2.4142  2.4016  2.4356  2.4019  2.4392  2.4374  2.4366  2.4378\n",
      "   2.4388  2.3959  2.4225  2.4016  2.4099  2.4134  2.4223  2.4229  2.4375  2.4224]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:05 - loss: 0.0264\n",
      "[[ 2.4137  2.4285  2.4245  2.4139  2.4447  2.4111  2.4478  2.4462   2.446  2.4466\n",
      "   2.4474  2.4064  2.4314  2.4126  2.4191  2.4238  2.4307  2.4317  2.4465  2.4331]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 6:45 - loss: 0.0261\n",
      "[[ 2.4267   2.439  2.4356  2.4236  2.4555  2.4213  2.4578  2.4567  2.4555  2.4569\n",
      "   2.4579  2.4164  2.4422  2.4221  2.4292  2.4334  2.4414  2.4416  2.4564  2.4418]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:26 - loss: 0.0258\n",
      "[[ 2.4306  2.4455  2.4409  2.4295  2.4626  2.4275  2.4654  2.4638  2.4628   2.464\n",
      "   2.4655  2.4205   2.449  2.4285  2.4355  2.4392  2.4481  2.4494  2.4628  2.4497]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:07 - loss: 0.0256\n",
      "[[ 2.4382  2.4522   2.447  2.4354  2.4687  2.4367  2.4716  2.4698  2.4702  2.4708\n",
      "   2.4726  2.4307   2.455  2.4349  2.4439  2.4474  2.4545  2.4573  2.4703   2.457]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 5:46 - loss: 0.0254\n",
      "[[ 2.4466  2.4605  2.4534  2.4407  2.4748  2.4445  2.4771  2.4753  2.4758   2.477\n",
      "   2.4784  2.4376  2.4633  2.4414  2.4526  2.4536  2.4634  2.4635  2.4758  2.4636]]\n",
      "\n",
      "1536/2048 [=====================>........] - ETA: 5:24 - loss: 0.0251\n",
      "[[  2.454  2.4686  2.4605  2.4493  2.4835    2.45  2.4858  2.4843  2.4845  2.4853\n",
      "   2.4865  2.4445  2.4724  2.4501  2.4603  2.4631   2.472  2.4713  2.4851   2.471]]\n",
      "\n",
      "1568/2048 [=====================>........] - ETA: 5:04 - loss: 0.0249\n",
      "[[ 2.4593  2.4758  2.4686  2.4584  2.4908  2.4565  2.4928  2.4924  2.4919  2.4923\n",
      "   2.4936  2.4501  2.4792  2.4559  2.4657  2.4683  2.4788  2.4788  2.4924  2.4776]]\n",
      "\n",
      "1600/2048 [======================>.......] - ETA: 4:43 - loss: 0.0249\n",
      "[[ 2.4683  2.4879  2.4771  2.4636  2.5023  2.4681  2.5039  2.5032  2.5027  2.5031\n",
      "   2.5044  2.4612  2.4916  2.4642  2.4742  2.4791  2.4914  2.4858  2.5031  2.4865]]\n",
      "\n",
      "1632/2048 [======================>.......] - ETA: 4:23 - loss: 0.0250\n",
      "[[ 2.4714  2.4928  2.4801  2.4664   2.506  2.4731  2.5078  2.5056  2.5073  2.5075\n",
      "   2.5079  2.4649  2.4943  2.4684   2.478  2.4855  2.4946  2.4906  2.5076  2.4906]]\n",
      "\n",
      "1664/2048 [=======================>......] - ETA: 4:04 - loss: 0.0251\n",
      "[[ 2.4743  2.5015  2.4905   2.474  2.5135  2.4769  2.5156  2.5138  2.5148  2.5152\n",
      "   2.5165  2.4671   2.502  2.4755  2.4821  2.4911  2.5032  2.4966  2.5156   2.496]]\n",
      "\n",
      "1696/2048 [=======================>......] - ETA: 3:44 - loss: 0.0252\n",
      "[[ 2.4823  2.5106  2.4948    2.48  2.5254  2.4838  2.5275   2.526  2.5271  2.5268\n",
      "   2.5286  2.4753  2.5114  2.4824  2.4926  2.4965  2.5119  2.5075  2.5275  2.5078]]\n",
      "\n",
      "1728/2048 [========================>.....] - ETA: 3:23 - loss: 0.0255\n",
      "[[ 2.4884  2.5146  2.4994  2.4845  2.5315  2.4894  2.5339  2.5324  2.5335   2.534\n",
      "   2.5342  2.4825  2.5152  2.4869  2.4943  2.5016  2.5154  2.5122  2.5339  2.5106]]\n",
      "\n",
      "1760/2048 [========================>.....] - ETA: 3:03 - loss: 0.0257\n",
      "[[ 2.4926  2.5194  2.5046  2.4907  2.5392  2.4894  2.5411  2.5385  2.5404  2.5398\n",
      "   2.5402  2.4901  2.5223  2.4934   2.498  2.5022  2.5217  2.5195  2.5407  2.5191]]\n",
      "\n",
      "1792/2048 [=========================>....] - ETA: 2:42 - loss: 0.0259\n",
      "[[ 2.5031  2.5266  2.5137  2.4966  2.5488  2.5004  2.5509  2.5487  2.5514  2.5495\n",
      "   2.5505  2.4964  2.5326  2.4996   2.507  2.5159  2.5315  2.5263  2.5509  2.5255]]\n",
      "\n",
      "1824/2048 [=========================>....] - ETA: 2:22 - loss: 0.0262\n",
      "[[   2.51  2.5353  2.5171  2.5031  2.5559    2.51  2.5573  2.5567  2.5565  2.5565\n",
      "   2.5573  2.5028  2.5379  2.5078   2.516   2.523   2.537  2.5341  2.5575  2.5329]]\n",
      "\n",
      "1856/2048 [==========================>...] - ETA: 2:02 - loss: 0.0265\n",
      "[[ 2.5112  2.5322  2.5219  2.5098   2.558  2.5067  2.5578  2.5586  2.5579  2.5578\n",
      "   2.5588  2.5014  2.5377  2.5066  2.5163  2.5174  2.5365  2.5376  2.5591  2.5377]]\n",
      "\n",
      "1888/2048 [==========================>...] - ETA: 1:42 - loss: 0.0269\n",
      "[[ 2.5218  2.5384  2.5338  2.5184  2.5724  2.5133   2.574  2.5742  2.5744  2.5734\n",
      "   2.5749  2.5066  2.5497  2.5165  2.5292  2.5317  2.5502  2.5495  2.5751  2.5491]]\n",
      "\n",
      "1920/2048 [===========================>..] - ETA: 1:21 - loss: 0.0275\n",
      "[[ 2.5381  2.5456  2.5395  2.5326  2.5727  2.5306   2.571  2.5731  2.5725  2.5716\n",
      "   2.5728  2.5259  2.5549  2.5301  2.5429  2.5414  2.5548   2.558   2.573  2.5545]]\n",
      "\n",
      "1952/2048 [===========================>..] - ETA: 1:01 - loss: 0.0282\n",
      "[[ 2.5432  2.5486  2.5477  2.5389  2.5694  2.5365  2.5692  2.5712  2.5695   2.569\n",
      "   2.5707  2.5339  2.5566  2.5363  2.5454  2.5464  2.5562  2.5587  2.5699  2.5579]]\n",
      "\n",
      "1984/2048 [============================>.] - ETA: 40s - loss: 0.0290\n",
      "[[ 2.5471   2.553  2.5533  2.5403  2.5749   2.536  2.5735  2.5752  2.5734  2.5735\n",
      "   2.5752  2.5365  2.5633  2.5405  2.5446  2.5556  2.5633  2.5537  2.5742  2.5545]]\n",
      "\n",
      "2016/2048 [============================>.] - ETA: 20s - loss: 0.0298\n",
      "[[ 2.5409  2.5494  2.5434  2.5309  2.5806  2.5378  2.5824  2.5838  2.5816  2.5828\n",
      "   2.5829   2.533  2.5684  2.5406  2.5503  2.5484  2.5688  2.5646   2.583   2.565]]\n",
      "\n",
      "2048/2048 [==============================] - ETA: 0s - loss: 0.0304\n",
      "[[ 2.5397    2.55  2.5491  2.5363  2.5873  2.5403  2.5919  2.5932  2.5901  2.5919\n",
      "   2.5911  2.5349  2.5698  2.5398   2.553  2.5518    2.57  2.5725  2.5919  2.5731]]\n",
      "\n",
      "2048/2048 [==============================] - 1302s 636ms/step - loss: 0.0304\n",
      "Epoch 4/1000\n",
      "  32/2048 [..............................] - ETA: 23:01 - loss: 0.0687\n",
      "[[ 2.5422  2.5481  2.5526  2.5322  2.5819  2.5374  2.5955  2.5964  2.5951  2.5969\n",
      "   2.5973  2.5315  2.5717  2.5367  2.5522  2.5578  2.5718  2.5699   2.595  2.5707]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 21:50 - loss: 0.0755\n",
      "[[  2.549  2.5547  2.5613  2.5401  2.5799  2.5367   2.601   2.602  2.5997  2.6025\n",
      "   2.6018  2.5293  2.5761  2.5424  2.5546  2.5626   2.576  2.5732  2.6008  2.5728]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 20:13 - loss: 0.0665\n",
      "[[ 2.5544  2.5658   2.575   2.548  2.5786  2.5464  2.6034  2.6047  2.6033  2.6053\n",
      "   2.6034  2.5317  2.5826  2.5503   2.558  2.5721  2.5831  2.5734  2.6025  2.5738]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 19:21 - loss: 0.0595\n",
      "[[ 2.5561  2.5662  2.5718  2.5511  2.5815  2.5519   2.604  2.6042  2.6032  2.6052\n",
      "   2.6038  2.5383  2.5826  2.5491  2.5631  2.5695   2.584  2.5765  2.6034  2.5767]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 19:29 - loss: 0.0530\n",
      "[[ 2.5608  2.5678  2.5752  2.5595   2.587  2.5559  2.6058  2.6062  2.6049  2.6061\n",
      "   2.6048  2.5448  2.5845  2.5583  2.5721  2.5752  2.5867  2.5844  2.6059  2.5838]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 19:29 - loss: 0.0485\n",
      "[[ 2.5666  2.5707  2.5758  2.5605  2.5873  2.5621  2.6086   2.609  2.6079  2.6085\n",
      "   2.6081  2.5515  2.5891  2.5609  2.5754  2.5794  2.5907   2.586  2.6084  2.5864]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 19:18 - loss: 0.0443\n",
      "[[  2.576  2.5791  2.5802  2.5674  2.5917  2.5717  2.6113  2.6115  2.6109  2.6114\n",
      "   2.6111  2.5636  2.5943   2.567  2.5831  2.5841  2.5956  2.5913  2.6111  2.5913]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 19:04 - loss: 0.0409\n",
      "[[ 2.5808  2.5843  2.5874  2.5747  2.5939  2.5746  2.6141  2.6139  2.6134  2.6144\n",
      "   2.6137  2.5672  2.5965  2.5715   2.585  2.5876  2.5976  2.5941  2.6139  2.5946]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 18:28 - loss: 0.0381\n",
      "[[ 2.5863  2.5887  2.5927  2.5797  2.5986  2.5799   2.618   2.618   2.618  2.6191\n",
      "    2.618  2.5727  2.6003  2.5759  2.5884  2.5937   2.601  2.5983  2.6182  2.5988]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 17:52 - loss: 0.0356\n",
      "[[ 2.5868  2.5918  2.5937  2.5838  2.6024  2.5833   2.618  2.6182  2.6182  2.6188\n",
      "   2.6185  2.5766  2.6036  2.5824  2.5928  2.5961  2.6043  2.6032  2.6188  2.6023]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 17:32 - loss: 0.0333\n",
      "[[ 2.5915  2.5969   2.599  2.5882  2.6039  2.5858  2.6191  2.6197  2.6197  2.6194\n",
      "   2.6195  2.5824  2.6077  2.5885  2.5942  2.5994   2.608   2.605  2.6202  2.6045]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 17:15 - loss: 0.0312\n",
      "[[ 2.5922  2.5978  2.5984  2.5902  2.6069  2.5919  2.6194  2.6197  2.6197  2.6195\n",
      "   2.6195  2.5865  2.6088  2.5918  2.5985  2.6007  2.6088  2.6082  2.6203  2.6079]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 17:09 - loss: 0.0295\n",
      "[[ 2.5969  2.6028  2.6031  2.5943    2.61  2.5961  2.6233  2.6234  2.6232  2.6233\n",
      "   2.6235  2.5915  2.6128  2.5942  2.6013  2.6035  2.6125  2.6108  2.6239  2.6105]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 16:56 - loss: 0.0280\n",
      "[[ 2.5991   2.606  2.6062  2.5973  2.6121  2.5995  2.6251  2.6254  2.6251  2.6251\n",
      "   2.6252  2.5944  2.6151  2.5971  2.6032  2.6054  2.6149   2.613  2.6251  2.6126]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 16:31 - loss: 0.0266\n",
      "[[ 2.5997  2.6057  2.6066  2.6001  2.6154  2.6032  2.6272  2.6274  2.6272  2.6274\n",
      "   2.6273  2.5959  2.6154  2.5984  2.6074  2.6083  2.6149  2.6161  2.6276  2.6156]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 15:58 - loss: 0.0254\n",
      "[[ 2.6026  2.6098    2.61  2.6023  2.6166  2.6058    2.63  2.6301  2.6303  2.6301\n",
      "   2.6303  2.5993  2.6194  2.6019  2.6089  2.6127  2.6188  2.6172  2.6302   2.617]]\n",
      "\n",
      " 544/2048 [======>.......................] - ETA: 15:34 - loss: 0.0244\n",
      "[[ 2.6083  2.6135  2.6136  2.6056  2.6204  2.6123  2.6333  2.6329  2.6335  2.6329\n",
      "   2.6331  2.6044  2.6234  2.6067  2.6154  2.6191   2.623  2.6212  2.6334  2.6208]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 15:23 - loss: 0.0236\n",
      "[[ 2.6112  2.6182  2.6179  2.6091  2.6223  2.6115  2.6362  2.6361  2.6364  2.6361\n",
      "   2.6359   2.605  2.6269  2.6084  2.6157  2.6189  2.6265  2.6231  2.6361   2.623]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 15:07 - loss: 0.0232\n",
      "[[ 2.6155  2.6187  2.6198    2.61   2.623  2.6126  2.6393   2.639  2.6394  2.6393\n",
      "   2.6395  2.6048  2.6293  2.6094  2.6169  2.6224  2.6281   2.624  2.6387   2.624]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 14:54 - loss: 0.0228\n",
      "[[ 2.6155  2.6175  2.6197  2.6095  2.6274  2.6135  2.6429  2.6431  2.6431  2.6433\n",
      "   2.6434  2.6038   2.628  2.6089  2.6196  2.6229  2.6271  2.6279  2.6428  2.6279]]\n",
      "\n",
      " 672/2048 [========>.....................] - ETA: 14:43 - loss: 0.0227\n",
      "[[ 2.6196  2.6212  2.6231  2.6107  2.6295  2.6177  2.6487  2.6484  2.6481  2.6483\n",
      "   2.6488  2.6101   2.634  2.6124  2.6219  2.6289  2.6332   2.631  2.6478    2.63]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 14:27 - loss: 0.0237\n",
      "[[ 2.6204   2.619  2.6246  2.6131  2.6372  2.6183  2.6541  2.6538  2.6535   2.654\n",
      "   2.6536  2.6115  2.6338  2.6126  2.6278  2.6275  2.6321  2.6384  2.6535  2.6373]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 14:01 - loss: 0.0244\n",
      "[[  2.617  2.6253  2.6312  2.6148  2.6402  2.6153  2.6602  2.6612  2.6596  2.6612\n",
      "   2.6613  2.6098  2.6401  2.6177  2.6245  2.6314  2.6389  2.6395  2.6606  2.6404]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 13:37 - loss: 0.0256\n",
      "[[ 2.6208  2.6301  2.6349  2.6174  2.6493  2.6189  2.6722  2.6728  2.6712  2.6722\n",
      "    2.672  2.6083  2.6485  2.6201  2.6292  2.6342  2.6485  2.6507  2.6723  2.6505]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 13:20 - loss: 0.0284\n",
      "[[ 2.6112  2.6195  2.6258  2.6092  2.6498  2.6163  2.6743  2.6631  2.6736  2.6726\n",
      "   2.6728  2.6033  2.6463  2.6163  2.6271  2.6318  2.6477  2.6505  2.6737  2.6498]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 13:02 - loss: 0.0308\n",
      "[[ 2.6194  2.6289  2.6319  2.6115  2.6453  2.6192  2.6755  2.6649  2.6741  2.6728\n",
      "   2.6726  2.6129  2.6504  2.6151  2.6248  2.6322  2.6506  2.6442  2.6736  2.6433]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 12:42 - loss: 0.0331\n",
      "[[ 2.6169  2.6386  2.6344  2.6191  2.6563  2.6178  2.6822   2.667  2.6822  2.6803\n",
      "     2.68  2.6143  2.6586  2.6157  2.6307  2.6295  2.6589  2.6545  2.6815  2.6527]]\n",
      "\n",
      " 896/2048 [============>.................] - ETA: 12:24 - loss: 0.0347\n",
      "[[ 2.6193  2.6429  2.6355  2.6149  2.6531  2.6187  2.6816   2.666  2.6817  2.6812\n",
      "   2.6795  2.6121  2.6646  2.6162  2.6306  2.6335  2.6626  2.6533  2.6812  2.6515]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 12:02 - loss: 0.0362\n",
      "[[ 2.6099  2.6351  2.6293  2.6068  2.6523  2.6091  2.6866  2.6607  2.6865  2.6862\n",
      "   2.6846  2.6019  2.6603  2.6078  2.6234  2.6301  2.6601  2.6522  2.6864  2.6507]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 11:38 - loss: 0.0367\n",
      "[[ 2.6268  2.6375  2.6358  2.6166  2.6535  2.6128  2.6834  2.6621  2.6831  2.6823\n",
      "   2.6815  2.6041  2.6569   2.613  2.6307  2.6321  2.6579  2.6528  2.6825  2.6519]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:16 - loss: 0.0371\n",
      "[[ 2.6297  2.6413  2.6385  2.6199  2.6529  2.6189  2.6848  2.6627  2.6847  2.6849\n",
      "   2.6847  2.6133  2.6629  2.6199  2.6334  2.6384  2.6632  2.6539  2.6832  2.6536]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 10:55 - loss: 0.0369\n",
      "[[ 2.6286  2.6442  2.6399  2.6264  2.6563  2.6223  2.6808  2.6614  2.6808  2.6817\n",
      "   2.6807  2.6167  2.6626   2.625   2.637  2.6374  2.6621  2.6559  2.6805  2.6557]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 10:33 - loss: 0.0372\n",
      "[[ 2.6321  2.6412  2.6412  2.6278  2.6547  2.6246  2.6766    2.66  2.6767  2.6777\n",
      "   2.6771  2.6208  2.6552  2.6236  2.6357   2.638  2.6551  2.6538  2.6765  2.6535]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:14 - loss: 0.0373\n",
      "[[ 2.6345  2.6426  2.6468    2.63  2.6529  2.6257  2.6801   2.662  2.6794  2.6793\n",
      "   2.6791  2.6152  2.6569  2.6254  2.6372  2.6453  2.6561  2.6519  2.6784  2.6527]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 9:55 - loss: 0.0371\n",
      "[[ 2.6358   2.646  2.6478  2.6297  2.6537  2.6269  2.6805  2.6635  2.6791  2.6789\n",
      "   2.6794  2.6202  2.6615  2.6288  2.6382  2.6452  2.6604  2.6539  2.6785  2.6542]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 9:33 - loss: 0.0365\n",
      "[[ 2.6384  2.6471  2.6484  2.6358  2.6601   2.634  2.6811  2.6642  2.6794  2.6798\n",
      "   2.6805  2.6288  2.6621  2.6334   2.645  2.6468  2.6625  2.6598  2.6796    2.66]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:11 - loss: 0.0358\n",
      "[[  2.645  2.6535  2.6524    2.64  2.6629  2.6407  2.6815  2.6671  2.6807  2.6804\n",
      "   2.6807  2.6351  2.6665  2.6397  2.6508   2.653  2.6663  2.6627  2.6808   2.663]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 8:52 - loss: 0.0350\n",
      "[[ 2.6515  2.6585  2.6568  2.6444  2.6643  2.6465  2.6821  2.6696  2.6817  2.6809\n",
      "   2.6818  2.6414  2.6697  2.6446  2.6556   2.659  2.6691  2.6644  2.6811  2.6649]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 8:33 - loss: 0.0343\n",
      "[[ 2.6554   2.661  2.6608  2.6502  2.6662  2.6505  2.6816  2.6713  2.6813  2.6811\n",
      "   2.6815  2.6449  2.6706  2.6496  2.6593  2.6622    2.67  2.6662  2.6813  2.6663]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:13 - loss: 0.0336\n",
      "[[ 2.6574  2.6643  2.6642  2.6561  2.6706  2.6559  2.6827  2.6735   2.682   2.682\n",
      "   2.6823   2.654  2.6729  2.6563  2.6624  2.6631  2.6724   2.671  2.6823  2.6709]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 7:55 - loss: 0.0329\n",
      "[[  2.659  2.6628  2.6652   2.659  2.6714  2.6598  2.6805  2.6714  2.6802  2.6799\n",
      "   2.6802  2.6569  2.6713  2.6589  2.6652   2.666  2.6709  2.6719  2.6806  2.6718]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 7:36 - loss: 0.0323\n",
      "[[ 2.6631  2.6672  2.6694  2.6624  2.6737  2.6623  2.6841   2.675  2.6838  2.6836\n",
      "   2.6839  2.6599   2.675  2.6622  2.6675    2.67  2.6749  2.6737  2.6843  2.6735]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:14 - loss: 0.0317\n",
      "[[ 2.6634  2.6665  2.6701  2.6641  2.6756  2.6631  2.6849  2.6754  2.6844  2.6846\n",
      "   2.6847   2.659  2.6745  2.6629  2.6695   2.672  2.6744  2.6752  2.6848  2.6758]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 6:52 - loss: 0.0311\n",
      "[[ 2.6656  2.6683  2.6717   2.666  2.6782  2.6657  2.6864  2.6775  2.6858  2.6862\n",
      "    2.686  2.6618  2.6757  2.6642  2.6711  2.6724  2.6758  2.6778   2.686   2.678]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:31 - loss: 0.0305\n",
      "[[ 2.6679  2.6712  2.6737  2.6674  2.6784  2.6663  2.6872  2.6785  2.6868  2.6874\n",
      "   2.6869  2.6631  2.6784  2.6674  2.6721  2.6746  2.6782  2.6778  2.6872   2.678]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:12 - loss: 0.0300\n",
      "[[ 2.6691  2.6738  2.6742  2.6672  2.6778  2.6693  2.6872  2.6794  2.6872  2.6875\n",
      "    2.687  2.6662  2.6799  2.6685  2.6725   2.675  2.6798  2.6772  2.6874  2.6775]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 5:52 - loss: 0.0294\n",
      "[[ 2.6699  2.6735  2.6732  2.6684    2.68  2.6723  2.6867  2.6785  2.6869  2.6866\n",
      "   2.6863  2.6681  2.6796  2.6695  2.6756  2.6752  2.6799  2.6798  2.6868  2.6799]]\n",
      "\n",
      "1536/2048 [=====================>........] - ETA: 5:32 - loss: 0.0290\n",
      "[[ 2.6728   2.677  2.6752  2.6693  2.6811  2.6733  2.6898   2.681    2.69  2.6898\n",
      "   2.6894  2.6696  2.6837  2.6706  2.6767  2.6764  2.6834  2.6813  2.6903  2.6811]]\n",
      "\n",
      "1568/2048 [=====================>........] - ETA: 5:12 - loss: 0.0286\n",
      "[[ 2.6737   2.677  2.6755  2.6675  2.6804  2.6719   2.692  2.6827  2.6918  2.6919\n",
      "   2.6918  2.6664  2.6839  2.6676   2.677  2.6768  2.6836  2.6806  2.6919  2.6804]]\n",
      "\n",
      "1600/2048 [======================>.......] - ETA: 4:51 - loss: 0.0283\n",
      "[[ 2.6748  2.6802  2.6779   2.668  2.6842  2.6745   2.697  2.6856  2.6976  2.6965\n",
      "   2.6971  2.6701  2.6884  2.6704  2.6782  2.6804  2.6886  2.6841  2.6965  2.6843]]\n",
      "\n",
      "1632/2048 [======================>.......] - ETA: 4:30 - loss: 0.0280\n",
      "[[ 2.6759  2.6752  2.6799  2.6725  2.6847  2.6765  2.6943  2.6859  2.6951  2.6942\n",
      "   2.6944   2.666   2.681  2.6684  2.6807  2.6815   2.681  2.6845  2.6945  2.6845]]\n",
      "\n",
      "1664/2048 [=======================>......] - ETA: 4:09 - loss: 0.0277\n",
      "[[ 2.6733  2.6799  2.6831    2.67   2.686  2.6726  2.7008  2.6888  2.7003  2.7008\n",
      "   2.7014  2.6652  2.6893  2.6707  2.6775  2.6833   2.689  2.6854  2.7006  2.6862]]\n",
      "\n",
      "1696/2048 [=======================>......] - ETA: 3:49 - loss: 0.0275\n",
      "[[ 2.6746  2.6822  2.6829  2.6742  2.6919  2.6765  2.7057  2.6916  2.7051  2.7049\n",
      "   2.7051  2.6719  2.6929  2.6745  2.6814  2.6839   2.693  2.6904  2.7052  2.6915]]\n",
      "\n",
      "1728/2048 [========================>.....] - ETA: 3:28 - loss: 0.0275\n",
      "[[ 2.6745  2.6813  2.6828  2.6733  2.6941  2.6746  2.7096  2.6925  2.7082  2.7083\n",
      "   2.7091  2.6696  2.6938  2.6729  2.6807   2.686  2.6946  2.6913  2.7083  2.6924]]\n",
      "\n",
      "1760/2048 [========================>.....] - ETA: 3:08 - loss: 0.0276\n",
      "[[ 2.6732  2.6814  2.6813  2.6684  2.6923  2.6684   2.712  2.6947  2.7119  2.7107\n",
      "   2.7114  2.6661   2.696   2.669  2.6766  2.6826  2.6957  2.6894  2.7119  2.6918]]\n",
      "\n",
      "1792/2048 [=========================>....] - ETA: 2:47 - loss: 0.0278\n",
      "[[ 2.6755   2.682  2.6817  2.6685  2.6955  2.6744  2.7159  2.6974   2.717  2.7153\n",
      "   2.7164  2.6649  2.6994  2.6693  2.6843  2.6875  2.6978  2.6949   2.717  2.6952]]\n",
      "\n",
      "1824/2048 [=========================>....] - ETA: 2:26 - loss: 0.0286\n",
      "[[ 2.6786  2.6856  2.6845  2.6725  2.6995  2.6769  2.7211  2.6996  2.7215  2.7201\n",
      "    2.721  2.6668  2.7033  2.6749  2.6885  2.6914  2.7026  2.6995  2.7215  2.7008]]\n",
      "\n",
      "1856/2048 [==========================>...] - ETA: 2:05 - loss: 0.0305\n",
      "[[ 2.6687  2.6825  2.6799  2.6744  2.7014  2.6731  2.7171  2.6992  2.7191   2.718\n",
      "   2.7175  2.6609  2.6993  2.6744  2.6844  2.6832  2.6975  2.7007  2.7187  2.7019]]\n",
      "\n",
      "1888/2048 [==========================>...] - ETA: 1:44 - loss: 0.0333\n",
      "[[  2.674  2.6931  2.6801  2.6773  2.7039  2.6747  2.7296  2.7036  2.7295  2.7297\n",
      "   2.7288  2.6737  2.7128  2.6827  2.6851  2.6902  2.7116  2.7037  2.7286  2.7058]]\n",
      "\n",
      "1920/2048 [===========================>..] - ETA: 1:23 - loss: 0.0354\n",
      "[[ 2.6668  2.6923  2.6786  2.6712  2.7114  2.6704  2.7386  2.7119  2.7393    2.74\n",
      "   2.7395   2.657  2.7148  2.6692  2.6866  2.6843  2.7133  2.7121  2.7397  2.7101]]\n",
      "\n",
      "1952/2048 [===========================>..] - ETA: 1:02 - loss: 0.0365\n",
      "[[ 2.6806  2.6941  2.6886  2.6735  2.7077  2.6785  2.7386  2.7199  2.7401  2.7408\n",
      "   2.7377  2.6592  2.7117   2.668  2.6852  2.6913  2.7115  2.7064  2.7385  2.7054]]\n",
      "\n",
      "1984/2048 [============================>.] - ETA: 41s - loss: 0.0370\n",
      "[[ 2.6845  2.6899  2.6894  2.6761   2.708  2.6827  2.7362  2.7147   2.739  2.7382\n",
      "   2.7377  2.6683  2.7113  2.6689   2.687  2.6936  2.7117  2.7087  2.7368  2.7083]]\n",
      "\n",
      "2016/2048 [============================>.] - ETA: 20s - loss: 0.0372\n",
      "[[ 2.6841  2.6906  2.6886  2.6766  2.7131  2.6801  2.7373  2.7135   2.738  2.7378\n",
      "   2.7373  2.6677  2.7117  2.6725  2.6911  2.6917  2.7124  2.7131  2.7381  2.7128]]\n",
      "\n",
      "2048/2048 [==============================] - ETA: 0s - loss: 0.0374\n",
      "[[ 2.6829  2.6905  2.6947  2.6814  2.7129  2.6805  2.7354  2.7134  2.7352   2.735\n",
      "   2.7348  2.6726  2.7106  2.6777  2.6913  2.6952  2.7104  2.7119  2.7352  2.7115]]\n",
      "\n",
      "2048/2048 [==============================] - 1346s 657ms/step - loss: 0.0374\n",
      "Epoch 5/1000\n",
      "  32/2048 [..............................] - ETA: 18:20 - loss: 0.0289\n",
      "[[ 2.6858  2.6927  2.6946  2.6825  2.7133  2.6817  2.7324  2.7106  2.7311  2.7325\n",
      "   2.7316  2.6759  2.7124  2.6824  2.6938  2.6974   2.712  2.7114  2.7312  2.7111]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 19:17 - loss: 0.0245\n",
      "[[ 2.6898  2.6996  2.6995  2.6843  2.7155  2.6875  2.7352  2.7138  2.7337  2.7352\n",
      "   2.7345  2.6796  2.7178  2.6875  2.6974  2.7018  2.7175  2.7138  2.7339  2.7138]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 20:23 - loss: 0.0216\n",
      "[[ 2.6913  2.7007  2.7001  2.6858  2.7136  2.6896   2.733  2.7138  2.7319  2.7327\n",
      "   2.7324  2.6831  2.7175  2.6888  2.6982  2.7018  2.7171  2.7129  2.7315  2.7122]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 20:34 - loss: 0.0190\n",
      "[[ 2.6958  2.7022  2.7031  2.6905  2.7129  2.6917  2.7295  2.7142  2.7287  2.7294\n",
      "   2.7288  2.6849  2.7153  2.6907  2.6998   2.703  2.7152  2.7122  2.7283   2.712]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 20:35 - loss: 0.0169\n",
      "[[ 2.6989  2.7016  2.7041  2.6927  2.7133   2.693  2.7282  2.7141  2.7278  2.7285\n",
      "   2.7279  2.6857  2.7134  2.6922  2.7022   2.705  2.7138   2.713  2.7274  2.7121]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 20:30 - loss: 0.0151\n",
      "[[ 2.6995  2.7014  2.7037  2.6939  2.7132  2.6953  2.7267  2.7124  2.7264  2.7269\n",
      "   2.7265  2.6902  2.7141  2.6949  2.7031  2.7056  2.7143  2.7129   2.726  2.7126]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 20:25 - loss: 0.0136\n",
      "[[ 2.6992  2.7027  2.7041  2.6954  2.7117  2.6961  2.7237  2.7124  2.7237  2.7239\n",
      "    2.724  2.6912  2.7127  2.6956   2.703  2.7042  2.7128  2.7117  2.7232  2.7115]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 20:00 - loss: 0.0125\n",
      "[[ 2.7011  2.7039  2.7059  2.6983  2.7115  2.6979  2.7222  2.7124  2.7223  2.7223\n",
      "   2.7224   2.694   2.712  2.6982  2.7039  2.7059  2.7123  2.7117   2.722  2.7117]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 19:12 - loss: 0.0116\n",
      "[[ 2.7008  2.7032   2.705  2.6991   2.711  2.6998  2.7196  2.7107    2.72  2.7197\n",
      "   2.7198  2.6961  2.7108   2.699  2.7046  2.7063   2.711  2.7112  2.7192  2.7114]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 18:59 - loss: 0.0108\n",
      "[[ 2.7021  2.7042  2.7073  2.7011  2.7121  2.7011  2.7205  2.7121  2.7211  2.7209\n",
      "   2.7211  2.6958  2.7112  2.7007  2.7067  2.7092  2.7116  2.7121  2.7207  2.7124]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 18:42 - loss: 0.0101\n",
      "[[ 2.7038  2.7061   2.709  2.7042  2.7138  2.7024  2.7205  2.7133  2.7207  2.7207\n",
      "    2.721  2.6987  2.7116  2.7037  2.7078  2.7093   2.712  2.7134  2.7205  2.7139]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 18:32 - loss: 0.0095\n",
      "[[ 2.7051  2.7088  2.7096  2.7051  2.7136  2.7042  2.7198  2.7138  2.7199  2.7201\n",
      "   2.7203   2.701  2.7133  2.7056  2.7085  2.7099  2.7138  2.7132    2.72  2.7135]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 18:15 - loss: 0.0090\n",
      "[[ 2.7055  2.7096  2.7098  2.7057  2.7135  2.7044  2.7198  2.7138  2.7201  2.7201\n",
      "   2.7202  2.7026  2.7141  2.7062  2.7083  2.7094  2.7143  2.7133  2.7197  2.7136]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 18:04 - loss: 0.0085\n",
      "[[ 2.7069  2.7102  2.7108  2.7076  2.7147  2.7069  2.7196   2.714    2.72    2.72\n",
      "   2.7202  2.7034  2.7139   2.708  2.7108  2.7118  2.7139  2.7147  2.7196  2.7148]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 17:38 - loss: 0.0081\n",
      "[[ 2.7086   2.711  2.7116  2.7076   2.714   2.708    2.72  2.7146  2.7202  2.7202\n",
      "   2.7203  2.7055  2.7145  2.7081  2.7102  2.7128  2.7149  2.7143    2.72  2.7144]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 17:12 - loss: 0.0079\n",
      "[[ 2.7054  2.7094  2.7102   2.707  2.7139  2.7074  2.7191  2.7131  2.7192  2.7192\n",
      "   2.7195  2.7042  2.7138  2.7076  2.7102  2.7121  2.7142   2.714  2.7191  2.7139]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 16:32 - loss: 0.0074\n",
      "[[ 2.7048  2.7089  2.7101  2.7058  2.7147  2.7053  2.7212  2.7136   2.721  2.7207\n",
      "    2.721  2.7025  2.7147  2.7067  2.7093  2.7105  2.7144  2.7146  2.7209  2.7145]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 16:11 - loss: 0.0073\n",
      "[[ 2.7055  2.7095  2.7108  2.7056  2.7133  2.7051  2.7219  2.7156   2.722  2.7221\n",
      "   2.7222  2.7006  2.7151  2.7052  2.7086    2.71  2.7153  2.7131  2.7219  2.7131]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 15:54 - loss: 0.0071\n",
      "[[ 2.7058  2.7095  2.7104  2.7041  2.7138  2.7053  2.7227  2.7157   2.723   2.723\n",
      "    2.723  2.7005  2.7156  2.7053  2.7092  2.7104  2.7161   2.714  2.7224  2.7143]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 15:13 - loss: 0.0071\n",
      "[[ 2.7041  2.7069  2.7099  2.7027  2.7149   2.704   2.725  2.7148  2.7248  2.7249\n",
      "   2.7251  2.6981  2.7151  2.7032   2.709  2.7113  2.7153  2.7147  2.7246  2.7147]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 14:45 - loss: 0.0072\n",
      "[[ 2.7048  2.7078  2.7094  2.7023  2.7139  2.7042  2.7239  2.7152  2.7241  2.7237\n",
      "   2.7243  2.6977  2.7152  2.7018  2.7085  2.7112  2.7151  2.7143  2.7235  2.7136]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 14:21 - loss: 0.0073\n",
      "[[  2.703  2.7072  2.7091  2.7026  2.7143  2.7038  2.7243  2.7147  2.7247  2.7236\n",
      "   2.7234   2.699   2.714  2.7036  2.7057   2.709  2.7142  2.7151  2.7233  2.7144]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 14:01 - loss: 0.0085\n",
      "[[ 2.6963  2.7007  2.7036  2.6944  2.7139  2.7001  2.7267  2.7127  2.7268   2.726\n",
      "   2.7262  2.6916  2.7119  2.6938  2.7024  2.7045  2.7108  2.7135  2.7271  2.7126]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 13:42 - loss: 0.0103\n",
      "[[ 2.6868  2.6953  2.6935  2.6838  2.7154  2.6916  2.7333  2.7089  2.7315  2.7311\n",
      "   2.7321  2.6862  2.7111  2.6861  2.6981  2.6986    2.71  2.7149  2.7339  2.7146]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 13:23 - loss: 0.0137\n",
      "[[ 2.6865  2.6949  2.6974  2.6859  2.7202  2.6863  2.7412  2.7131  2.7404  2.7398\n",
      "   2.7405  2.6841  2.7143  2.6861  2.6995  2.7022  2.7163  2.7175  2.7418  2.7187]]\n",
      "\n",
      " 896/2048 [============>.................] - ETA: 13:04 - loss: 0.0170\n",
      "[[ 2.6781  2.6933  2.6923  2.6773  2.7206  2.6796  2.7459  2.7162  2.7452  2.7482\n",
      "   2.7498  2.6666  2.7171  2.6748  2.6951  2.6971  2.7144    2.72  2.7494   2.718]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 12:40 - loss: 0.0195\n",
      "[[ 2.6651  2.6934  2.6914  2.6724  2.7123   2.658  2.7442  2.7103  2.7448  2.7462\n",
      "   2.7487  2.6631  2.7242  2.6731  2.6812  2.6884  2.7208   2.714  2.7469   2.713]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 12:17 - loss: 0.0218\n",
      "[[ 2.6735  2.6989  2.6969  2.6781   2.715  2.6718  2.7513  2.7187  2.7516  2.7519\n",
      "   2.7531  2.6649  2.7307  2.6768  2.6888   2.691  2.7262  2.7166  2.7522  2.7163]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:53 - loss: 0.0236\n",
      "[[   2.67  2.6888  2.6907  2.6683  2.7112  2.6678  2.7515  2.7143  2.7512  2.7522\n",
      "   2.7521  2.6591  2.7292    2.67  2.6856   2.688  2.7266  2.7126  2.7519  2.7123]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 11:35 - loss: 0.0253\n",
      "[[ 2.6696  2.6959  2.6964  2.6737  2.7129  2.6746  2.7514  2.7195  2.7516  2.7521\n",
      "   2.7522  2.6594  2.7269  2.6729  2.6865  2.6909  2.7263  2.7152    2.75  2.7148]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 11:14 - loss: 0.0279\n",
      "[[ 2.6847  2.6984  2.7078  2.6846  2.7172  2.6791  2.7582   2.726  2.7598   2.759\n",
      "   2.7595  2.6683  2.7269  2.6798  2.6921  2.7029  2.7263  2.7184  2.7601   2.719]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:53 - loss: 0.0307\n",
      "[[  2.686  2.6957  2.7025  2.6812  2.7261  2.6837  2.7638  2.7331  2.7647  2.7652\n",
      "   2.7662  2.6578  2.7266  2.6748   2.704  2.7048  2.7262  2.7292  2.7644  2.7287]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 10:33 - loss: 0.0331\n",
      "[[ 2.6853  2.7016  2.7133  2.6827  2.7228  2.6806  2.7687   2.739  2.7703  2.7686\n",
      "   2.7693  2.6581  2.7283  2.6701  2.6995   2.706  2.7272  2.7248  2.7683  2.7259]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 10:12 - loss: 0.0341\n",
      "[[ 2.6899  2.7021  2.7106  2.6893  2.7248  2.6815   2.763  2.7318  2.7655  2.7636\n",
      "   2.7639  2.6652  2.7274   2.673  2.7002  2.7064  2.7261  2.7272  2.7631   2.729]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:48 - loss: 0.0340\n",
      "[[ 2.6934   2.704  2.7062  2.6831  2.7205  2.6829  2.7595  2.7288  2.7609  2.7581\n",
      "   2.7587  2.6683  2.7298  2.6772  2.7006  2.7093  2.7291  2.7223  2.7587  2.7234]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 9:24 - loss: 0.0337\n",
      "[[ 2.6949   2.704  2.7084  2.6892  2.7215  2.6903  2.7542  2.7287  2.7543  2.7532\n",
      "   2.7537  2.6752  2.7264  2.6828  2.7036  2.7106  2.7248  2.7214  2.7528  2.7226]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 9:03 - loss: 0.0333\n",
      "[[ 2.7031  2.7101   2.713   2.695  2.7204  2.6927  2.7497  2.7302    2.75  2.7492\n",
      "   2.7491  2.6819  2.7263  2.6916  2.7061  2.7137  2.7256  2.7209  2.7485  2.7214]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:41 - loss: 0.0328\n",
      "[[ 2.7019  2.7102  2.7129  2.6982  2.7196  2.6967  2.7443   2.728  2.7449  2.7444\n",
      "   2.7446  2.6886  2.7261   2.696   2.708  2.7145  2.7244  2.7204  2.7442  2.7202]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 8:20 - loss: 0.0322\n",
      "[[ 2.7054  2.7108  2.7122  2.7015  2.7213  2.7034  2.7398  2.7239  2.7402  2.7399\n",
      "   2.7402  2.6975  2.7255  2.7014  2.7119   2.716  2.7245  2.7221  2.7401  2.7221]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 8:00 - loss: 0.0317\n",
      "[[ 2.7059  2.7121  2.7142  2.7049  2.7224  2.7047  2.7391   2.724  2.7393  2.7389\n",
      "   2.7395  2.7016  2.7262  2.7048  2.7108  2.7156  2.7256   2.723  2.7395  2.7227]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:39 - loss: 0.0312\n",
      "[[ 2.7069  2.7126  2.7156   2.707  2.7235  2.7055  2.7377  2.7247  2.7377   2.738\n",
      "   2.7379  2.7022  2.7241  2.7062  2.7121  2.7151  2.7239  2.7239  2.7381  2.7235]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 7:16 - loss: 0.0306\n",
      "[[ 2.7083  2.7142  2.7171  2.7073  2.7222  2.7061  2.7364  2.7253  2.7362  2.7364\n",
      "   2.7365  2.7034   2.724  2.7065  2.7118  2.7154  2.7237  2.7222  2.7365  2.7223]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:52 - loss: 0.0299\n",
      "[[ 2.7105  2.7165  2.7187   2.711  2.7228  2.7089  2.7346  2.7256  2.7346  2.7346\n",
      "   2.7348  2.7071  2.7242  2.7099  2.7137  2.7165  2.7236  2.7224  2.7345  2.7229]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:30 - loss: 0.0293\n",
      "[[ 2.7121  2.7161  2.7173  2.7111  2.7215  2.7119  2.7314  2.7233  2.7314  2.7314\n",
      "   2.7315  2.7085  2.7227  2.7109  2.7156  2.7176  2.7224  2.7216  2.7315  2.7219]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 6:10 - loss: 0.0288\n",
      "[[ 2.7137  2.7172   2.717  2.7119  2.7221  2.7143  2.7303  2.7231  2.7302  2.7303\n",
      "   2.7303  2.7109  2.7229  2.7123  2.7174  2.7178  2.7228  2.7221  2.7303  2.7222]]\n",
      "\n",
      "1536/2048 [=====================>........] - ETA: 5:48 - loss: 0.0282\n",
      "[[ 2.7148  2.7176   2.717  2.7119   2.722  2.7145  2.7294   2.723  2.7294  2.7296\n",
      "   2.7294  2.7124  2.7232  2.7125  2.7175  2.7174  2.7231  2.7222  2.7297  2.7222]]\n",
      "\n",
      "1568/2048 [=====================>........] - ETA: 5:27 - loss: 0.0277\n",
      "[[ 2.7176  2.7192  2.7192  2.7144   2.723   2.717  2.7297   2.724  2.7298  2.7297\n",
      "   2.7298  2.7147  2.7242  2.7144  2.7195    2.72  2.7243   2.723  2.7296  2.7228]]\n",
      "\n",
      "1600/2048 [======================>.......] - ETA: 5:06 - loss: 0.0271\n",
      "[[  2.718  2.7203    2.72  2.7155  2.7233  2.7174  2.7295  2.7242  2.7295  2.7292\n",
      "   2.7294  2.7159  2.7247  2.7157  2.7199  2.7206  2.7247  2.7232  2.7293   2.723]]\n",
      "\n",
      "1632/2048 [======================>.......] - ETA: 4:44 - loss: 0.0266\n",
      "[[  2.718  2.7197   2.721  2.7168  2.7228  2.7188  2.7285  2.7238  2.7284  2.7283\n",
      "   2.7285  2.7159  2.7236  2.7164  2.7203  2.7217  2.7236  2.7229  2.7284  2.7226]]\n",
      "\n",
      "1664/2048 [=======================>......] - ETA: 4:21 - loss: 0.0261\n",
      "[[ 2.7184  2.7206   2.722  2.7174  2.7223  2.7178  2.7283  2.7241  2.7281  2.7279\n",
      "   2.7282  2.7157  2.7239  2.7171  2.7197   2.722  2.7239  2.7225  2.7282  2.7224]]\n",
      "\n",
      "1696/2048 [=======================>......] - ETA: 3:58 - loss: 0.0257\n",
      "[[ 2.7183  2.7203   2.721  2.7174  2.7222  2.7177  2.7274  2.7233  2.7274   2.727\n",
      "   2.7272  2.7164  2.7236  2.7175  2.7199  2.7215  2.7238  2.7222  2.7276  2.7221]]\n",
      "\n",
      "1728/2048 [========================>.....] - ETA: 3:36 - loss: 0.0252\n",
      "[[ 2.7183  2.7198  2.7203   2.717  2.7222  2.7179  2.7268   2.723  2.7269  2.7264\n",
      "   2.7267  2.7168  2.7232   2.717  2.7195  2.7208  2.7232  2.7221  2.7268  2.7219]]\n",
      "\n",
      "1760/2048 [========================>.....] - ETA: 3:13 - loss: 0.0248\n",
      "[[ 2.7189  2.7197    2.72  2.7173  2.7223  2.7179  2.7268  2.7231  2.7271  2.7265\n",
      "   2.7267  2.7157  2.7225   2.717    2.72  2.7214  2.7227  2.7225  2.7267  2.7224]]\n",
      "\n",
      "1792/2048 [=========================>....] - ETA: 2:52 - loss: 0.0244\n",
      "[[ 2.7188  2.7197  2.7198  2.7175  2.7226  2.7181  2.7264  2.7229  2.7267  2.7263\n",
      "   2.7265   2.716  2.7225  2.7172  2.7207   2.721  2.7226  2.7227  2.7265  2.7227]]\n",
      "\n",
      "1824/2048 [=========================>....] - ETA: 2:31 - loss: 0.0239\n",
      "[[ 2.7188    2.72  2.7206  2.7184  2.7228   2.719  2.7261  2.7229  2.7263   2.726\n",
      "   2.7262  2.7162  2.7222  2.7176  2.7208  2.7213  2.7224  2.7227  2.7259  2.7228]]\n",
      "\n",
      "1856/2048 [==========================>...] - ETA: 2:09 - loss: 0.0235\n",
      "[[  2.719    2.72  2.7205  2.7189  2.7225  2.7182  2.7257  2.7227  2.7258  2.7256\n",
      "   2.7255  2.7159  2.7216  2.7177  2.7202  2.7203  2.7217  2.7226  2.7255  2.7228]]\n",
      "\n",
      "1888/2048 [==========================>...] - ETA: 1:48 - loss: 0.0232\n",
      "[[ 2.7185    2.72  2.7202  2.7182  2.7224  2.7172  2.7261  2.7227  2.7262   2.726\n",
      "    2.726  2.7152  2.7222  2.7176  2.7201  2.7203  2.7222  2.7227  2.7261  2.7227]]\n",
      "\n",
      "1920/2048 [===========================>..] - ETA: 1:26 - loss: 0.0228\n",
      "[[ 2.7189  2.7204  2.7203  2.7186  2.7231  2.7182  2.7264  2.7233  2.7267  2.7265\n",
      "   2.7266  2.7158  2.7228   2.718  2.7209  2.7205  2.7227  2.7234  2.7267  2.7234]]\n",
      "\n",
      "1952/2048 [===========================>..] - ETA: 1:04 - loss: 0.0225\n",
      "[[ 2.7162  2.7193  2.7189  2.7162  2.7223  2.7163  2.7268  2.7231  2.7269  2.7266\n",
      "   2.7268  2.7145  2.7224  2.7157  2.7186  2.7177  2.7223   2.722  2.7269  2.7223]]\n",
      "\n",
      "1984/2048 [============================>.] - ETA: 43s - loss: 0.0222\n",
      "[[ 2.7162  2.7172  2.7177  2.7131  2.7209  2.7143  2.7276  2.7231  2.7272  2.7274\n",
      "   2.7279  2.7106  2.7207  2.7113   2.717  2.7173  2.7203  2.7209  2.7278  2.7211]]\n",
      "\n",
      "2016/2048 [============================>.] - ETA: 21s - loss: 0.0220\n",
      "[[ 2.7137  2.7144  2.7173  2.7115   2.721  2.7126  2.7298  2.7211  2.7292  2.7294\n",
      "   2.7299  2.7103  2.7206  2.7114  2.7157  2.7181  2.7203  2.7213  2.7297  2.7213]]\n",
      "\n",
      "2048/2048 [==============================] - ETA: 0s - loss: 0.0218\n",
      "[[ 2.7107  2.7158  2.7158  2.7101  2.7227  2.7113  2.7326  2.7223  2.7323  2.7313\n",
      "   2.7313  2.7095  2.7227  2.7118  2.7162  2.7147  2.7223   2.723  2.7321  2.7236]]\n",
      "\n",
      "2048/2048 [==============================] - 1384s 676ms/step - loss: 0.0218\n",
      "Epoch 6/1000\n",
      "  32/2048 [..............................] - ETA: 19:55 - loss: 0.0176\n",
      "[[ 2.7177  2.7173   2.716  2.7068   2.723  2.7143  2.7362  2.7235  2.7359  2.7351\n",
      "   2.7351  2.7142  2.7275  2.7101  2.7171  2.7192  2.7273  2.7224  2.7352  2.7224]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 21:12 - loss: 0.0249\n",
      "[[  2.714  2.7196  2.7175  2.7055   2.728  2.7119  2.7452  2.7265  2.7445   2.744\n",
      "   2.7429  2.7083  2.7304  2.7098  2.7187  2.7209  2.7306  2.7276  2.7439  2.7274]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 21:48 - loss: 0.0293\n",
      "[[ 2.7085  2.7194  2.7183  2.7017  2.7272  2.7106  2.7464   2.728  2.7485  2.7476\n",
      "   2.7485  2.7047  2.7351  2.7111  2.7174  2.7211   2.733  2.7289  2.7476  2.7286]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 21:52 - loss: 0.0368\n",
      "[[ 2.7018  2.7109  2.7124  2.6987  2.7362  2.7031  2.7574  2.7296  2.7585  2.7559\n",
      "   2.7592  2.6925  2.7316  2.6998  2.7192  2.7174  2.7327   2.736  2.7587  2.7352]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 21:36 - loss: 0.0478\n",
      "[[ 2.6928  2.7091  2.7034  2.6901  2.7361  2.7002  2.7568  2.7271  2.7584  2.7567\n",
      "   2.7602  2.6903   2.728  2.6924  2.7153  2.7055  2.7304  2.7357  2.7561  2.7364]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 21:23 - loss: 0.0544\n",
      "[[ 2.7015  2.7099  2.7052   2.684  2.7326  2.7075  2.7677  2.7284  2.7685  2.7671\n",
      "   2.7689  2.6946  2.7349  2.6943  2.7164  2.7158   2.737  2.7325  2.7654  2.7334]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 21:04 - loss: 0.0575\n",
      "[[ 2.6959  2.7176  2.7084  2.6856   2.734  2.6992  2.7707  2.7375  2.7702  2.7717\n",
      "   2.7749  2.6809  2.7378  2.6887  2.7126  2.7075  2.7373  2.7346  2.7686  2.7363]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 20:43 - loss: 0.0593\n",
      "[[ 2.6947  2.7192  2.7087  2.6875   2.731  2.6942  2.7644   2.732   2.763  2.7647\n",
      "   2.7653  2.6835  2.7459  2.7006  2.7107  2.7111   2.746  2.7319  2.7621   2.731]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 20:03 - loss: 0.0616\n",
      "[[ 2.6912  2.7152  2.7101  2.6896  2.7297  2.6886  2.7635  2.7325  2.7653  2.7655\n",
      "    2.766    2.68  2.7429  2.7004  2.7078   2.713  2.7411  2.7324  2.7645  2.7313]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 19:12 - loss: 0.0632\n",
      "[[  2.694  2.7082  2.7166  2.6963  2.7274  2.6827  2.7616  2.7293  2.7629  2.7641\n",
      "   2.7646  2.6718  2.7351  2.7011  2.7035  2.7166  2.7351  2.7302  2.7627  2.7277]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 18:30 - loss: 0.0631\n",
      "[[  2.696  2.7103  2.7111   2.691   2.723  2.6877  2.7552  2.7249  2.7569  2.7554\n",
      "   2.7558  2.6793  2.7317  2.6952  2.7011  2.7099  2.7338  2.7219  2.7562  2.7213]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 18:17 - loss: 0.0615\n",
      "[[ 2.6891  2.6988  2.7024  2.6846  2.7215  2.6878  2.7515  2.7172  2.7524   2.752\n",
      "   2.7517  2.6742  2.7237  2.6864  2.7015  2.7074  2.7255  2.7217  2.7538  2.7218]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 18:01 - loss: 0.0599\n",
      "[[ 2.6857  2.7026  2.6998  2.6836  2.7238  2.6867  2.7503  2.7192  2.7494  2.7497\n",
      "   2.7504  2.6747  2.7248  2.6859  2.7022  2.7024   2.726  2.7238  2.7513  2.7234]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 17:43 - loss: 0.0578\n",
      "[[ 2.6885  2.7012  2.6998  2.6863  2.7221  2.6907   2.746  2.7168  2.7465  2.7466\n",
      "   2.7471  2.6832  2.7232   2.687   2.702  2.7025  2.7234  2.7227  2.7465  2.7218]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 17:24 - loss: 0.0558\n",
      "[[ 2.6874  2.6984  2.7002  2.6881  2.7193  2.6939  2.7446  2.7157  2.7448   2.745\n",
      "   2.7451  2.6806  2.7196  2.6887   2.704  2.7107   2.719  2.7202  2.7454  2.7202]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 17:07 - loss: 0.0543\n",
      "[[ 2.6919   2.706  2.7012  2.6884  2.7194  2.6959  2.7423  2.7189  2.7437  2.7428\n",
      "   2.7424  2.6853  2.7235  2.6911  2.7049  2.7076  2.7232  2.7208  2.7427  2.7208]]\n",
      "\n",
      " 544/2048 [======>.......................] - ETA: 16:51 - loss: 0.0525\n",
      "[[ 2.6931  2.7062  2.7021  2.6874  2.7169   2.699  2.7416   2.718  2.7419  2.7414\n",
      "   2.7413  2.6855   2.724  2.6908  2.7061  2.7104  2.7245  2.7176  2.7424  2.7176]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 16:31 - loss: 0.0514\n",
      "[[ 2.6981  2.7107  2.7103   2.696   2.721  2.6984  2.7438  2.7241  2.7445  2.7429\n",
      "   2.7441  2.6889  2.7247  2.6948  2.7058  2.7101  2.7244  2.7212  2.7448  2.7215]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 16:01 - loss: 0.0506\n",
      "[[ 2.7029  2.7096  2.7136  2.7005  2.7215   2.695  2.7445  2.7259  2.7447  2.7439\n",
      "   2.7447  2.6861  2.7232  2.6963   2.708  2.7096   2.724  2.7207  2.7449  2.7205]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 15:30 - loss: 0.0495\n",
      "[[ 2.7007  2.7018  2.7079  2.6928  2.7201  2.7003  2.7438   2.722   2.744  2.7434\n",
      "   2.7444  2.6808  2.7183  2.6908   2.714  2.7162  2.7178  2.7194  2.7454  2.7198]]\n",
      "\n",
      " 672/2048 [========>.....................] - ETA: 15:00 - loss: 0.0484\n",
      "[[ 2.7033  2.7018  2.7093    2.69  2.7206  2.6997  2.7473  2.7244  2.7477   2.747\n",
      "   2.7477  2.6854  2.7233  2.6905  2.7123  2.7179  2.7218  2.7213  2.7489  2.7208]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 14:42 - loss: 0.0478\n",
      "[[ 2.7015  2.6992  2.7073  2.6925  2.7235  2.6995  2.7451  2.7239  2.7462  2.7451\n",
      "   2.7448  2.6859  2.7187  2.6886  2.7131   2.711  2.7169   2.724  2.7464  2.7239]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 14:25 - loss: 0.0468\n",
      "[[ 2.7021  2.7027  2.7091  2.6944  2.7222  2.6975  2.7431  2.7226  2.7429  2.7426\n",
      "    2.743   2.687  2.7207  2.6927  2.7102  2.7109  2.7207   2.723   2.744  2.7227]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 14:07 - loss: 0.0461\n",
      "[[ 2.6985  2.7101  2.7136  2.7006  2.7271  2.7001  2.7462   2.726  2.7462  2.7463\n",
      "   2.7463  2.6897  2.7273  2.7012  2.7085  2.7107  2.7276   2.729  2.7464  2.7288]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 13:47 - loss: 0.0457\n",
      "[[ 2.7029  2.7092  2.7129  2.6995  2.7278  2.7018  2.7501  2.7265  2.7497  2.7494\n",
      "   2.7494  2.6902  2.7275  2.6988  2.7111  2.7139  2.7275  2.7287  2.7504  2.7288]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 13:27 - loss: 0.0454\n",
      "[[  2.704  2.7119  2.7164  2.7005  2.7238  2.7051  2.7477  2.7272  2.7465   2.746\n",
      "   2.7461  2.6928  2.7257   2.698  2.7086  2.7154   2.727  2.7235  2.7465  2.7246]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 13:08 - loss: 0.0451\n",
      "[[ 2.7057  2.7191   2.716  2.7031  2.7304  2.7035  2.7504  2.7335  2.7499  2.7498\n",
      "   2.7505  2.6956  2.7316  2.6958  2.7124  2.7133   2.731  2.7282  2.7498  2.7305]]\n",
      "\n",
      " 896/2048 [============>.................] - ETA: 12:50 - loss: 0.0448\n",
      "[[  2.705  2.7214  2.7153  2.6982   2.727  2.6972  2.7497  2.7331  2.7493  2.7493\n",
      "   2.7495  2.6924  2.7345  2.6954  2.7063  2.7123  2.7335  2.7266  2.7491  2.7282]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 12:25 - loss: 0.0446\n",
      "[[ 2.7019    2.72  2.7164  2.7006  2.7305  2.6998  2.7527  2.7335  2.7526  2.7522\n",
      "   2.7539   2.691  2.7382  2.6989  2.7105  2.7155  2.7359  2.7294  2.7527  2.7299]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 12:00 - loss: 0.0442\n",
      "[[ 2.7035  2.7201  2.7182  2.7031  2.7232  2.6951  2.7489   2.736  2.7474  2.7473\n",
      "   2.7488  2.6833   2.733  2.6997  2.7071  2.7142  2.7317  2.7239   2.748  2.7223]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:36 - loss: 0.0437\n",
      "[[ 2.7081  2.7214  2.7203  2.7086  2.7256  2.7004  2.7477  2.7361  2.7472  2.7473\n",
      "    2.749  2.6917  2.7338  2.7047  2.7127  2.7188  2.7323  2.7256  2.7476  2.7245]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 11:17 - loss: 0.0433\n",
      "[[ 2.7022  2.7173  2.7203  2.7078  2.7271  2.6952  2.7463  2.7338  2.7468  2.7471\n",
      "   2.7476   2.686  2.7292  2.7025  2.7107  2.7143  2.7281  2.7263  2.7465  2.7259]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 10:57 - loss: 0.0436\n",
      "[[ 2.6987  2.7111  2.7152  2.7029  2.7254  2.6892   2.745  2.7288  2.7448  2.7466\n",
      "   2.7459  2.6798   2.725  2.6988  2.7085  2.7117  2.7238  2.7246  2.7452  2.7242]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:36 - loss: 0.0439\n",
      "[[ 2.6961  2.7088  2.7123   2.695  2.7162  2.6859  2.7419  2.7248  2.7414  2.7428\n",
      "   2.7416  2.6739  2.7206  2.6897  2.6996  2.7083  2.7197  2.7152  2.7413  2.7156]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 10:16 - loss: 0.0440\n",
      "[[ 2.6907  2.7095  2.7066  2.6877  2.7179  2.6813   2.752  2.7289  2.7506   2.752\n",
      "   2.7515  2.6687  2.7269  2.6885  2.6972  2.7073  2.7262   2.718  2.7509  2.7188]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 9:55 - loss: 0.0439\n",
      "[[ 2.6905  2.7076  2.7109  2.6987  2.7244  2.6865  2.7501  2.7281  2.7483  2.7501\n",
      "   2.7494   2.676   2.722  2.6907  2.7005  2.7061  2.7219  2.7216  2.7499  2.7236]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:35 - loss: 0.0434\n",
      "[[ 2.6966  2.7108   2.711  2.6981  2.7263  2.6931  2.7522  2.7298  2.7517  2.7523\n",
      "   2.7523  2.6804  2.7261  2.6943  2.7062  2.7123  2.7256  2.7256  2.7521  2.7253]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 9:15 - loss: 0.0428\n",
      "[[ 2.7023  2.7136  2.7154  2.7015  2.7255   2.698  2.7496  2.7307  2.7496   2.749\n",
      "   2.7492  2.6833  2.7253   2.696  2.7115  2.7163  2.7259  2.7253  2.7499  2.7253]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 8:51 - loss: 0.0422\n",
      "[[ 2.7054  2.7165  2.7169  2.7042  2.7278   2.706  2.7485  2.7315  2.7484  2.7481\n",
      "   2.7484  2.6957  2.7291  2.7016  2.7161   2.717  2.7293   2.727  2.7487  2.7271]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:27 - loss: 0.0416\n",
      "[[ 2.7069  2.7185  2.7179  2.7063  2.7324  2.7087  2.7491  2.7308  2.7488  2.7491\n",
      "   2.7483  2.7035  2.7331  2.7084  2.7193  2.7174  2.7327  2.7325  2.7495  2.7325]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 8:04 - loss: 0.0410\n",
      "[[ 2.7049  2.7127  2.7166  2.7085  2.7347  2.7118  2.7471  2.7273  2.7477  2.7484\n",
      "   2.7473  2.7045   2.728  2.7091  2.7213  2.7192  2.7281   2.735   2.748   2.735]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 7:44 - loss: 0.0405\n",
      "[[ 2.7094   2.718  2.7209  2.7115  2.7342  2.7155  2.7494  2.7302  2.7499  2.7494\n",
      "   2.7493  2.7104  2.7326  2.7137  2.7219  2.7237  2.7326   2.735  2.7499  2.7346]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:23 - loss: 0.0399\n",
      "[[ 2.7125  2.7168  2.7239  2.7155  2.7351  2.7147  2.7492   2.732    2.75  2.7492\n",
      "   2.7488  2.7066  2.7298   2.712  2.7245   2.727  2.7303  2.7347  2.7501   2.735]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 7:03 - loss: 0.0392\n",
      "[[  2.714  2.7194   2.725  2.7158  2.7328  2.7158  2.7471  2.7324  2.7468  2.7465\n",
      "   2.7465  2.7097  2.7298  2.7127  2.7225  2.7254  2.7306  2.7326  2.7474  2.7325]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:42 - loss: 0.0386\n",
      "[[  2.716  2.7235  2.7256  2.7167  2.7326  2.7147  2.7467  2.7334  2.7467  2.7466\n",
      "   2.7456  2.7091  2.7339  2.7171  2.7234  2.7256  2.7336  2.7324  2.7474  2.7325]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:22 - loss: 0.0383\n",
      "[[ 2.7144  2.7235  2.7234  2.7124  2.7281  2.7152  2.7449  2.7302  2.7447  2.7449\n",
      "   2.7438  2.7109  2.7345  2.7164  2.7206   2.725  2.7342  2.7279  2.7451  2.7283]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 6:01 - loss: 0.0381\n",
      "[[ 2.7092  2.7185  2.7189  2.7063  2.7303  2.7109  2.7469  2.7291  2.7482  2.7486\n",
      "   2.7465  2.7049  2.7321  2.7102  2.7196  2.7212  2.7322  2.7299  2.7482  2.7302]]\n",
      "\n",
      "1536/2048 [=====================>........] - ETA: 5:41 - loss: 0.0382\n",
      "[[ 2.7138  2.7243  2.7189  2.7067  2.7315  2.7121  2.7495  2.7343  2.7517  2.7509\n",
      "   2.7495  2.7083  2.7371  2.7098  2.7199  2.7188  2.7367  2.7313  2.7514  2.7313]]\n",
      "\n",
      "1568/2048 [=====================>........] - ETA: 5:19 - loss: 0.0388\n",
      "[[ 2.7112  2.7203  2.7174  2.7036  2.7266   2.707    2.75  2.7339  2.7506  2.7518\n",
      "   2.7497   2.698  2.7331   2.701  2.7156  2.7177  2.7324  2.7264  2.7505  2.7259]]\n",
      "\n",
      "1600/2048 [======================>.......] - ETA: 4:57 - loss: 0.0394\n",
      "[[ 2.7083  2.7192  2.7161  2.6998  2.7304  2.7065  2.7554  2.7346  2.7565  2.7577\n",
      "   2.7561  2.7005  2.7353  2.7012  2.7147  2.7178  2.7345   2.731  2.7563  2.7303]]\n",
      "\n",
      "1632/2048 [======================>.......] - ETA: 4:35 - loss: 0.0399\n",
      "[[ 2.7194  2.7066  2.7205  2.7048  2.7273  2.7072   2.752  2.7348  2.7522   2.752\n",
      "   2.7518   2.694  2.7188  2.6901  2.7147   2.721  2.7181  2.7275  2.7518  2.7273]]\n",
      "\n",
      "1664/2048 [=======================>......] - ETA: 4:14 - loss: 0.0402\n",
      "[[ 2.7109  2.7078  2.7215  2.7001  2.7293  2.6978  2.7624  2.7346  2.7607  2.7613\n",
      "   2.7619  2.6844  2.7259  2.6934   2.712  2.7207  2.7254  2.7296  2.7617  2.7301]]\n",
      "\n",
      "1696/2048 [=======================>......] - ETA: 3:53 - loss: 0.0403\n",
      "[[ 2.7058  2.7095  2.7143  2.6947  2.7287   2.699  2.7599  2.7316  2.7597  2.7598\n",
      "   2.7606  2.6838  2.7305  2.6942  2.7137  2.7195  2.7304  2.7281  2.7593  2.7291]]\n",
      "\n",
      "1728/2048 [========================>.....] - ETA: 3:32 - loss: 0.0402\n",
      "[[ 2.7079  2.7203    2.72  2.6981  2.7293  2.6963    2.76  2.7394  2.7592  2.7588\n",
      "   2.7592  2.6887   2.734   2.695  2.7102  2.7169  2.7341  2.7289  2.7586  2.7292]]\n",
      "\n",
      "1760/2048 [========================>.....] - ETA: 3:11 - loss: 0.0400\n",
      "[[ 2.7066  2.7148  2.7124  2.6932   2.727  2.6978  2.7542  2.7324  2.7542  2.7535\n",
      "   2.7533   2.688  2.7295  2.6956  2.7105  2.7162  2.7308  2.7273   2.754  2.7274]]\n",
      "\n",
      "1792/2048 [=========================>....] - ETA: 2:50 - loss: 0.0396\n",
      "[[ 2.7029  2.7098   2.706  2.6916  2.7245  2.7034  2.7472  2.7248  2.7476  2.7478\n",
      "   2.7477  2.6902  2.7268  2.6953  2.7131  2.7168   2.728  2.7245  2.7478  2.7243]]\n",
      "\n",
      "1824/2048 [=========================>....] - ETA: 2:29 - loss: 0.0394\n",
      "[[ 2.7059  2.7128  2.7113  2.6983  2.7249  2.7027  2.7435  2.7271  2.7444  2.7444\n",
      "    2.744  2.6915   2.725  2.6984  2.7133  2.7154  2.7253  2.7253  2.7443   2.725]]\n",
      "\n",
      "1856/2048 [==========================>...] - ETA: 2:08 - loss: 0.0392\n",
      "[[ 2.7002  2.7082  2.7113  2.6995   2.724  2.6958  2.7424  2.7244  2.7425  2.7426\n",
      "   2.7425  2.6842  2.7208  2.6963  2.7095  2.7137  2.7212  2.7235  2.7425   2.724]]\n",
      "\n",
      "1888/2048 [==========================>...] - ETA: 1:46 - loss: 0.0389\n",
      "[[ 2.7033  2.7103  2.7132  2.6979  2.7191  2.6955  2.7402  2.7246    2.74  2.7404\n",
      "   2.7401  2.6835  2.7208  2.6946  2.7074  2.7134  2.7203  2.7192  2.7393  2.7191]]\n",
      "\n",
      "1920/2048 [===========================>..] - ETA: 1:24 - loss: 0.0385\n",
      "[[ 2.7032  2.7104   2.712  2.7012  2.7196  2.7011  2.7368  2.7221  2.7381   2.737\n",
      "   2.7375   2.692  2.7224  2.6995  2.7083  2.7136  2.7221  2.7205  2.7374    2.72]]\n",
      "\n",
      "1952/2048 [===========================>..] - ETA: 1:03 - loss: 0.0382\n",
      "[[ 2.7013  2.7082  2.7101  2.7013  2.7206  2.7026  2.7347  2.7194  2.7354  2.7344\n",
      "   2.7349  2.6947  2.7198  2.7021  2.7103  2.7117  2.7198  2.7208  2.7343  2.7206]]\n",
      "\n",
      "1984/2048 [============================>.] - ETA: 42s - loss: 0.0379\n",
      "[[ 2.7056  2.7116  2.7112  2.6995  2.7197  2.7023  2.7376  2.7211  2.7378  2.7374\n",
      "   2.7374   2.695  2.7249  2.7014  2.7107  2.7145   2.725  2.7202  2.7377  2.7198]]\n",
      "\n",
      "2016/2048 [============================>.] - ETA: 21s - loss: 0.0376\n",
      "[[ 2.7044  2.7085  2.7111  2.7003  2.7215  2.7042  2.7382  2.7184  2.7382  2.7378\n",
      "   2.7383  2.7008  2.7238  2.7033  2.7108  2.7141  2.7239  2.7214  2.7375  2.7217]]\n",
      "\n",
      "2048/2048 [==============================] - ETA: 0s - loss: 0.0374\n",
      "[[ 2.7077  2.7115  2.7116  2.7006  2.7231  2.7017    2.74  2.7219  2.7394  2.7398\n",
      "   2.7394  2.6998  2.7243  2.7019  2.7132  2.7128  2.7236  2.7231  2.7392  2.7236]]\n",
      "\n",
      "2048/2048 [==============================] - 1359s 664ms/step - loss: 0.0374\n",
      "Epoch 7/1000\n",
      "  32/2048 [..............................] - ETA: 22:15 - loss: 0.0270\n",
      "[[ 2.7057  2.7112  2.7089  2.6976  2.7237  2.7033  2.7428  2.7216   2.742  2.7422\n",
      "   2.7419  2.6998  2.7273  2.7009  2.7112  2.7119  2.7271  2.7237  2.7418  2.7239]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 22:39 - loss: 0.0290\n",
      "[[  2.702  2.7073  2.7099  2.6944  2.7217  2.6989  2.7456  2.7206  2.7451  2.7446\n",
      "   2.7449  2.6922   2.725  2.6968  2.7083  2.7168  2.7247  2.7209  2.7444  2.7212]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 22:46 - loss: 0.0274\n",
      "[[ 2.6999   2.705  2.7069  2.6937  2.7203  2.7016  2.7405  2.7191  2.7409  2.7408\n",
      "   2.7401  2.6918  2.7222  2.6968  2.7083  2.7123  2.7215  2.7199  2.7403  2.7189]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 22:11 - loss: 0.0265\n",
      "[[ 2.6952  2.7042  2.7041  2.6925  2.7213  2.6987  2.7409  2.7199  2.7417   2.741\n",
      "   2.7404  2.6898  2.7211  2.6948  2.7062  2.7067  2.7209  2.7213  2.7405  2.7207]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 21:02 - loss: 0.0259\n",
      "[[ 2.6968  2.7009  2.7001  2.6923  2.7233   2.703  2.7371  2.7173  2.7388  2.7374\n",
      "   2.7374  2.6937  2.7161  2.6922  2.7101  2.7026  2.7155  2.7237  2.7376  2.7224]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 19:57 - loss: 0.0252\n",
      "[[ 2.6958   2.701  2.7012  2.6896  2.7204   2.699  2.7386  2.7157  2.7393  2.7381\n",
      "   2.7381  2.6942  2.7205  2.6931  2.7068  2.7059  2.7197  2.7203  2.7387  2.7194]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 19:26 - loss: 0.0244\n",
      "[[ 2.6944  2.7052  2.7046   2.693  2.7217  2.6997  2.7397  2.7184  2.7395  2.7389\n",
      "    2.739  2.6938   2.723  2.6951  2.7068  2.7046  2.7223  2.7213  2.7396  2.7209]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 19:17 - loss: 0.0241\n",
      "[[ 2.6998   2.708  2.7136  2.6992   2.721  2.6997    2.74  2.7239  2.7399  2.7398\n",
      "   2.7398  2.6935  2.7232  2.6969  2.7058   2.709   2.723  2.7208  2.7396  2.7207]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 19:08 - loss: 0.0240\n",
      "[[ 2.7006  2.7095  2.7155  2.7006   2.725   2.702  2.7438  2.7243  2.7441  2.7444\n",
      "   2.7444  2.6959  2.7261  2.7016  2.7108  2.7157  2.7262  2.7254  2.7439  2.7248]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 18:53 - loss: 0.0239\n",
      "[[  2.697   2.709  2.7116  2.6996  2.7245  2.6999  2.7413  2.7211   2.741  2.7414\n",
      "   2.7409  2.6978  2.7261  2.7037  2.7088  2.7117  2.7259  2.7241  2.7411   2.724]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 18:36 - loss: 0.0238\n",
      "[[ 2.7029  2.7128  2.7181  2.7058  2.7263  2.7014  2.7438  2.7271  2.7432  2.7439\n",
      "   2.7436  2.7002  2.7284  2.7066  2.7093  2.7144  2.7275  2.7256  2.7432  2.7259]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 18:22 - loss: 0.0238\n",
      "[[ 2.7018  2.7115  2.7156  2.7045  2.7234  2.7029  2.7403  2.7249  2.7397  2.7402\n",
      "     2.74  2.6956  2.7237  2.7045  2.7115  2.7152  2.7223  2.7222  2.7401  2.7235]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 18:08 - loss: 0.0246\n",
      "[[    2.7  2.7086  2.7109  2.7024  2.7265  2.7026  2.7411  2.7219  2.7411  2.7416\n",
      "   2.7413  2.6966  2.7254  2.7018   2.712  2.7114  2.7245  2.7256  2.7416  2.7264]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 17:55 - loss: 0.0252\n",
      "[[ 2.6948  2.7032  2.7067  2.6954  2.7209  2.7001  2.7384  2.7181  2.7392  2.7388\n",
      "   2.7388  2.6905  2.7206  2.6943  2.7086  2.7094    2.72  2.7205   2.739  2.7206]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 17:25 - loss: 0.0257\n",
      "[[ 2.6904  2.6979  2.7044  2.6955  2.7225  2.7006  2.7363  2.7149   2.737  2.7368\n",
      "   2.7368  2.6895  2.7172  2.6943  2.7094  2.7084  2.7168   2.722  2.7363  2.7224]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 16:48 - loss: 0.0269\n",
      "[[ 2.6868  2.7011  2.7057  2.6937  2.7229  2.6987  2.7421  2.7187  2.7425  2.7428\n",
      "   2.7428  2.6875    2.72   2.694  2.7039  2.7064  2.7188  2.7222  2.7418  2.7231]]\n",
      "\n",
      " 544/2048 [======>.......................] - ETA: 16:23 - loss: 0.0276\n",
      "[[ 2.6847  2.6987  2.7024  2.6898  2.7235  2.6988  2.7427  2.7176  2.7438  2.7424\n",
      "   2.7436  2.6833  2.7192  2.6896  2.7061  2.7075  2.7182  2.7224  2.7421  2.7229]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 16:03 - loss: 0.0283\n",
      "[[ 2.6948  2.7071  2.7069  2.6879  2.7193  2.6921  2.7432  2.7202  2.7435  2.7425\n",
      "    2.744  2.6859  2.7211  2.6895  2.6999  2.7058  2.7203  2.7176  2.7421  2.7196]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 15:49 - loss: 0.0296\n",
      "[[ 2.7061  2.7018  2.7097  2.6921  2.7198  2.7038  2.7469  2.7231  2.7459   2.746\n",
      "   2.7459   2.691  2.7187  2.6894  2.7081  2.7153  2.7183  2.7176  2.7453  2.7193]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 15:30 - loss: 0.0299\n",
      "[[ 2.7195  2.7024  2.7059  2.6906  2.7196  2.7106  2.7428  2.7226  2.7426  2.7422\n",
      "   2.7429  2.6859  2.7147  2.6851  2.7179  2.7198  2.7156  2.7185  2.7412  2.7186]]\n",
      "\n",
      " 672/2048 [========>.....................] - ETA: 15:13 - loss: 0.0302\n",
      "[[ 2.7148  2.6985  2.7043  2.6896  2.7239  2.7101  2.7467   2.723  2.7469  2.7457\n",
      "   2.7459  2.6908  2.7187  2.6836   2.718  2.7148  2.7186  2.7236  2.7457  2.7237]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 14:55 - loss: 0.0310\n",
      "[[ 2.7098   2.695   2.698  2.6812  2.7207  2.7003  2.7486  2.7217  2.7497  2.7478\n",
      "   2.7471  2.6822  2.7164  2.6782  2.7119  2.7084  2.7168  2.7194  2.7478  2.7208]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 14:35 - loss: 0.0315\n",
      "[[ 2.7055  2.6988  2.7013  2.6844  2.7187  2.6938  2.7437  2.7196   2.743  2.7428\n",
      "   2.7427  2.6819  2.7175  2.6814  2.7055  2.7066  2.7176  2.7182  2.7427  2.7184]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 14:15 - loss: 0.0321\n",
      "[[ 2.7054  2.7044  2.7029  2.6859  2.7188  2.6933  2.7462  2.7235   2.746  2.7467\n",
      "    2.746  2.6856  2.7194  2.6814  2.7038  2.7019  2.7192  2.7189  2.7457  2.7191]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 13:50 - loss: 0.0336\n",
      "[[ 2.7065  2.6984  2.7074  2.6909  2.7218  2.6928   2.748  2.7236  2.7478  2.7483\n",
      "   2.7473  2.6851  2.7151  2.6835  2.7038  2.7066  2.7158  2.7218  2.7475  2.7212]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 13:26 - loss: 0.0350\n",
      "[[ 2.7003  2.6998  2.7102  2.6912  2.7221  2.6912   2.753  2.7251  2.7537  2.7536\n",
      "   2.7527  2.6866  2.7188  2.6844  2.7012  2.7032  2.7187   2.722  2.7538  2.7225]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 13:04 - loss: 0.0363\n",
      "[[ 2.7082  2.7102  2.7143  2.6985  2.7283  2.6902   2.756  2.7341  2.7565  2.7578\n",
      "   2.7563  2.6915  2.7237  2.6889  2.7019  2.7044  2.7236  2.7287  2.7562  2.7287]]\n",
      "\n",
      " 896/2048 [============>.................] - ETA: 12:41 - loss: 0.0371\n",
      "[[ 2.7073  2.7099   2.712  2.6944  2.7247  2.6885  2.7494  2.7304  2.7494  2.7502\n",
      "   2.7494  2.6939  2.7252  2.6882  2.7011  2.7047  2.7251  2.7248    2.75  2.7254]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 12:22 - loss: 0.0376\n",
      "[[ 2.7027  2.7111  2.7093  2.6879  2.7223   2.683  2.7537  2.7289  2.7542  2.7553\n",
      "   2.7547   2.689  2.7319  2.6859  2.6982  2.7045  2.7313  2.7229  2.7541  2.7233]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 12:03 - loss: 0.0376\n",
      "[[ 2.7076  2.7127  2.7132  2.6927   2.718  2.6879  2.7505  2.7316  2.7503  2.7511\n",
      "    2.751  2.6932  2.7299  2.6914   2.698   2.705  2.7301  2.7186  2.7507  2.7196]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:43 - loss: 0.0371\n",
      "[[ 2.7085  2.7088  2.7108  2.6938  2.7188  2.6893  2.7477  2.7267  2.7476  2.7485\n",
      "    2.748  2.6929  2.7275  2.6942  2.7025  2.7079   2.728  2.7194  2.7479    2.72]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 11:23 - loss: 0.0364\n",
      "[[ 2.7096  2.7079  2.7124  2.6985  2.7211  2.6946  2.7435  2.7252  2.7434   2.744\n",
      "   2.7438   2.694  2.7245  2.6975  2.7068  2.7096  2.7248  2.7218  2.7436  2.7221]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 11:02 - loss: 0.0358\n",
      "[[ 2.7092  2.7078  2.7127  2.7003  2.7211  2.6974  2.7406  2.7233  2.7405   2.741\n",
      "   2.7409   2.699  2.7226  2.6994  2.7071  2.7098  2.7223  2.7213  2.7409  2.7216]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:42 - loss: 0.0352\n",
      "[[ 2.7119  2.7085  2.7156  2.7027  2.7196  2.6996  2.7373   2.724  2.7371  2.7374\n",
      "   2.7375   2.698  2.7203  2.6987  2.7089  2.7121  2.7199  2.7198  2.7374  2.7199]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 10:21 - loss: 0.0344\n",
      "[[ 2.7089    2.71  2.7127  2.7008  2.7204  2.6987  2.7357  2.7226  2.7356  2.7357\n",
      "   2.7361  2.7006  2.7215  2.6984  2.7083   2.709  2.7212  2.7205  2.7358  2.7205]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 9:56 - loss: 0.0339\n",
      "[[ 2.7107  2.7105  2.7126  2.7042  2.7225  2.7031  2.7353  2.7233  2.7352  2.7353\n",
      "   2.7354  2.7027  2.7201  2.6999  2.7111  2.7097  2.7201  2.7224   2.735  2.7223]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:34 - loss: 0.0332\n",
      "[[ 2.7112  2.7114  2.7121  2.7031  2.7218  2.7036  2.7363  2.7225   2.736  2.7361\n",
      "    2.736   2.704  2.7217  2.7016  2.7108  2.7112  2.7218  2.7216  2.7357  2.7216]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 9:14 - loss: 0.0325\n",
      "[[  2.714   2.713  2.7133  2.7037  2.7216  2.7062  2.7349  2.7224  2.7347  2.7348\n",
      "   2.7349  2.7047  2.7222  2.7039  2.7127  2.7152  2.7223  2.7216  2.7345  2.7217]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 8:54 - loss: 0.0318\n",
      "[[  2.714  2.7147  2.7133  2.7043   2.721  2.7052  2.7331  2.7229  2.7332  2.7331\n",
      "   2.7328  2.7049  2.7228  2.7052  2.7127   2.714  2.7228  2.7207  2.7327   2.721]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:34 - loss: 0.0312\n",
      "[[ 2.7139  2.7162  2.7155  2.7077  2.7235  2.7087  2.7332  2.7235  2.7335  2.7334\n",
      "   2.7332  2.7104  2.7238  2.7101  2.7149  2.7163  2.7238  2.7232   2.733  2.7236]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 8:14 - loss: 0.0305\n",
      "[[ 2.7151  2.7146  2.7156  2.7104   2.724  2.7121  2.7309  2.7218  2.7312  2.7311\n",
      "   2.7311  2.7112  2.7218   2.711  2.7176  2.7182  2.7216  2.7239  2.7309  2.7242]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 7:53 - loss: 0.0299\n",
      "[[ 2.7137  2.7134  2.7154  2.7089  2.7215    2.71  2.7297  2.7209  2.7298    2.73\n",
      "     2.73  2.7099    2.72  2.7094  2.7141  2.7162  2.7199  2.7215  2.7296  2.7219]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:33 - loss: 0.0293\n",
      "[[ 2.7148  2.7133  2.7165  2.7117  2.7231  2.7107  2.7293  2.7218  2.7294  2.7293\n",
      "   2.7293  2.7097  2.7194  2.7112  2.7157  2.7165  2.7194  2.7231  2.7292  2.7235]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 7:13 - loss: 0.0287\n",
      "[[ 2.7153  2.7138  2.7166  2.7125  2.7221  2.7119  2.7271   2.721  2.7273  2.7271\n",
      "   2.7272  2.7106  2.7193  2.7124  2.7164   2.717   2.719   2.722  2.7271  2.7223]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:51 - loss: 0.0281\n",
      "[[ 2.7141  2.7123  2.7149   2.711  2.7219  2.7123  2.7273  2.7193  2.7275  2.7273\n",
      "   2.7275  2.7113  2.7195  2.7121  2.7165  2.7171  2.7193  2.7219  2.7273   2.722]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:29 - loss: 0.0277\n",
      "[[ 2.7135  2.7128  2.7147  2.7107  2.7197  2.7128  2.7255  2.7181  2.7253  2.7255\n",
      "   2.7256   2.712  2.7191  2.7116  2.7151   2.717  2.7187  2.7196  2.7254  2.7196]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 6:07 - loss: 0.0272\n",
      "[[ 2.7123  2.7108  2.7116   2.708  2.7207  2.7122  2.7267  2.7177  2.7265  2.7267\n",
      "   2.7268  2.7107  2.7185  2.7083   2.715  2.7141  2.7181  2.7207  2.7266  2.7201]]\n",
      "\n",
      "1536/2048 [=====================>........] - ETA: 5:45 - loss: 0.0269\n",
      "[[ 2.7139  2.7151   2.713   2.707  2.7187  2.7082  2.7278    2.72  2.7279   2.728\n",
      "   2.7281  2.7099  2.7222  2.7084  2.7123  2.7127  2.7219  2.7189  2.7281  2.7186]]\n",
      "\n",
      "1568/2048 [=====================>........] - ETA: 5:23 - loss: 0.0270\n",
      "[[ 2.7147  2.7124  2.7113  2.7051  2.7189  2.7055  2.7308  2.7218  2.7306  2.7307\n",
      "   2.7312  2.7029  2.7189  2.7006   2.711  2.7098  2.7188  2.7191  2.7309  2.7191]]\n",
      "\n",
      "1600/2048 [======================>.......] - ETA: 5:01 - loss: 0.0277\n",
      "[[  2.711  2.7147  2.7098   2.695  2.7148  2.7001  2.7376  2.7224   2.737  2.7374\n",
      "   2.7386  2.7025  2.7256  2.6953  2.7028  2.7075  2.7258  2.7152   2.738  2.7143]]\n",
      "\n",
      "1632/2048 [======================>.......] - ETA: 4:40 - loss: 0.0287\n",
      "[[ 2.7115   2.696  2.7013  2.6914   2.722  2.6983  2.7394  2.7208  2.7393  2.7393\n",
      "    2.739  2.6841  2.7133  2.6822  2.7147  2.7058  2.7137  2.7229  2.7397  2.7229]]\n",
      "\n",
      "1664/2048 [=======================>......] - ETA: 4:19 - loss: 0.0297\n",
      "[[  2.716   2.708  2.7126  2.6958  2.7111  2.6911  2.7465  2.7255  2.7461   2.746\n",
      "   2.7453  2.6839  2.7234  2.6914  2.7025  2.7157  2.7246  2.7109  2.7462  2.7102]]\n",
      "\n",
      "1696/2048 [=======================>......] - ETA: 3:57 - loss: 0.0305\n",
      "[[ 2.7083  2.7097  2.7064  2.6905   2.723  2.6917   2.754  2.7276  2.7531  2.7528\n",
      "   2.7534  2.6908  2.7309  2.6877  2.7081  2.7064  2.7317  2.7237  2.7542  2.7226]]\n",
      "\n",
      "1728/2048 [========================>.....] - ETA: 3:35 - loss: 0.0316\n",
      "[[ 2.7092  2.7048  2.7091  2.6888  2.7213  2.6877  2.7531  2.7249  2.7529  2.7523\n",
      "   2.7529  2.6861  2.7283  2.6856  2.7044  2.7092  2.7282  2.7217  2.7528  2.7213]]\n",
      "\n",
      "1760/2048 [========================>.....] - ETA: 3:14 - loss: 0.0324\n",
      "[[ 2.7061  2.7091  2.7142  2.6866  2.7209  2.6723  2.7563  2.7291  2.7556  2.7555\n",
      "   2.7562  2.6787   2.727  2.6831  2.6995   2.704  2.7273  2.7211  2.7561  2.7205]]\n",
      "\n",
      "1792/2048 [=========================>....] - ETA: 2:52 - loss: 0.0332\n",
      "[[ 2.7076  2.7111  2.7102  2.6858  2.7169  2.6735  2.7551  2.7287  2.7552   2.755\n",
      "   2.7549  2.6778  2.7274  2.6866  2.6946  2.7026  2.7265  2.7171  2.7546  2.7167]]\n",
      "\n",
      "1824/2048 [=========================>....] - ETA: 2:30 - loss: 0.0335\n",
      "[[ 2.7088  2.7127  2.7108  2.6902    2.72  2.6788  2.7534  2.7302  2.7538  2.7545\n",
      "   2.7538  2.6861  2.7261  2.6888  2.6963  2.7015  2.7261  2.7209  2.7532   2.721]]\n",
      "\n",
      "1856/2048 [==========================>...] - ETA: 2:09 - loss: 0.0341\n",
      "[[ 2.7067  2.7119  2.7085  2.6865  2.7167  2.6757  2.7521  2.7279  2.7522  2.7528\n",
      "   2.7521  2.6818  2.7258  2.6833  2.6917  2.6987  2.7254  2.7171  2.7517  2.7168]]\n",
      "\n",
      "1888/2048 [==========================>...] - ETA: 1:47 - loss: 0.0350\n",
      "[[ 2.7091  2.7113  2.7087  2.6904  2.7218  2.6806  2.7528  2.7289  2.7528  2.7535\n",
      "   2.7533  2.6842  2.7279  2.6876  2.6989  2.7038  2.7268  2.7219  2.7526  2.7212]]\n",
      "\n",
      "1920/2048 [===========================>..] - ETA: 1:25 - loss: 0.0362\n",
      "[[ 2.7088  2.7073  2.7047   2.682  2.7242  2.6792  2.7592  2.7279  2.7593  2.7596\n",
      "   2.7603  2.6819  2.7338  2.6819  2.6998   2.703  2.7324  2.7253  2.7591  2.7239]]\n",
      "\n",
      "1952/2048 [===========================>..] - ETA: 1:04 - loss: 0.0370\n",
      "[[ 2.7174  2.7149  2.7119  2.6866  2.7227  2.6886  2.7573  2.7367  2.7569  2.7589\n",
      "   2.7591  2.6908  2.7336  2.6857  2.7038  2.7088  2.7329  2.7231  2.7569  2.7226]]\n",
      "\n",
      "1984/2048 [============================>.] - ETA: 43s - loss: 0.0375\n",
      "[[ 2.7111  2.7101  2.7084  2.6852  2.7236  2.6908  2.7551   2.734  2.7558  2.7554\n",
      "   2.7566  2.6918  2.7276  2.6813  2.7017  2.7061  2.7269  2.7229  2.7543  2.7221]]\n",
      "\n",
      "2016/2048 [============================>.] - ETA: 21s - loss: 0.0375\n",
      "[[ 2.7132  2.7098  2.7083    2.69  2.7287   2.694  2.7552  2.7328  2.7556  2.7546\n",
      "   2.7555   2.696  2.7293  2.6871  2.7087  2.7072  2.7284  2.7283  2.7551  2.7275]]\n",
      "\n",
      "2048/2048 [==============================] - ETA: 0s - loss: 0.0375\n",
      "[[ 2.7093  2.7076  2.7096  2.6949  2.7297   2.698  2.7525  2.7272  2.7522  2.7512\n",
      "   2.7518  2.6982  2.7262  2.6929  2.7104  2.7093  2.7255  2.7286  2.7522  2.7285]]\n",
      "\n",
      "2048/2048 [==============================] - 1375s 671ms/step - loss: 0.0375\n",
      "Epoch 8/1000\n",
      "  32/2048 [..............................] - ETA: 20:31 - loss: 0.0231\n",
      "[[   2.71   2.712  2.7106  2.6945  2.7272  2.6965   2.749  2.7257  2.7487   2.748\n",
      "   2.7481   2.697  2.7307  2.6977  2.7099  2.7127  2.7302  2.7264  2.7486  2.7256]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 20:49 - loss: 0.0241\n",
      "[[  2.715  2.7173  2.7162  2.7003  2.7307  2.7025  2.7511  2.7291  2.7508  2.7501\n",
      "   2.7506  2.7024  2.7344  2.7032  2.7155  2.7165  2.7339  2.7297   2.751  2.7297]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 21:07 - loss: 0.0252\n",
      "[[ 2.7175  2.7177  2.7173  2.7017  2.7275  2.7036  2.7496  2.7292  2.7492  2.7491\n",
      "   2.7491  2.7034  2.7344  2.7038  2.7147  2.7189  2.7342  2.7272  2.7498  2.7269]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 21:04 - loss: 0.0251\n",
      "[[ 2.7161   2.716  2.7177   2.703  2.7256   2.701  2.7453  2.7286  2.7451  2.7452\n",
      "   2.7447  2.7014  2.7303  2.7029  2.7119  2.7166  2.7305  2.7256  2.7454  2.7254]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 20:39 - loss: 0.0246\n",
      "[[ 2.7158  2.7133  2.7153  2.7032  2.7285  2.7048  2.7458  2.7272  2.7456  2.7454\n",
      "   2.7451  2.7026  2.7283  2.7045  2.7153  2.7173  2.7289  2.7281  2.7457   2.728]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 20:11 - loss: 0.0238\n",
      "[[ 2.7129  2.7111  2.7129  2.7007  2.7264  2.7039  2.7452  2.7239  2.7454  2.7451\n",
      "   2.7448  2.7043   2.729  2.7045  2.7132  2.7164  2.7292  2.7259  2.7452  2.7262]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 19:47 - loss: 0.0222\n",
      "[[ 2.7133   2.712  2.7117  2.7007  2.7256  2.7053  2.7416  2.7248  2.7421  2.7422\n",
      "   2.7418  2.7053   2.727  2.7029  2.7132  2.7126  2.7269  2.7256  2.7416  2.7257]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 19:46 - loss: 0.0206\n",
      "[[  2.714  2.7136  2.7156  2.7042  2.7243  2.7053  2.7402  2.7253  2.7406  2.7408\n",
      "   2.7406   2.705  2.7258   2.705  2.7134  2.7154  2.7258  2.7245  2.7403  2.7241]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 19:20 - loss: 0.0191\n",
      "[[ 2.7144  2.7121  2.7138  2.7049  2.7235  2.7082  2.7362   2.723  2.7363  2.7365\n",
      "   2.7365  2.7067  2.7231  2.7051  2.7143  2.7149  2.7232  2.7236  2.7359   2.723]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 18:52 - loss: 0.0178\n",
      "[[ 2.7159  2.7121  2.7168  2.7084  2.7253  2.7096  2.7363  2.7238  2.7367  2.7367\n",
      "   2.7365  2.7074  2.7233  2.7088  2.7167   2.719  2.7232   2.725  2.7361   2.725]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 18:21 - loss: 0.0166\n",
      "[[ 2.7174  2.7149  2.7184  2.7123  2.7262  2.7113  2.7348  2.7247  2.7351  2.7351\n",
      "    2.735  2.7105  2.7239  2.7127  2.7181    2.72   2.724  2.7262  2.7347   2.726]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 17:57 - loss: 0.0155\n",
      "[[ 2.7184  2.7174   2.719  2.7128  2.7256  2.7129  2.7339  2.7245  2.7341  2.7342\n",
      "   2.7341  2.7134  2.7255  2.7146  2.7185  2.7203  2.7253  2.7253   2.734  2.7254]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 17:46 - loss: 0.0146\n",
      "[[ 2.7187  2.7185  2.7198  2.7146  2.7252  2.7145  2.7324  2.7249  2.7326  2.7326\n",
      "   2.7326  2.7148  2.7245  2.7145  2.7188  2.7199  2.7244  2.7251  2.7325  2.7251]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 17:33 - loss: 0.0138\n",
      "[[ 2.7202  2.7196  2.7213  2.7168  2.7264  2.7161  2.7323  2.7254  2.7324  2.7325\n",
      "   2.7326  2.7164  2.7247  2.7162  2.7208  2.7215  2.7247  2.7264  2.7326  2.7264]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 17:15 - loss: 0.0131\n",
      "[[ 2.7211  2.7201  2.7216  2.7166  2.7249  2.7164   2.732  2.7256   2.732  2.7319\n",
      "   2.7321  2.7167  2.7248  2.7161    2.72   2.722  2.7248   2.725  2.7322  2.7249]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 16:57 - loss: 0.0126\n",
      "[[ 2.7182   2.718  2.7192  2.7147  2.7236  2.7147  2.7306  2.7226  2.7306  2.7308\n",
      "   2.7309  2.7148   2.724  2.7159  2.7189  2.7216  2.7242  2.7235  2.7309  2.7235]]\n",
      "\n",
      " 544/2048 [======>.......................] - ETA: 16:41 - loss: 0.0123\n",
      "[[ 2.7191  2.7169  2.7189  2.7147   2.724  2.7159   2.731  2.7229   2.731  2.7309\n",
      "    2.731  2.7144  2.7229  2.7145    2.72  2.7215  2.7229  2.7239  2.7311  2.7238]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 16:12 - loss: 0.0121\n",
      "[[ 2.7169  2.7173  2.7195  2.7143  2.7247  2.7123  2.7322  2.7232   2.732  2.7319\n",
      "   2.7322  2.7133  2.7246  2.7152  2.7181  2.7196  2.7244  2.7244  2.7323  2.7246]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 15:53 - loss: 0.0122\n",
      "[[ 2.7179  2.7169  2.7182  2.7096  2.7222  2.7072   2.735  2.7251  2.7352  2.7355\n",
      "   2.7356  2.7068  2.7253  2.7093  2.7161  2.7181  2.7253   2.722  2.7351  2.7221]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 15:34 - loss: 0.0124\n",
      "[[ 2.7182  2.7172  2.7165  2.7072  2.7242  2.7084  2.7391  2.7264  2.7387  2.7392\n",
      "   2.7391  2.7071  2.7273    2.71  2.7178  2.7196  2.7273  2.7238  2.7392  2.7241]]\n",
      "\n",
      " 672/2048 [========>.....................] - ETA: 15:12 - loss: 0.0128\n",
      "[[ 2.7154  2.7098  2.7116  2.7021  2.7223  2.7079  2.7366   2.721  2.7357  2.7363\n",
      "   2.7361  2.7019  2.7222  2.7054  2.7169  2.7187  2.7224  2.7227  2.7369  2.7223]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 14:53 - loss: 0.0139\n",
      "[[ 2.7122  2.7061  2.7111   2.699  2.7259  2.6996  2.7456  2.7236  2.7453  2.7447\n",
      "    2.745  2.6978  2.7261  2.7012  2.7116   2.714  2.7251  2.7254  2.7455  2.7256]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 14:36 - loss: 0.0146\n",
      "[[ 2.7152  2.7117  2.7141  2.7011  2.7204  2.7008  2.7405  2.7255   2.741  2.7406\n",
      "   2.7411  2.6995  2.7265  2.7012  2.7113  2.7168   2.725  2.7207  2.7405  2.7208]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 14:18 - loss: 0.0154\n",
      "[[ 2.7105  2.7121  2.7145  2.7039  2.7227  2.6998  2.7414  2.7255  2.7417  2.7419\n",
      "   2.7416  2.7035  2.7256  2.7046  2.7065  2.7125  2.7241  2.7236  2.7413  2.7231]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 14:03 - loss: 0.0184\n",
      "[[ 2.7044  2.7032  2.7071  2.6953  2.7239  2.6982  2.7441  2.7211  2.7442  2.7435\n",
      "   2.7437  2.7001  2.7222  2.6971  2.7069  2.7089  2.7215  2.7242  2.7441   2.724]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 13:46 - loss: 0.0215\n",
      "[[  2.696  2.6874  2.6973  2.6878  2.7277  2.6925  2.7544  2.7139  2.7538  2.7543\n",
      "   2.7548  2.6939  2.7227  2.6916  2.7038  2.7093  2.7208  2.7276  2.7544   2.727]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 13:27 - loss: 0.0239\n",
      "[[ 2.6995  2.6823  2.6963  2.6833  2.7241  2.6875  2.7544    2.72  2.7541  2.7535\n",
      "    2.754  2.6912  2.7222   2.685  2.7024  2.7076  2.7215  2.7256  2.7553  2.7245]]\n",
      "\n",
      " 896/2048 [============>.................] - ETA: 13:06 - loss: 0.0256\n",
      "[[ 2.7031  2.6827  2.6986  2.6803  2.7215  2.6786  2.7572  2.7223  2.7564  2.7566\n",
      "   2.7576  2.6818   2.724  2.6809  2.7011  2.7128  2.7236  2.7237   2.758  2.7229]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 12:45 - loss: 0.0276\n",
      "[[ 2.6918  2.6849  2.6975  2.6794  2.7163  2.6679  2.7526  2.7168  2.7516  2.7521\n",
      "   2.7518  2.6751  2.7216  2.6803  2.6933  2.7007  2.7219  2.7185  2.7531  2.7181]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 12:19 - loss: 0.0289\n",
      "[[ 2.7017  2.6929  2.7067  2.6853  2.7156  2.6809  2.7489  2.7226  2.7493  2.7487\n",
      "   2.7491  2.6869  2.7267  2.6834  2.6953  2.7033  2.7262  2.7154   2.749  2.7155]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:54 - loss: 0.0298\n",
      "[[ 2.6963  2.6944  2.7011  2.6795  2.7188  2.6825  2.7532  2.7184  2.7533  2.7529\n",
      "   2.7531  2.6847   2.726  2.6816   2.699  2.7008  2.7267  2.7176  2.7534  2.7182]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 11:33 - loss: 0.0306\n",
      "[[ 2.6964  2.6896  2.6972  2.6783  2.7157   2.689  2.7491  2.7138   2.749  2.7493\n",
      "   2.7494  2.6857  2.7223  2.6798  2.6986  2.6991  2.7227  2.7147  2.7491  2.7146]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 11:10 - loss: 0.0324\n",
      "[[ 2.7073  2.6955  2.7071  2.6901  2.7216  2.6948  2.7566  2.7233  2.7566  2.7566\n",
      "   2.7567  2.6864  2.7248  2.6849  2.7072  2.7062   2.725  2.7222  2.7568  2.7216]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:50 - loss: 0.0336\n",
      "[[ 2.7111  2.6967  2.7068  2.6903  2.7275  2.7007  2.7572  2.7257  2.7573  2.7579\n",
      "    2.758  2.6901  2.7257  2.6886   2.714  2.7129  2.7261  2.7272  2.7575  2.7274]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 10:31 - loss: 0.0343\n",
      "[[ 2.7077  2.7047  2.7152  2.6928  2.7276  2.6977  2.7616  2.7308  2.7615  2.7624\n",
      "   2.7617  2.6927  2.7304  2.6906  2.7074   2.714  2.7303  2.7268  2.7619  2.7267]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 10:11 - loss: 0.0344\n",
      "[[ 2.7047  2.7077  2.7145   2.697  2.7281  2.6936  2.7571  2.7299  2.7572  2.7578\n",
      "   2.7576  2.6921  2.7295  2.6925  2.7078  2.7098  2.7292  2.7276  2.7575  2.7271]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:51 - loss: 0.0342\n",
      "[[ 2.7056  2.7084   2.709  2.6937  2.7273  2.6945  2.7529   2.727  2.7534  2.7535\n",
      "   2.7536  2.6947  2.7284  2.6921  2.7068   2.707  2.7276  2.7265  2.7531  2.7263]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 9:32 - loss: 0.0338\n",
      "[[ 2.7075  2.7086  2.7113  2.6951  2.7253   2.696  2.7488  2.7261  2.7493  2.7492\n",
      "   2.7491  2.6945  2.7251  2.6935  2.7073  2.7102  2.7249   2.724  2.7488  2.7238]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 9:11 - loss: 0.0333\n",
      "[[ 2.7098  2.7108  2.7128  2.6968  2.7235  2.6971   2.746  2.7263  2.7463  2.7466\n",
      "   2.7462  2.6973  2.7244   2.696  2.7072  2.7111  2.7247  2.7222  2.7461  2.7222]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:50 - loss: 0.0328\n",
      "[[ 2.7112  2.7116  2.7137  2.6982  2.7208  2.7003  2.7417  2.7255  2.7414  2.7419\n",
      "   2.7416  2.7007  2.7241   2.698  2.7078  2.7129  2.7242  2.7199  2.7415  2.7199]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 8:27 - loss: 0.0323\n",
      "[[  2.712  2.7088  2.7123  2.7029  2.7243  2.7062  2.7391  2.7216  2.7394  2.7398\n",
      "   2.7397  2.7035  2.7219  2.7024  2.7142  2.7158  2.7223  2.7243  2.7394  2.7237]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 8:02 - loss: 0.0319\n",
      "[[ 2.7097  2.7098  2.7149  2.7056  2.7237  2.7057  2.7383  2.7212  2.7388  2.7389\n",
      "   2.7387  2.7063   2.723  2.7063  2.7114  2.7156  2.7238  2.7235  2.7384  2.7231]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:38 - loss: 0.0315\n",
      "[[ 2.7118  2.7101  2.7158   2.707   2.726  2.7069  2.7402  2.7234  2.7406  2.7405\n",
      "   2.7403  2.7049  2.7231  2.7055  2.7143  2.7145  2.7233  2.7258  2.7405  2.7255]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 7:17 - loss: 0.0309\n",
      "[[ 2.7126  2.7121  2.7175  2.7081  2.7246  2.7071  2.7386  2.7242  2.7387  2.7387\n",
      "   2.7387  2.7064  2.7242  2.7073  2.7147   2.717  2.7241  2.7245  2.7389  2.7242]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:56 - loss: 0.0304\n",
      "[[ 2.7153  2.7153  2.7178  2.7102   2.726  2.7093  2.7383  2.7256  2.7383  2.7384\n",
      "   2.7383  2.7096  2.7257  2.7093  2.7162   2.717  2.7256  2.7258  2.7385  2.7259]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:34 - loss: 0.0298\n",
      "[[ 2.7158  2.7151  2.7159  2.7093  2.7237  2.7105  2.7355  2.7234  2.7353  2.7356\n",
      "   2.7354  2.7102  2.7251  2.7096  2.7164  2.7176  2.7249  2.7239  2.7356  2.7241]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 6:13 - loss: 0.0293\n",
      "[[  2.718  2.7164  2.7162  2.7106  2.7241  2.7139  2.7346  2.7239  2.7347  2.7348\n",
      "   2.7348  2.7131  2.7256  2.7113  2.7183   2.718   2.725  2.7243  2.7348  2.7244]]\n",
      "\n",
      "1536/2048 [=====================>........] - ETA: 5:51 - loss: 0.0288\n",
      "[[ 2.7175  2.7164  2.7156  2.7088  2.7234  2.7119  2.7343   2.724  2.7344  2.7345\n",
      "   2.7344   2.712   2.725    2.71  2.7169  2.7166  2.7247  2.7238  2.7345  2.7236]]\n",
      "\n",
      "1568/2048 [=====================>........] - ETA: 5:29 - loss: 0.0284\n",
      "[[ 2.7207  2.7191  2.7187  2.7106  2.7253  2.7157  2.7374  2.7267  2.7374  2.7373\n",
      "   2.7374   2.715  2.7286  2.7121  2.7192  2.7202   2.728  2.7254  2.7375  2.7254]]\n",
      "\n",
      "1600/2048 [======================>.......] - ETA: 5:08 - loss: 0.0280\n",
      "[[ 2.7216  2.7212  2.7185  2.7097  2.7237  2.7144  2.7363  2.7268  2.7361  2.7356\n",
      "   2.7359  2.7143  2.7292  2.7119  2.7187    2.72  2.7293  2.7235  2.7362  2.7236]]\n",
      "\n",
      "1632/2048 [======================>.......] - ETA: 4:46 - loss: 0.0276\n",
      "[[  2.722  2.7197  2.7205  2.7117  2.7244  2.7159  2.7373  2.7283  2.7372  2.7371\n",
      "   2.7372  2.7136  2.7279  2.7121  2.7186  2.7217  2.7279  2.7244   2.737  2.7244]]\n",
      "\n",
      "1664/2048 [=======================>......] - ETA: 4:23 - loss: 0.0271\n",
      "[[ 2.7229  2.7215  2.7233  2.7134  2.7235  2.7171  2.7353  2.7296  2.7355  2.7356\n",
      "   2.7356  2.7158  2.7272  2.7132  2.7183  2.7228  2.7273  2.7239  2.7352  2.7239]]\n",
      "\n",
      "1696/2048 [=======================>......] - ETA: 4:00 - loss: 0.0267\n",
      "[[ 2.7214  2.7203  2.7223  2.7152  2.7244  2.7178  2.7338  2.7265  2.7339  2.7336\n",
      "   2.7339  2.7165  2.7269  2.7166  2.7205  2.7237   2.727   2.725  2.7336  2.7248]]\n",
      "\n",
      "1728/2048 [========================>.....] - ETA: 3:38 - loss: 0.0262\n",
      "[[ 2.7215  2.7204   2.722  2.7172  2.7265  2.7177  2.7341  2.7273  2.7343  2.7339\n",
      "    2.734  2.7166  2.7262  2.7165  2.7212  2.7218  2.7264  2.7268  2.7339  2.7268]]\n",
      "\n",
      "1760/2048 [========================>.....] - ETA: 3:16 - loss: 0.0259\n",
      "[[ 2.7225  2.7198  2.7209  2.7153  2.7261  2.7173  2.7353  2.7264  2.7355   2.735\n",
      "   2.7351  2.7152  2.7272  2.7164  2.7221  2.7239  2.7272  2.7262  2.7351  2.7262]]\n",
      "\n",
      "1792/2048 [=========================>....] - ETA: 2:54 - loss: 0.0255\n",
      "[[ 2.7217  2.7181  2.7194  2.7142  2.7268  2.7157  2.7369  2.7262  2.7368  2.7363\n",
      "   2.7365  2.7131  2.7268  2.7148  2.7216  2.7225  2.7269  2.7266  2.7366  2.7266]]\n",
      "\n",
      "1824/2048 [=========================>....] - ETA: 2:33 - loss: 0.0253\n",
      "[[ 2.7224  2.7172  2.7186  2.7125  2.7256  2.7163   2.737  2.7266  2.7369  2.7363\n",
      "   2.7368  2.7122  2.7248  2.7109  2.7205  2.7203  2.7251  2.7253  2.7367  2.7256]]\n",
      "\n",
      "1856/2048 [==========================>...] - ETA: 2:11 - loss: 0.0250\n",
      "[[ 2.7239  2.7187   2.721  2.7135  2.7255  2.7158  2.7382   2.729  2.7375  2.7378\n",
      "   2.7383  2.7114  2.7247  2.7097  2.7192  2.7208  2.7249  2.7248  2.7382  2.7253]]\n",
      "\n",
      "1888/2048 [==========================>...] - ETA: 1:49 - loss: 0.0248\n",
      "[[ 2.7224   2.717  2.7173  2.7098  2.7238  2.7135  2.7376  2.7271  2.7372   2.737\n",
      "   2.7374  2.7092  2.7241  2.7069  2.7185  2.7183   2.725  2.7234  2.7376  2.7236]]\n",
      "\n",
      "1920/2048 [===========================>..] - ETA: 1:27 - loss: 0.0247\n",
      "[[ 2.7201   2.714  2.7138  2.7049  2.7238  2.7141  2.7376  2.7245  2.7371  2.7373\n",
      "   2.7377  2.7083  2.7251  2.7047  2.7189  2.7175   2.725  2.7233  2.7373  2.7231]]\n",
      "\n",
      "1952/2048 [===========================>..] - ETA: 1:05 - loss: 0.0246\n",
      "[[  2.717  2.7148  2.7162  2.7093   2.727  2.7138  2.7367  2.7252  2.7368   2.737\n",
      "   2.7373  2.7104  2.7261  2.7082  2.7191  2.7169   2.725  2.7262  2.7366   2.726]]\n",
      "\n",
      "1984/2048 [============================>.] - ETA: 43s - loss: 0.0246\n",
      "[[ 2.7158  2.7153  2.7145  2.7048   2.729  2.7114  2.7409  2.7259  2.7412  2.7419\n",
      "   2.7423  2.7088  2.7298   2.707  2.7193  2.7169  2.7286  2.7293  2.7413  2.7281]]\n",
      "\n",
      "2016/2048 [============================>.] - ETA: 21s - loss: 0.0251\n",
      "[[ 2.7069  2.7069  2.7067  2.6974  2.7248  2.7043  2.7386  2.7201  2.7393   2.739\n",
      "   2.7395  2.7038  2.7239  2.6992  2.7129  2.7095  2.7236  2.7259  2.7389  2.7255]]\n",
      "\n",
      "2048/2048 [==============================] - ETA: 0s - loss: 0.0259\n",
      "[[  2.712  2.7102  2.7115  2.7015  2.7343  2.7021  2.7523  2.7287  2.7526  2.7522\n",
      "   2.7523  2.7011   2.726  2.7009  2.7178  2.7177   2.726  2.7345  2.7527   2.734]]\n",
      "\n",
      "2048/2048 [==============================] - 1397s 682ms/step - loss: 0.0259\n",
      "Epoch 9/1000\n",
      "  32/2048 [..............................] - ETA: 22:33 - loss: 0.0879\n",
      "[[ 2.7113  2.7176  2.7058  2.6877  2.7305  2.6824  2.7586  2.7341  2.7587  2.7579\n",
      "   2.7582  2.6908   2.735  2.6891  2.7058   2.706  2.7341  2.7307  2.7586  2.7303]]\n",
      "\n",
      "  64/2048 [..............................] - ETA: 22:40 - loss: 0.1013\n",
      "[[ 2.7156  2.7018  2.6966  2.6701   2.726  2.6768  2.7656  2.7278  2.7654  2.7633\n",
      "   2.7646  2.6708  2.7323  2.6742   2.712  2.7157  2.7317  2.7266  2.7649  2.7264]]\n",
      "\n",
      "  96/2048 [>.............................] - ETA: 22:24 - loss: 0.1193\n",
      "[[ 2.7075  2.7013  2.6949  2.6655  2.7164   2.673  2.7597  2.7261  2.7603  2.7588\n",
      "   2.7599  2.6724  2.7255  2.6663  2.6955  2.6994  2.7263  2.7162  2.7601  2.7172]]\n",
      "\n",
      " 128/2048 [>.............................] - ETA: 22:17 - loss: 0.1170\n",
      "[[ 2.7076   2.704  2.7051  2.6717  2.7159  2.6653  2.7638  2.7336  2.7649  2.7632\n",
      "   2.7637  2.6615  2.7292  2.6671  2.6893     2.7  2.7291  2.7176  2.7642  2.7185]]\n",
      "\n",
      " 160/2048 [=>............................] - ETA: 22:09 - loss: 0.1084\n",
      "[[ 2.7052  2.6998   2.697  2.6744  2.7237  2.6901  2.7571  2.7235  2.7576  2.7567\n",
      "   2.7573  2.6813  2.7261  2.6794  2.7032   2.704  2.7267  2.7254  2.7574  2.7256]]\n",
      "\n",
      " 192/2048 [=>............................] - ETA: 22:11 - loss: 0.0976\n",
      "[[ 2.7021  2.6975  2.6979  2.6729  2.7234  2.6901  2.7551  2.7206   2.755  2.7541\n",
      "   2.7547  2.6809  2.7246  2.6807  2.7036  2.7051  2.7258  2.7237  2.7553  2.7246]]\n",
      "\n",
      " 224/2048 [==>...........................] - ETA: 21:41 - loss: 0.0883\n",
      "[[ 2.7025  2.6981  2.7019  2.6767  2.7184  2.6873  2.7496  2.7225  2.7495  2.7496\n",
      "   2.7496    2.68  2.7188  2.6767  2.6989  2.7021  2.7196  2.7183    2.75  2.7188]]\n",
      "\n",
      " 256/2048 [==>...........................] - ETA: 21:07 - loss: 0.0815\n",
      "[[ 2.7029  2.6994  2.7062  2.6826  2.7193  2.6855  2.7506   2.725  2.7501  2.7503\n",
      "     2.75   2.681  2.7204  2.6824  2.6985  2.7031  2.7222  2.7193  2.7504  2.7199]]\n",
      "\n",
      " 288/2048 [===>..........................] - ETA: 20:26 - loss: 0.0770\n",
      "[[ 2.7014  2.6988  2.7028  2.6845  2.7218  2.6905  2.7462  2.7197  2.7462  2.7457\n",
      "   2.7456  2.6887  2.7228  2.6892  2.7027  2.7045  2.7235  2.7217  2.7457  2.7223]]\n",
      "\n",
      " 320/2048 [===>..........................] - ETA: 19:39 - loss: 0.0736\n",
      "[[ 2.7043  2.6973  2.7069  2.6885  2.7219  2.6902  2.7462  2.7207  2.7468  2.7459\n",
      "    2.746  2.6869    2.72   2.692  2.7048  2.7118  2.7213  2.7214  2.7461  2.7224]]\n",
      "\n",
      " 352/2048 [====>.........................] - ETA: 19:09 - loss: 0.0700\n",
      "[[  2.705  2.7004   2.711  2.6938  2.7249  2.6879  2.7483  2.7246  2.7482  2.7478\n",
      "   2.7476  2.6888  2.7199  2.6948  2.7032  2.7095  2.7204  2.7245  2.7481  2.7252]]\n",
      "\n",
      " 384/2048 [====>.........................] - ETA: 18:48 - loss: 0.0661\n",
      "[[ 2.7098  2.7076  2.7095  2.6927   2.723  2.6948  2.7442  2.7244  2.7441  2.7439\n",
      "   2.7439  2.6931  2.7226  2.6949  2.7079  2.7102  2.7233  2.7225   2.744   2.723]]\n",
      "\n",
      " 416/2048 [=====>........................] - ETA: 18:29 - loss: 0.0626\n",
      "[[ 2.7069   2.711  2.7088  2.6954  2.7219  2.6955  2.7402  2.7217  2.7404  2.7401\n",
      "   2.7403  2.6991  2.7253  2.6994  2.7084  2.7088  2.7256  2.7213  2.7403  2.7221]]\n",
      "\n",
      " 448/2048 [=====>........................] - ETA: 18:12 - loss: 0.0598\n",
      "[[ 2.7134  2.7123  2.7144  2.7013   2.723  2.6996  2.7402  2.7241  2.7406    2.74\n",
      "   2.7401  2.6984  2.7234  2.6993  2.7112  2.7147  2.7241  2.7223  2.7401  2.7227]]\n",
      "\n",
      " 480/2048 [======>.......................] - ETA: 18:00 - loss: 0.0569\n",
      "[[ 2.7142  2.7104  2.7152  2.7013  2.7219  2.7025   2.742  2.7253  2.7424  2.7421\n",
      "    2.742  2.7001  2.7235  2.6995  2.7104  2.7167  2.7236   2.722  2.7421  2.7216]]\n",
      "\n",
      " 512/2048 [======>.......................] - ETA: 17:42 - loss: 0.0542\n",
      "[[ 2.7099  2.7077  2.7119  2.7021  2.7229  2.7045  2.7382  2.7203  2.7386  2.7384\n",
      "   2.7383   2.703  2.7225  2.7032  2.7118  2.7157  2.7224  2.7231  2.7382  2.7232]]\n",
      "\n",
      " 544/2048 [======>.......................] - ETA: 17:17 - loss: 0.0520\n",
      "[[ 2.7077  2.7057  2.7102  2.7029  2.7231  2.7054  2.7356  2.7175  2.7358  2.7355\n",
      "   2.7356  2.7037  2.7205  2.7043  2.7122  2.7143  2.7201  2.7234  2.7356  2.7233]]\n",
      "\n",
      " 576/2048 [=======>......................] - ETA: 16:53 - loss: 0.0505\n",
      "[[ 2.7079  2.7085  2.7121  2.7033  2.7256  2.7033  2.7401  2.7211  2.7401  2.7397\n",
      "     2.74  2.7043  2.7241  2.7041  2.7105  2.7115  2.7236  2.7256  2.7399  2.7258]]\n",
      "\n",
      " 608/2048 [=======>......................] - ETA: 16:22 - loss: 0.0489\n",
      "[[ 2.7105  2.7098   2.715  2.7034  2.7231  2.7006   2.742  2.7252  2.7422  2.7416\n",
      "   2.7417  2.7003  2.7246  2.7018  2.7083  2.7134  2.7243  2.7229  2.7421  2.7231]]\n",
      "\n",
      " 640/2048 [========>.....................] - ETA: 15:56 - loss: 0.0472\n",
      "[[ 2.7159  2.7121  2.7152  2.7028  2.7237   2.705  2.7435  2.7282  2.7433  2.7429\n",
      "   2.7429  2.7025  2.7251  2.7015  2.7125  2.7163  2.7253  2.7239  2.7434  2.7237]]\n",
      "\n",
      " 672/2048 [========>.....................] - ETA: 15:31 - loss: 0.0456\n",
      "[[ 2.7161  2.7097  2.7141  2.7024  2.7215  2.7044  2.7385  2.7252  2.7384  2.7382\n",
      "   2.7383  2.7007  2.7225  2.7009  2.7126  2.7161  2.7225  2.7219  2.7384  2.7214]]\n",
      "\n",
      " 704/2048 [=========>....................] - ETA: 15:09 - loss: 0.0441\n",
      "[[ 2.7162  2.7106  2.7156  2.7033   2.722  2.7025  2.7391  2.7257   2.739   2.738\n",
      "   2.7386   2.701   2.723   2.702  2.7125  2.7166  2.7229  2.7218  2.7389  2.7214]]\n",
      "\n",
      " 736/2048 [=========>....................] - ETA: 14:50 - loss: 0.0426\n",
      "[[ 2.7179  2.7129   2.717  2.7062  2.7211  2.7065  2.7358  2.7256   2.736  2.7353\n",
      "   2.7357  2.7042  2.7217  2.7043  2.7143  2.7183  2.7213  2.7212  2.7358  2.7207]]\n",
      "\n",
      " 768/2048 [==========>...................] - ETA: 14:29 - loss: 0.0410\n",
      "[[ 2.7148   2.714  2.7152   2.706  2.7224  2.7072   2.735  2.7232  2.7349  2.7348\n",
      "   2.7348  2.7081  2.7228  2.7063  2.7126  2.7157  2.7228  2.7225  2.7348   2.722]]\n",
      "\n",
      " 800/2048 [==========>...................] - ETA: 14:09 - loss: 0.0396\n",
      "[[ 2.7136  2.7133  2.7157  2.7085   2.721   2.708  2.7319  2.7216  2.7317  2.7315\n",
      "   2.7315   2.709  2.7202  2.7079  2.7119  2.7152  2.7204  2.7207  2.7316  2.7207]]\n",
      "\n",
      " 832/2048 [===========>..................] - ETA: 13:47 - loss: 0.0383\n",
      "[[ 2.7143  2.7137  2.7146  2.7076  2.7209  2.7084  2.7313  2.7208  2.7309  2.7311\n",
      "   2.7311  2.7094   2.721  2.7081  2.7128  2.7149  2.7212  2.7206  2.7312  2.7205]]\n",
      "\n",
      " 864/2048 [===========>..................] - ETA: 13:25 - loss: 0.0372\n",
      "[[ 2.7156  2.7164  2.7151  2.7085  2.7215  2.7097   2.732  2.7224   2.732  2.7318\n",
      "   2.7319  2.7112  2.7233  2.7092  2.7141  2.7155  2.7231  2.7215  2.7322  2.7216]]\n",
      "\n",
      " 928/2048 [============>.................] - ETA: 12:38 - loss: 0.0351\n",
      "[[ 2.7124  2.7128  2.7116  2.7038  2.7181  2.7059  2.7298  2.7195  2.7297  2.7294\n",
      "   2.7296  2.7078  2.7211  2.7037  2.7098  2.7123  2.7213  2.7185  2.7299  2.7185]]\n",
      "\n",
      " 960/2048 [=============>................] - ETA: 12:15 - loss: 0.0343\n",
      "[[ 2.7152  2.7158  2.7134  2.7067  2.7193  2.7067  2.7305  2.7228  2.7307  2.7309\n",
      "   2.7308  2.7081  2.7228  2.7052  2.7115  2.7126  2.7227  2.7197  2.7309  2.7198]]\n",
      "\n",
      " 992/2048 [=============>................] - ETA: 11:52 - loss: 0.0335\n",
      "[[ 2.7118   2.711  2.7111  2.7014  2.7173  2.7025  2.7318  2.7206  2.7318  2.7315\n",
      "   2.7312  2.7032  2.7213  2.7008  2.7071  2.7087  2.7211   2.717  2.7316  2.7172]]\n",
      "\n",
      "1024/2048 [==============>...............] - ETA: 11:29 - loss: 0.0329\n",
      "[[ 2.7139  2.7104  2.7134  2.7033   2.718  2.7061  2.7325   2.721  2.7327  2.7325\n",
      "   2.7322  2.7049  2.7214  2.7036  2.7095  2.7125  2.7216   2.718  2.7323  2.7184]]\n",
      "\n",
      "1056/2048 [==============>...............] - ETA: 11:09 - loss: 0.0324\n",
      "[[ 2.7167   2.715  2.7157  2.7078  2.7208  2.7038  2.7335  2.7254  2.7334  2.7331\n",
      "   2.7331  2.7025  2.7214  2.7044  2.7112  2.7106  2.7212  2.7213  2.7335  2.7213]]\n",
      "\n",
      "1088/2048 [==============>...............] - ETA: 10:48 - loss: 0.0322\n",
      "[[ 2.7152  2.7113  2.7133  2.7013  2.7221  2.7017  2.7386  2.7238  2.7386  2.7384\n",
      "   2.7386  2.6996  2.7224  2.6972  2.7112  2.7126  2.7221  2.7227  2.7388  2.7226]]\n",
      "\n",
      "1120/2048 [===============>..............] - ETA: 10:26 - loss: 0.0324\n",
      "[[ 2.7066  2.7089  2.7124  2.7002  2.7233   2.702   2.741  2.7238  2.7414  2.7411\n",
      "   2.7417  2.7029  2.7236  2.6983  2.7084  2.7089  2.7225   2.724  2.7411  2.7236]]\n",
      "\n",
      "1152/2048 [===============>..............] - ETA: 10:04 - loss: 0.0327\n",
      "[[ 2.7016  2.7072  2.7078   2.695  2.7212  2.6962  2.7386   2.723  2.7385  2.7391\n",
      "   2.7391  2.7015  2.7218  2.6936   2.701  2.7004   2.721  2.7204  2.7389  2.7204]]\n",
      "\n",
      "1184/2048 [================>.............] - ETA: 9:42 - loss: 0.0332\n",
      "[[ 2.7067  2.7069  2.7022  2.6863  2.7231  2.6993  2.7479  2.7263  2.7477  2.7476\n",
      "   2.7479  2.7032  2.7252   2.687  2.7016  2.7005  2.7244  2.7221   2.748  2.7221]]\n",
      "\n",
      "1216/2048 [================>.............] - ETA: 9:21 - loss: 0.0344\n",
      "[[ 2.7116  2.7092  2.7095  2.6858  2.7269  2.7034   2.758  2.7343  2.7581  2.7586\n",
      "   2.7587  2.7043  2.7273  2.6869  2.7061  2.7074  2.7271  2.7274  2.7581  2.7266]]\n",
      "\n",
      "1248/2048 [=================>............] - ETA: 8:57 - loss: 0.0350\n",
      "[[ 2.7175  2.7144  2.7136  2.6885   2.724  2.7035  2.7568  2.7371  2.7575  2.7576\n",
      "   2.7578  2.7039  2.7319  2.6842   2.707  2.7066  2.7313  2.7231  2.7571  2.7245]]\n",
      "\n",
      "1280/2048 [=================>............] - ETA: 8:34 - loss: 0.0356\n",
      "[[ 2.7106  2.7125  2.7177  2.6981  2.7347  2.7058  2.7645  2.7395  2.7646  2.7646\n",
      "   2.7648  2.7129   2.737  2.6952  2.7082  2.7104  2.7369  2.7342  2.7646  2.7347]]\n",
      "\n",
      "1312/2048 [==================>...........] - ETA: 8:12 - loss: 0.0360\n",
      "[[ 2.7092  2.6981  2.7142  2.6978  2.7335  2.7114  2.7597  2.7305  2.7597  2.7601\n",
      "   2.7599  2.7027  2.7234  2.6903  2.7134  2.7181  2.7225  2.7327  2.7597  2.7326]]\n",
      "\n",
      "1344/2048 [==================>...........] - ETA: 7:49 - loss: 0.0363\n",
      "[[ 2.7148  2.7094  2.7201  2.6999  2.7285  2.7078  2.7598   2.734  2.7598  2.7601\n",
      "   2.7592  2.7066  2.7319  2.6972  2.7097  2.7216  2.7313  2.7275  2.7595  2.7279]]\n",
      "\n",
      "1376/2048 [===================>..........] - ETA: 7:28 - loss: 0.0366\n",
      "[[ 2.7103  2.7048  2.7181  2.6999  2.7309  2.7073  2.7586  2.7351  2.7589  2.7588\n",
      "   2.7583  2.7039  2.7272  2.6924  2.7083  2.7131  2.7264  2.7291  2.7583  2.7298]]\n",
      "\n",
      "1408/2048 [===================>..........] - ETA: 7:07 - loss: 0.0370\n",
      "[[ 2.7075  2.6965  2.7071   2.695  2.7321  2.7119  2.7524  2.7249  2.7531  2.7532\n",
      "   2.7529  2.7058  2.7227  2.6922  2.7131  2.7104  2.7206  2.7299  2.7526  2.7314]]\n",
      "\n",
      "1440/2048 [====================>.........] - ETA: 6:46 - loss: 0.0372\n",
      "[[ 2.7081   2.703  2.7115  2.6961  2.7282  2.7061  2.7541  2.7291  2.7536  2.7542\n",
      "   2.7543  2.7041  2.7249  2.6928  2.7086  2.7095  2.7248  2.7279  2.7541  2.7275]]\n",
      "\n",
      "1472/2048 [====================>.........] - ETA: 6:24 - loss: 0.0375\n",
      "[[ 2.7074  2.7042  2.7083   2.693  2.7268  2.7075  2.7517  2.7251  2.7512  2.7513\n",
      "   2.7518  2.7061  2.7272  2.6949  2.7097  2.7109  2.7272  2.7268  2.7515  2.7266]]\n",
      "\n",
      "1504/2048 [=====================>........] - ETA: 6:03 - loss: 0.0374\n",
      "[[ 2.7082  2.7066  2.7108  2.6995  2.7328  2.7124  2.7501  2.7266  2.7504  2.7499\n",
      "   2.7503  2.7098  2.7255  2.6999  2.7139  2.7109  2.7252  2.7329  2.7501   2.732]]\n",
      "\n",
      "1512/2048 [=====================>........] - ETA: 5:58 - loss: 0.0374"
     ]
    }
   ],
   "source": [
    "history = model_loss.fit([x0, dW, dN], target, batch_size=32, initial_epoch=0, epochs=1000, callbacks=callbacks, shuffle=False)\n",
    "df_loss = pd.DataFrame(history.history['loss'])\n",
    "df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15728 calls to <function build_model.<locals>.hx at 0x7f81203c4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15728 calls to <function build_model.<locals>.hx at 0x7f81203c4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15728 calls to <function build_model.<locals>.hy at 0x7f81203c4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15728 calls to <function build_model.<locals>.hy at 0x7f81203c4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15729 calls to <function build_model.<locals>.hx at 0x7f81203c4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15729 calls to <function build_model.<locals>.hx at 0x7f81203c4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15729 calls to <function build_model.<locals>.hy at 0x7f81203c4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15729 calls to <function build_model.<locals>.hy at 0x7f81203c4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 20), dtype=float32, numpy=\n",
       "array([[-0.36553, -0.029806, -0.038016, -0.12811, -0.087944, -0.2571, -0.020347,\n",
       "        -0.10123, -0.081024, -0.25093, -0.083344, -0.16077, 0.093976, -0.021119,\n",
       "        0.055735, -0.28888, -0.0015695, -0.11305, -0.19566, 0.070627],\n",
       "       [-0.089612, 0.0020633, 0.04134, -0.17079, -0.19602, -0.28469, -0.35105, -0.37901,\n",
       "        -0.19069, -0.31459, -0.065847, -0.20243, -0.086986, -0.16529, -0.19942, 0.12533,\n",
       "        -0.16334, -0.19621, -0.0995, -0.081897],\n",
       "       [-0.19206, 0.21398, 0.12785, -0.050076, -0.24237, -0.070113, -0.39388, -0.10904,\n",
       "        -0.15356, -0.35116, -0.13377, 0.22848, -0.08092, -0.0054543, 0.040828, -0.19012,\n",
       "        -0.045498, -0.098609, -0.16785, 0.11103],\n",
       "       [-0.11195, -0.023458, -0.098779, -0.050293, -0.40543, -0.11394, -0.42345,\n",
       "        0.050131, -0.17321, -0.38357, -0.094167, 0.064404, -0.20383, -0.017039,\n",
       "        -0.04739, 0.26115,  0.1042, -0.05389, -0.4997, -0.038131],\n",
       "       [-0.072355, 0.081953, -0.081077, -0.26793, 0.069514, -0.1014, -0.0059988,\n",
       "        0.17636, -0.17462, -0.054312, 0.16497, -0.020587, 0.10186, 0.11938, 0.038918,\n",
       "        0.0020339, 0.27612, 0.00087929, -0.018268, 0.06659],\n",
       "       [-0.2007, -0.31256, -0.20453, -0.27738, 0.08399, -0.16905, 0.02135, -0.028212,\n",
       "        -0.24603, -0.1447, 0.038892, -0.39373, 0.02102, 0.093797, 0.51076, 0.064141,\n",
       "        -0.43324, -0.079059, -0.23097, 0.10064],\n",
       "       [-0.1646, -0.04939, -0.12481, -0.15126, -0.096141, -0.11337, -0.045651, -0.25893,\n",
       "        -0.20849, -0.25377, -0.18276, -0.034668, -0.028732, -0.19667, -0.19308,\n",
       "        -0.35553, -0.13573, -0.10397, -0.26445, -0.069731],\n",
       "       [-0.39899, -0.51485, -0.031861, -0.093412, 0.073376, -0.020634, -0.16785,\n",
       "        -0.09335, -0.025495, -0.20029, -0.026458, -0.17817, -0.084926, -0.031542,\n",
       "        0.092947, -0.38166, 0.19964, -0.12549, -0.41877, -0.046093],\n",
       "       [-0.18309, -0.27566, -0.092859, -0.43943, 0.02922, 0.11344, -0.22723, -0.17126,\n",
       "        -0.12282, -0.10795, -0.18276, -0.094867, 0.032041, 0.016216, -0.16716, -0.10312,\n",
       "        0.073626, -0.09308, -0.044889, 0.44424],\n",
       "       [-0.1926, -0.03359, -0.059261, -0.1628, -0.089389, -0.16497, -0.27032, 0.082008,\n",
       "        -0.074584, -0.09718, -0.11125, -0.069922, -0.21319, -0.027022, -0.19388,\n",
       "        0.065399, -0.12707, -0.16434, -0.33324, -0.026521]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss([x0[:10], dW[:10], dN[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "* Alpha has to grow with number of dimensions to avoid explosion\n",
    "* Number of samples has to grow with number of dimensions to converge to Y_0, loss will be close to zero if the number of samples is too small. What is a good number of samples?\n",
    "* Too little samples leads to plain memorizing overfitting. Could this be a theorem? Maybe reduce the size of the neural net to avoid overfitting, it's overparameterized? Can we quantify overfitting?\n",
    "* Perhaps come up with another example with closed solution that satisfies monotonicity property"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
