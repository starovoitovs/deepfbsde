{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup\n",
    "\n",
    "Consider\n",
    "\n",
    "$$\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "W \\in \\R ^m, N \\in \\R^\\ell\\\\\n",
    "X\\in \\R^d, Y\\in \\R^n, Z\\in \\R^{n\\times m}, R \\in \\R ^{n\\times \\ell}\\\\\n",
    "\\gamma \\in \\R^{d \\times \\ell}, \\sigma \\in \\R^{d\\times m} \\\\\n",
    "f\\in C^2(\\R^d, E\\subset\\R^n)\\\\\n",
    "$$\n",
    "\n",
    "For the control problem with jumps, we would have the FBSDE of the form (assuming zero drift on $X$)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\sigma_t dW_t + \\gamma_t dN_t\\\\\n",
    "dY_t &= (\\dots)dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assume that $Y=f(X)$. Necessarily, by Ito's lemma (Oksendal, p.9, 1.2.8; for k-th column):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_t &= (D f(X_t))^T \\cdot \\sigma_t \\\\\n",
    "R^{(\\cdot k)}_t &= f\\left(X_t+\\gamma ^{(\\cdot k)}_t\\right) - f(X_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assuming that $D f(X) D f(X)^T$ is never singular and $f^{-1}\\in C^2(E, \\R^d)$ exists, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_t &= \\left(D f(X_t) D f(X_t) ^T \\right) ^{-1} D f(X_t) Z =: \\beta(X_t) Z_t\\\\\n",
    "\\gamma^{(\\cdot k)}_t &= f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for $\\beta(X) = \\left(D f(X) D f(X) ^T \\right) ^{-1} D f(X)$. Thus we obtain by Ito's lemma (note that $D^2 f$ is a 3d-tensor indexed by $i,j,k$ with $k\\in 1,\\dots, n$)\n",
    "\n",
    "$$\n",
    "(\\dots)dt = \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt\n",
    "$$\n",
    "\n",
    "Altogether, we can rewrite\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\beta(X_t) Z_t dW_t + \\left(f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(X)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= \\delta_{ij} \\exp(X^{(i)}) =\\operatorname{diag}(\\exp(X))\\\\\n",
    "D^2 f(X)^{(ijk)} &= \\delta_{ijk} \\exp(X^{(i)})\\\\\n",
    "\\beta(X) &= \\delta_{ij}\\exp(-X^{(i)}) = \\operatorname{diag}(\\exp(-X))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\operatorname{diag}(\\exp(-X)) Z_t dW_t + \\left(\\log\\left(\\exp(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\operatorname{diag} ( \\operatorname{diag}(\\exp(-X)) Z_t( \\operatorname{diag}(\\exp(-X)) Z_t)^T) \\exp(X_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(X_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1a\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(aX)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\frac 1a \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= a\\delta_{ij} \\exp(aX^{(i)}) = a\\operatorname{diag}(\\exp(aX))\\\\\n",
    "D^2 f(X)^{(ijk)} &= a^2 \\delta_{ijk} \\exp(aX^{(i)})\\\\\n",
    "\\beta(X) &= \\frac 1a \\delta_{ij}\\exp(-aX^{(i)}) = \\frac 1a \\operatorname{diag}(\\exp(-aX))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\frac 1a \\operatorname{diag}(\\exp(-aX)) Z_t dW_t + \\left(\\frac 1a \\log\\left(\\exp(aX_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac {1}{2} \\operatorname{diag} ( \\operatorname{diag}(\\exp(-aX)) Z_t( \\operatorname{diag}(\\exp(-aX)) Z_t)^T) \\exp(aX_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(aX_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 15:02:01.560881: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-17 15:02:01.665740: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-17 15:02:01.698842: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-17 15:02:02.257262: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-17 15:02:02.257327: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-17 15:02:02.257332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer, BatchNormalization, Add\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.metrics import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gpu_utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(pick_gpu_lowest_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=11, linewidth=90, formatter=dict(float=lambda x: \"%7.5g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{timestamp}\"\n",
    "tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "output_dir = f\"_output/models/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_paths = 2 ** 14\n",
    "n_timesteps = 16\n",
    "time_horizon = 1.\n",
    "n_x_dimensions = 20\n",
    "n_y_dimensions = 20\n",
    "n_diffusion_factors = 20\n",
    "n_jump_factors = 20\n",
    "intensity = 5.\n",
    "alpha = 4.\n",
    "stddev = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model parameters\n",
    "# n_paths = 2 ** 12\n",
    "# n_timesteps = 4\n",
    "# time_horizon = 1.\n",
    "# n_x_dimensions = 2\n",
    "# n_y_dimensions = 2\n",
    "# n_diffusion_factors = 2\n",
    "# n_jump_factors = 2\n",
    "# intensity = 5.\n",
    "# alpha = 4.\n",
    "# stddev = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = time_horizon / n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    return tf.zeros((n_x_dimensions,))\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z) / alpha\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    # floor the log argument\n",
    "    res = tf.maximum(tf.exp(alpha * x) + r, 1e-2)\n",
    "    return (tf.math.log(res) / alpha - x)\n",
    "\n",
    "def f(t, x, y, z, r):\n",
    "    res = tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z)\n",
    "    return 0.5 * tf.einsum('ij,j', tf.linalg.matmul(res, res, transpose_b=True), tf.exp(alpha * x))\n",
    "\n",
    "def g(x):\n",
    "    return tf.exp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Custom layers and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Y0Callback(Callback):\n",
    "    \n",
    "    def __init__(self, filepath=None, freq=32):\n",
    "        super(Y0Callback, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.batch = 1\n",
    "        self.freq = freq\n",
    "        self.y0s = np.zeros((0, n_y_dimensions))\n",
    "    \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        y0 = self.model.get_layer('y_0').y0.numpy()\n",
    "        self.y0s = np.append(self.y0s, y0, axis=0)\n",
    "        if self.batch % self.freq == 0:\n",
    "            print(f\"\\n{y0}\\n\")\n",
    "        self.batch += 1\n",
    "\n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        self.batch = 1\n",
    "        \n",
    "#     def on_train_end(self, logs=None):\n",
    "#         if self.filepath is not None:\n",
    "#             pd.DataFrame(self.y0s).to_csv(self.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSave(Callback):\n",
    "    def __init__(self, directory):\n",
    "        self.batch = 1\n",
    "        self.epoch = 1\n",
    "        self.directory = directory\n",
    "        shutil.rmtree(directory, ignore_errors=True)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        self.epoch += 1\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        filename = os.path.join(self.directory, f\"weights_{self.epoch:03}_{self.batch:03}.h5\")\n",
    "        self.model.save_weights(filename)\n",
    "        self.batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_x_dimensions, n_y_dimensions, n_diffusion_factors, n_jump_factors, n_timesteps, time_horizon):\n",
    "\n",
    "    dt = time_horizon / n_timesteps\n",
    "\n",
    "    def dX(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', s(t, x, y, z, r), dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', v(t, x, y, z, r), dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    def dY(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', z, dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', r, dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))        \n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    @tf.function\n",
    "    def hx(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return x + dX(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    @tf.function\n",
    "    def hy(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return y + dY(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    #n_hidden_units = n_x_dimensions + n_diffusion_factors + n_jump_factors + 10\n",
    "    n_hidden_units = 5\n",
    "\n",
    "    inputs_x0 = Input(shape=(n_x_dimensions))\n",
    "    inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "    inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "\n",
    "    # constant x0\n",
    "\n",
    "    x0 = tf.Variable([[1. for _ in range(n_x_dimensions)]], trainable=False)\n",
    "    y0 = tf.Variable([[0. for _ in range(n_y_dimensions)]], trainable=True)\n",
    "\n",
    "    x = InitialValue(x0, trainable=False, name='x_0')(inputs_dW)\n",
    "    y = InitialValue(y0, trainable=True, name='y_0')(inputs_dW)\n",
    "\n",
    "    # adjoints\n",
    "\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z1_0')(z)\n",
    "    z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z2_0')(z)\n",
    "    z = BatchNormalization(name='zbn_0')(z)\n",
    "    z = Reshape((n_y_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r1_0')(r)\n",
    "    r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r2_0')(r)\n",
    "    r = BatchNormalization(name='rbn_0')(r)\n",
    "    r = Reshape((n_y_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "\n",
    "    # pre-compile lambda layers\n",
    "    \n",
    "    for i in range(n_timesteps):\n",
    "\n",
    "        step = InitialValue(tf.Variable(i, dtype=tf.float32, trainable=False))(inputs_dW)\n",
    "\n",
    "        dW = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dW, step])\n",
    "        dN = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dN, step])\n",
    "\n",
    "        x, y = (\n",
    "            Lambda(hx, name=f'x_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "            Lambda(hy, name=f'y_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "        )\n",
    "\n",
    "        # we don't train z for the last time step; keep for consistency\n",
    "        z = concatenate([x, y])\n",
    "        z = Dense(n_hidden_units, activation='relu', name=f'z1_{i+1}')(z)\n",
    "        z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "        z = Reshape((n_y_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "        z = BatchNormalization(name=f'zbn_{i+1}')(z)\n",
    "\n",
    "        # we don't train r for the last time step; keep for consistency\n",
    "        r = concatenate([x, y])\n",
    "        r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r1_{i+1}')(r)\n",
    "        r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r2_{i+1}')(r)\n",
    "        r = Reshape((n_y_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "        r = BatchNormalization(name=f'rbn_{i+1}')(r)\n",
    "\n",
    "        paths += [[x, y, z, r]]\n",
    "\n",
    "    outputs_loss = Lambda(lambda r: r[1] - tf.vectorized_map(g, r[0]))([x, y])\n",
    "    \n",
    "    # remember that z and r are matrices\n",
    "    outputs_paths = tf.stack(\n",
    "        [tf.stack([p[0] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "        [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "    model_loss = Model([inputs_x0, inputs_dW, inputs_dN], outputs_loss)\n",
    "\n",
    "    # (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "    # skips the first time step\n",
    "    model_paths = Model([inputs_x0, inputs_dW, inputs_dN], outputs_paths)\n",
    "\n",
    "    return model_loss, model_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "dt = time_horizon / n_timesteps\n",
    "model_loss, model_paths = build_model(n_x_dimensions=n_x_dimensions,\n",
    "                                      n_y_dimensions=n_y_dimensions,\n",
    "                                      n_diffusion_factors=n_diffusion_factors,\n",
    "                                      n_jump_factors=n_jump_factors,\n",
    "                                      n_timesteps=n_timesteps,\n",
    "                                      time_horizon=time_horizon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 16, 20)]     0           []                               \n",
      "                                                                                                  \n",
      " x_0 (InitialValue)             (1, 20)              20          ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " y_0 (InitialValue)             (1, 20)              20          ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_152 (Concatenate)  (1, 40)              0           ['x_0[0][0]',                    \n",
      "                                                                  'y_0[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_153 (Concatenate)  (1, 40)              0           ['x_0[0][0]',                    \n",
      "                                                                  'y_0[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_0 (Dense)                   (1, 5)               205         ['concatenate_152[0][0]']        \n",
      "                                                                                                  \n",
      " r1_0 (Dense)                   (1, 5)               205         ['concatenate_153[0][0]']        \n",
      "                                                                                                  \n",
      " z2_0 (Dense)                   (1, 400)             2400        ['z1_0[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_0 (Dense)                   (1, 400)             2400        ['r1_0[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_68 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zbn_0 (BatchNormalization)     (1, 400)             1600        ['z2_0[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_0 (BatchNormalization)     (1, 400)             1600        ['r2_0[0][0]']                   \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)          [(None, 16, 20)]     0           []                               \n",
      "                                                                                                  \n",
      " zr_0 (Reshape)                 (1, 20, 20)          0           ['zbn_0[0][0]']                  \n",
      "                                                                                                  \n",
      " rr_0 (Reshape)                 (1, 20, 20)          0           ['rbn_0[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_144 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_68[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_145 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_68[0][0]']       \n",
      "                                                                                                  \n",
      " x_1 (Lambda)                   (None, 20)           0           ['initial_value_68[0][0]',       \n",
      "                                                                  'x_0[0][0]',                    \n",
      "                                                                  'y_0[0][0]',                    \n",
      "                                                                  'zr_0[0][0]',                   \n",
      "                                                                  'rr_0[0][0]',                   \n",
      "                                                                  'lambda_144[0][0]',             \n",
      "                                                                  'lambda_145[0][0]']             \n",
      "                                                                                                  \n",
      " y_1 (Lambda)                   (None, 20)           0           ['initial_value_68[0][0]',       \n",
      "                                                                  'x_0[0][0]',                    \n",
      "                                                                  'y_0[0][0]',                    \n",
      "                                                                  'zr_0[0][0]',                   \n",
      "                                                                  'rr_0[0][0]',                   \n",
      "                                                                  'lambda_144[0][0]',             \n",
      "                                                                  'lambda_145[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_154 (Concatenate)  (None, 40)           0           ['x_1[0][0]',                    \n",
      "                                                                  'y_1[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_155 (Concatenate)  (None, 40)           0           ['x_1[0][0]',                    \n",
      "                                                                  'y_1[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_1 (Dense)                   (None, 5)            205         ['concatenate_154[0][0]']        \n",
      "                                                                                                  \n",
      " r1_1 (Dense)                   (None, 5)            205         ['concatenate_155[0][0]']        \n",
      "                                                                                                  \n",
      " z2_1 (Dense)                   (None, 400)          2400        ['z1_1[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_1 (Dense)                   (None, 400)          2400        ['r1_1[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_69 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_1 (Reshape)                 (None, 20, 20)       0           ['z2_1[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_1 (Reshape)                 (None, 20, 20)       0           ['r2_1[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_1 (BatchNormalization)     (None, 20, 20)       80          ['zr_1[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_1 (BatchNormalization)     (None, 20, 20)       80          ['rr_1[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_146 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_69[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_147 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_69[0][0]']       \n",
      "                                                                                                  \n",
      " x_2 (Lambda)                   (None, 20)           0           ['initial_value_69[0][0]',       \n",
      "                                                                  'x_1[0][0]',                    \n",
      "                                                                  'y_1[0][0]',                    \n",
      "                                                                  'zbn_1[0][0]',                  \n",
      "                                                                  'rbn_1[0][0]',                  \n",
      "                                                                  'lambda_146[0][0]',             \n",
      "                                                                  'lambda_147[0][0]']             \n",
      "                                                                                                  \n",
      " y_2 (Lambda)                   (None, 20)           0           ['initial_value_69[0][0]',       \n",
      "                                                                  'x_1[0][0]',                    \n",
      "                                                                  'y_1[0][0]',                    \n",
      "                                                                  'zbn_1[0][0]',                  \n",
      "                                                                  'rbn_1[0][0]',                  \n",
      "                                                                  'lambda_146[0][0]',             \n",
      "                                                                  'lambda_147[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_156 (Concatenate)  (None, 40)           0           ['x_2[0][0]',                    \n",
      "                                                                  'y_2[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_157 (Concatenate)  (None, 40)           0           ['x_2[0][0]',                    \n",
      "                                                                  'y_2[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_2 (Dense)                   (None, 5)            205         ['concatenate_156[0][0]']        \n",
      "                                                                                                  \n",
      " r1_2 (Dense)                   (None, 5)            205         ['concatenate_157[0][0]']        \n",
      "                                                                                                  \n",
      " z2_2 (Dense)                   (None, 400)          2400        ['z1_2[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_2 (Dense)                   (None, 400)          2400        ['r1_2[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_70 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_2 (Reshape)                 (None, 20, 20)       0           ['z2_2[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_2 (Reshape)                 (None, 20, 20)       0           ['r2_2[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_2 (BatchNormalization)     (None, 20, 20)       80          ['zr_2[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_2 (BatchNormalization)     (None, 20, 20)       80          ['rr_2[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_148 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_70[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_149 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_70[0][0]']       \n",
      "                                                                                                  \n",
      " x_3 (Lambda)                   (None, 20)           0           ['initial_value_70[0][0]',       \n",
      "                                                                  'x_2[0][0]',                    \n",
      "                                                                  'y_2[0][0]',                    \n",
      "                                                                  'zbn_2[0][0]',                  \n",
      "                                                                  'rbn_2[0][0]',                  \n",
      "                                                                  'lambda_148[0][0]',             \n",
      "                                                                  'lambda_149[0][0]']             \n",
      "                                                                                                  \n",
      " y_3 (Lambda)                   (None, 20)           0           ['initial_value_70[0][0]',       \n",
      "                                                                  'x_2[0][0]',                    \n",
      "                                                                  'y_2[0][0]',                    \n",
      "                                                                  'zbn_2[0][0]',                  \n",
      "                                                                  'rbn_2[0][0]',                  \n",
      "                                                                  'lambda_148[0][0]',             \n",
      "                                                                  'lambda_149[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_158 (Concatenate)  (None, 40)           0           ['x_3[0][0]',                    \n",
      "                                                                  'y_3[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_159 (Concatenate)  (None, 40)           0           ['x_3[0][0]',                    \n",
      "                                                                  'y_3[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_3 (Dense)                   (None, 5)            205         ['concatenate_158[0][0]']        \n",
      "                                                                                                  \n",
      " r1_3 (Dense)                   (None, 5)            205         ['concatenate_159[0][0]']        \n",
      "                                                                                                  \n",
      " z2_3 (Dense)                   (None, 400)          2400        ['z1_3[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_3 (Dense)                   (None, 400)          2400        ['r1_3[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_71 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_3 (Reshape)                 (None, 20, 20)       0           ['z2_3[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_3 (Reshape)                 (None, 20, 20)       0           ['r2_3[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_3 (BatchNormalization)     (None, 20, 20)       80          ['zr_3[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_3 (BatchNormalization)     (None, 20, 20)       80          ['rr_3[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_150 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_71[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_151 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_71[0][0]']       \n",
      "                                                                                                  \n",
      " x_4 (Lambda)                   (None, 20)           0           ['initial_value_71[0][0]',       \n",
      "                                                                  'x_3[0][0]',                    \n",
      "                                                                  'y_3[0][0]',                    \n",
      "                                                                  'zbn_3[0][0]',                  \n",
      "                                                                  'rbn_3[0][0]',                  \n",
      "                                                                  'lambda_150[0][0]',             \n",
      "                                                                  'lambda_151[0][0]']             \n",
      "                                                                                                  \n",
      " y_4 (Lambda)                   (None, 20)           0           ['initial_value_71[0][0]',       \n",
      "                                                                  'x_3[0][0]',                    \n",
      "                                                                  'y_3[0][0]',                    \n",
      "                                                                  'zbn_3[0][0]',                  \n",
      "                                                                  'rbn_3[0][0]',                  \n",
      "                                                                  'lambda_150[0][0]',             \n",
      "                                                                  'lambda_151[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_160 (Concatenate)  (None, 40)           0           ['x_4[0][0]',                    \n",
      "                                                                  'y_4[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_161 (Concatenate)  (None, 40)           0           ['x_4[0][0]',                    \n",
      "                                                                  'y_4[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_4 (Dense)                   (None, 5)            205         ['concatenate_160[0][0]']        \n",
      "                                                                                                  \n",
      " r1_4 (Dense)                   (None, 5)            205         ['concatenate_161[0][0]']        \n",
      "                                                                                                  \n",
      " z2_4 (Dense)                   (None, 400)          2400        ['z1_4[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_4 (Dense)                   (None, 400)          2400        ['r1_4[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_72 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_4 (Reshape)                 (None, 20, 20)       0           ['z2_4[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_4 (Reshape)                 (None, 20, 20)       0           ['r2_4[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_4 (BatchNormalization)     (None, 20, 20)       80          ['zr_4[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_4 (BatchNormalization)     (None, 20, 20)       80          ['rr_4[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_152 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_72[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_153 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_72[0][0]']       \n",
      "                                                                                                  \n",
      " x_5 (Lambda)                   (None, 20)           0           ['initial_value_72[0][0]',       \n",
      "                                                                  'x_4[0][0]',                    \n",
      "                                                                  'y_4[0][0]',                    \n",
      "                                                                  'zbn_4[0][0]',                  \n",
      "                                                                  'rbn_4[0][0]',                  \n",
      "                                                                  'lambda_152[0][0]',             \n",
      "                                                                  'lambda_153[0][0]']             \n",
      "                                                                                                  \n",
      " y_5 (Lambda)                   (None, 20)           0           ['initial_value_72[0][0]',       \n",
      "                                                                  'x_4[0][0]',                    \n",
      "                                                                  'y_4[0][0]',                    \n",
      "                                                                  'zbn_4[0][0]',                  \n",
      "                                                                  'rbn_4[0][0]',                  \n",
      "                                                                  'lambda_152[0][0]',             \n",
      "                                                                  'lambda_153[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_162 (Concatenate)  (None, 40)           0           ['x_5[0][0]',                    \n",
      "                                                                  'y_5[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_163 (Concatenate)  (None, 40)           0           ['x_5[0][0]',                    \n",
      "                                                                  'y_5[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_5 (Dense)                   (None, 5)            205         ['concatenate_162[0][0]']        \n",
      "                                                                                                  \n",
      " r1_5 (Dense)                   (None, 5)            205         ['concatenate_163[0][0]']        \n",
      "                                                                                                  \n",
      " z2_5 (Dense)                   (None, 400)          2400        ['z1_5[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_5 (Dense)                   (None, 400)          2400        ['r1_5[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_73 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_5 (Reshape)                 (None, 20, 20)       0           ['z2_5[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_5 (Reshape)                 (None, 20, 20)       0           ['r2_5[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_5 (BatchNormalization)     (None, 20, 20)       80          ['zr_5[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_5 (BatchNormalization)     (None, 20, 20)       80          ['rr_5[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_154 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_73[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_155 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_73[0][0]']       \n",
      "                                                                                                  \n",
      " x_6 (Lambda)                   (None, 20)           0           ['initial_value_73[0][0]',       \n",
      "                                                                  'x_5[0][0]',                    \n",
      "                                                                  'y_5[0][0]',                    \n",
      "                                                                  'zbn_5[0][0]',                  \n",
      "                                                                  'rbn_5[0][0]',                  \n",
      "                                                                  'lambda_154[0][0]',             \n",
      "                                                                  'lambda_155[0][0]']             \n",
      "                                                                                                  \n",
      " y_6 (Lambda)                   (None, 20)           0           ['initial_value_73[0][0]',       \n",
      "                                                                  'x_5[0][0]',                    \n",
      "                                                                  'y_5[0][0]',                    \n",
      "                                                                  'zbn_5[0][0]',                  \n",
      "                                                                  'rbn_5[0][0]',                  \n",
      "                                                                  'lambda_154[0][0]',             \n",
      "                                                                  'lambda_155[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_164 (Concatenate)  (None, 40)           0           ['x_6[0][0]',                    \n",
      "                                                                  'y_6[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_165 (Concatenate)  (None, 40)           0           ['x_6[0][0]',                    \n",
      "                                                                  'y_6[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_6 (Dense)                   (None, 5)            205         ['concatenate_164[0][0]']        \n",
      "                                                                                                  \n",
      " r1_6 (Dense)                   (None, 5)            205         ['concatenate_165[0][0]']        \n",
      "                                                                                                  \n",
      " z2_6 (Dense)                   (None, 400)          2400        ['z1_6[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_6 (Dense)                   (None, 400)          2400        ['r1_6[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_74 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_6 (Reshape)                 (None, 20, 20)       0           ['z2_6[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_6 (Reshape)                 (None, 20, 20)       0           ['r2_6[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_6 (BatchNormalization)     (None, 20, 20)       80          ['zr_6[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_6 (BatchNormalization)     (None, 20, 20)       80          ['rr_6[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_156 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_74[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_157 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_74[0][0]']       \n",
      "                                                                                                  \n",
      " x_7 (Lambda)                   (None, 20)           0           ['initial_value_74[0][0]',       \n",
      "                                                                  'x_6[0][0]',                    \n",
      "                                                                  'y_6[0][0]',                    \n",
      "                                                                  'zbn_6[0][0]',                  \n",
      "                                                                  'rbn_6[0][0]',                  \n",
      "                                                                  'lambda_156[0][0]',             \n",
      "                                                                  'lambda_157[0][0]']             \n",
      "                                                                                                  \n",
      " y_7 (Lambda)                   (None, 20)           0           ['initial_value_74[0][0]',       \n",
      "                                                                  'x_6[0][0]',                    \n",
      "                                                                  'y_6[0][0]',                    \n",
      "                                                                  'zbn_6[0][0]',                  \n",
      "                                                                  'rbn_6[0][0]',                  \n",
      "                                                                  'lambda_156[0][0]',             \n",
      "                                                                  'lambda_157[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_166 (Concatenate)  (None, 40)           0           ['x_7[0][0]',                    \n",
      "                                                                  'y_7[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_167 (Concatenate)  (None, 40)           0           ['x_7[0][0]',                    \n",
      "                                                                  'y_7[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_7 (Dense)                   (None, 5)            205         ['concatenate_166[0][0]']        \n",
      "                                                                                                  \n",
      " r1_7 (Dense)                   (None, 5)            205         ['concatenate_167[0][0]']        \n",
      "                                                                                                  \n",
      " z2_7 (Dense)                   (None, 400)          2400        ['z1_7[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_7 (Dense)                   (None, 400)          2400        ['r1_7[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_75 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_7 (Reshape)                 (None, 20, 20)       0           ['z2_7[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_7 (Reshape)                 (None, 20, 20)       0           ['r2_7[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_7 (BatchNormalization)     (None, 20, 20)       80          ['zr_7[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_7 (BatchNormalization)     (None, 20, 20)       80          ['rr_7[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_158 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_75[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_159 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_75[0][0]']       \n",
      "                                                                                                  \n",
      " x_8 (Lambda)                   (None, 20)           0           ['initial_value_75[0][0]',       \n",
      "                                                                  'x_7[0][0]',                    \n",
      "                                                                  'y_7[0][0]',                    \n",
      "                                                                  'zbn_7[0][0]',                  \n",
      "                                                                  'rbn_7[0][0]',                  \n",
      "                                                                  'lambda_158[0][0]',             \n",
      "                                                                  'lambda_159[0][0]']             \n",
      "                                                                                                  \n",
      " y_8 (Lambda)                   (None, 20)           0           ['initial_value_75[0][0]',       \n",
      "                                                                  'x_7[0][0]',                    \n",
      "                                                                  'y_7[0][0]',                    \n",
      "                                                                  'zbn_7[0][0]',                  \n",
      "                                                                  'rbn_7[0][0]',                  \n",
      "                                                                  'lambda_158[0][0]',             \n",
      "                                                                  'lambda_159[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_168 (Concatenate)  (None, 40)           0           ['x_8[0][0]',                    \n",
      "                                                                  'y_8[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_169 (Concatenate)  (None, 40)           0           ['x_8[0][0]',                    \n",
      "                                                                  'y_8[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_8 (Dense)                   (None, 5)            205         ['concatenate_168[0][0]']        \n",
      "                                                                                                  \n",
      " r1_8 (Dense)                   (None, 5)            205         ['concatenate_169[0][0]']        \n",
      "                                                                                                  \n",
      " z2_8 (Dense)                   (None, 400)          2400        ['z1_8[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_8 (Dense)                   (None, 400)          2400        ['r1_8[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_76 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_8 (Reshape)                 (None, 20, 20)       0           ['z2_8[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_8 (Reshape)                 (None, 20, 20)       0           ['r2_8[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_8 (BatchNormalization)     (None, 20, 20)       80          ['zr_8[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_8 (BatchNormalization)     (None, 20, 20)       80          ['rr_8[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_160 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_76[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_161 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_76[0][0]']       \n",
      "                                                                                                  \n",
      " x_9 (Lambda)                   (None, 20)           0           ['initial_value_76[0][0]',       \n",
      "                                                                  'x_8[0][0]',                    \n",
      "                                                                  'y_8[0][0]',                    \n",
      "                                                                  'zbn_8[0][0]',                  \n",
      "                                                                  'rbn_8[0][0]',                  \n",
      "                                                                  'lambda_160[0][0]',             \n",
      "                                                                  'lambda_161[0][0]']             \n",
      "                                                                                                  \n",
      " y_9 (Lambda)                   (None, 20)           0           ['initial_value_76[0][0]',       \n",
      "                                                                  'x_8[0][0]',                    \n",
      "                                                                  'y_8[0][0]',                    \n",
      "                                                                  'zbn_8[0][0]',                  \n",
      "                                                                  'rbn_8[0][0]',                  \n",
      "                                                                  'lambda_160[0][0]',             \n",
      "                                                                  'lambda_161[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_170 (Concatenate)  (None, 40)           0           ['x_9[0][0]',                    \n",
      "                                                                  'y_9[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate_171 (Concatenate)  (None, 40)           0           ['x_9[0][0]',                    \n",
      "                                                                  'y_9[0][0]']                    \n",
      "                                                                                                  \n",
      " z1_9 (Dense)                   (None, 5)            205         ['concatenate_170[0][0]']        \n",
      "                                                                                                  \n",
      " r1_9 (Dense)                   (None, 5)            205         ['concatenate_171[0][0]']        \n",
      "                                                                                                  \n",
      " z2_9 (Dense)                   (None, 400)          2400        ['z1_9[0][0]']                   \n",
      "                                                                                                  \n",
      " r2_9 (Dense)                   (None, 400)          2400        ['r1_9[0][0]']                   \n",
      "                                                                                                  \n",
      " initial_value_77 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_9 (Reshape)                 (None, 20, 20)       0           ['z2_9[0][0]']                   \n",
      "                                                                                                  \n",
      " rr_9 (Reshape)                 (None, 20, 20)       0           ['r2_9[0][0]']                   \n",
      "                                                                                                  \n",
      " zbn_9 (BatchNormalization)     (None, 20, 20)       80          ['zr_9[0][0]']                   \n",
      "                                                                                                  \n",
      " rbn_9 (BatchNormalization)     (None, 20, 20)       80          ['rr_9[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_162 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_77[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_163 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_77[0][0]']       \n",
      "                                                                                                  \n",
      " x_10 (Lambda)                  (None, 20)           0           ['initial_value_77[0][0]',       \n",
      "                                                                  'x_9[0][0]',                    \n",
      "                                                                  'y_9[0][0]',                    \n",
      "                                                                  'zbn_9[0][0]',                  \n",
      "                                                                  'rbn_9[0][0]',                  \n",
      "                                                                  'lambda_162[0][0]',             \n",
      "                                                                  'lambda_163[0][0]']             \n",
      "                                                                                                  \n",
      " y_10 (Lambda)                  (None, 20)           0           ['initial_value_77[0][0]',       \n",
      "                                                                  'x_9[0][0]',                    \n",
      "                                                                  'y_9[0][0]',                    \n",
      "                                                                  'zbn_9[0][0]',                  \n",
      "                                                                  'rbn_9[0][0]',                  \n",
      "                                                                  'lambda_162[0][0]',             \n",
      "                                                                  'lambda_163[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_172 (Concatenate)  (None, 40)           0           ['x_10[0][0]',                   \n",
      "                                                                  'y_10[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate_173 (Concatenate)  (None, 40)           0           ['x_10[0][0]',                   \n",
      "                                                                  'y_10[0][0]']                   \n",
      "                                                                                                  \n",
      " z1_10 (Dense)                  (None, 5)            205         ['concatenate_172[0][0]']        \n",
      "                                                                                                  \n",
      " r1_10 (Dense)                  (None, 5)            205         ['concatenate_173[0][0]']        \n",
      "                                                                                                  \n",
      " z2_10 (Dense)                  (None, 400)          2400        ['z1_10[0][0]']                  \n",
      "                                                                                                  \n",
      " r2_10 (Dense)                  (None, 400)          2400        ['r1_10[0][0]']                  \n",
      "                                                                                                  \n",
      " initial_value_78 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_10 (Reshape)                (None, 20, 20)       0           ['z2_10[0][0]']                  \n",
      "                                                                                                  \n",
      " rr_10 (Reshape)                (None, 20, 20)       0           ['r2_10[0][0]']                  \n",
      "                                                                                                  \n",
      " zbn_10 (BatchNormalization)    (None, 20, 20)       80          ['zr_10[0][0]']                  \n",
      "                                                                                                  \n",
      " rbn_10 (BatchNormalization)    (None, 20, 20)       80          ['rr_10[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_164 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_78[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_165 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_78[0][0]']       \n",
      "                                                                                                  \n",
      " x_11 (Lambda)                  (None, 20)           0           ['initial_value_78[0][0]',       \n",
      "                                                                  'x_10[0][0]',                   \n",
      "                                                                  'y_10[0][0]',                   \n",
      "                                                                  'zbn_10[0][0]',                 \n",
      "                                                                  'rbn_10[0][0]',                 \n",
      "                                                                  'lambda_164[0][0]',             \n",
      "                                                                  'lambda_165[0][0]']             \n",
      "                                                                                                  \n",
      " y_11 (Lambda)                  (None, 20)           0           ['initial_value_78[0][0]',       \n",
      "                                                                  'x_10[0][0]',                   \n",
      "                                                                  'y_10[0][0]',                   \n",
      "                                                                  'zbn_10[0][0]',                 \n",
      "                                                                  'rbn_10[0][0]',                 \n",
      "                                                                  'lambda_164[0][0]',             \n",
      "                                                                  'lambda_165[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_174 (Concatenate)  (None, 40)           0           ['x_11[0][0]',                   \n",
      "                                                                  'y_11[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate_175 (Concatenate)  (None, 40)           0           ['x_11[0][0]',                   \n",
      "                                                                  'y_11[0][0]']                   \n",
      "                                                                                                  \n",
      " z1_11 (Dense)                  (None, 5)            205         ['concatenate_174[0][0]']        \n",
      "                                                                                                  \n",
      " r1_11 (Dense)                  (None, 5)            205         ['concatenate_175[0][0]']        \n",
      "                                                                                                  \n",
      " z2_11 (Dense)                  (None, 400)          2400        ['z1_11[0][0]']                  \n",
      "                                                                                                  \n",
      " r2_11 (Dense)                  (None, 400)          2400        ['r1_11[0][0]']                  \n",
      "                                                                                                  \n",
      " initial_value_79 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_11 (Reshape)                (None, 20, 20)       0           ['z2_11[0][0]']                  \n",
      "                                                                                                  \n",
      " rr_11 (Reshape)                (None, 20, 20)       0           ['r2_11[0][0]']                  \n",
      "                                                                                                  \n",
      " zbn_11 (BatchNormalization)    (None, 20, 20)       80          ['zr_11[0][0]']                  \n",
      "                                                                                                  \n",
      " rbn_11 (BatchNormalization)    (None, 20, 20)       80          ['rr_11[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_166 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_79[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_167 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_79[0][0]']       \n",
      "                                                                                                  \n",
      " x_12 (Lambda)                  (None, 20)           0           ['initial_value_79[0][0]',       \n",
      "                                                                  'x_11[0][0]',                   \n",
      "                                                                  'y_11[0][0]',                   \n",
      "                                                                  'zbn_11[0][0]',                 \n",
      "                                                                  'rbn_11[0][0]',                 \n",
      "                                                                  'lambda_166[0][0]',             \n",
      "                                                                  'lambda_167[0][0]']             \n",
      "                                                                                                  \n",
      " y_12 (Lambda)                  (None, 20)           0           ['initial_value_79[0][0]',       \n",
      "                                                                  'x_11[0][0]',                   \n",
      "                                                                  'y_11[0][0]',                   \n",
      "                                                                  'zbn_11[0][0]',                 \n",
      "                                                                  'rbn_11[0][0]',                 \n",
      "                                                                  'lambda_166[0][0]',             \n",
      "                                                                  'lambda_167[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_176 (Concatenate)  (None, 40)           0           ['x_12[0][0]',                   \n",
      "                                                                  'y_12[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate_177 (Concatenate)  (None, 40)           0           ['x_12[0][0]',                   \n",
      "                                                                  'y_12[0][0]']                   \n",
      "                                                                                                  \n",
      " z1_12 (Dense)                  (None, 5)            205         ['concatenate_176[0][0]']        \n",
      "                                                                                                  \n",
      " r1_12 (Dense)                  (None, 5)            205         ['concatenate_177[0][0]']        \n",
      "                                                                                                  \n",
      " z2_12 (Dense)                  (None, 400)          2400        ['z1_12[0][0]']                  \n",
      "                                                                                                  \n",
      " r2_12 (Dense)                  (None, 400)          2400        ['r1_12[0][0]']                  \n",
      "                                                                                                  \n",
      " initial_value_80 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_12 (Reshape)                (None, 20, 20)       0           ['z2_12[0][0]']                  \n",
      "                                                                                                  \n",
      " rr_12 (Reshape)                (None, 20, 20)       0           ['r2_12[0][0]']                  \n",
      "                                                                                                  \n",
      " zbn_12 (BatchNormalization)    (None, 20, 20)       80          ['zr_12[0][0]']                  \n",
      "                                                                                                  \n",
      " rbn_12 (BatchNormalization)    (None, 20, 20)       80          ['rr_12[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_168 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_80[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_169 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_80[0][0]']       \n",
      "                                                                                                  \n",
      " x_13 (Lambda)                  (None, 20)           0           ['initial_value_80[0][0]',       \n",
      "                                                                  'x_12[0][0]',                   \n",
      "                                                                  'y_12[0][0]',                   \n",
      "                                                                  'zbn_12[0][0]',                 \n",
      "                                                                  'rbn_12[0][0]',                 \n",
      "                                                                  'lambda_168[0][0]',             \n",
      "                                                                  'lambda_169[0][0]']             \n",
      "                                                                                                  \n",
      " y_13 (Lambda)                  (None, 20)           0           ['initial_value_80[0][0]',       \n",
      "                                                                  'x_12[0][0]',                   \n",
      "                                                                  'y_12[0][0]',                   \n",
      "                                                                  'zbn_12[0][0]',                 \n",
      "                                                                  'rbn_12[0][0]',                 \n",
      "                                                                  'lambda_168[0][0]',             \n",
      "                                                                  'lambda_169[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_178 (Concatenate)  (None, 40)           0           ['x_13[0][0]',                   \n",
      "                                                                  'y_13[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate_179 (Concatenate)  (None, 40)           0           ['x_13[0][0]',                   \n",
      "                                                                  'y_13[0][0]']                   \n",
      "                                                                                                  \n",
      " z1_13 (Dense)                  (None, 5)            205         ['concatenate_178[0][0]']        \n",
      "                                                                                                  \n",
      " r1_13 (Dense)                  (None, 5)            205         ['concatenate_179[0][0]']        \n",
      "                                                                                                  \n",
      " z2_13 (Dense)                  (None, 400)          2400        ['z1_13[0][0]']                  \n",
      "                                                                                                  \n",
      " r2_13 (Dense)                  (None, 400)          2400        ['r1_13[0][0]']                  \n",
      "                                                                                                  \n",
      " initial_value_81 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_13 (Reshape)                (None, 20, 20)       0           ['z2_13[0][0]']                  \n",
      "                                                                                                  \n",
      " rr_13 (Reshape)                (None, 20, 20)       0           ['r2_13[0][0]']                  \n",
      "                                                                                                  \n",
      " zbn_13 (BatchNormalization)    (None, 20, 20)       80          ['zr_13[0][0]']                  \n",
      "                                                                                                  \n",
      " rbn_13 (BatchNormalization)    (None, 20, 20)       80          ['rr_13[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_170 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_81[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_171 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_81[0][0]']       \n",
      "                                                                                                  \n",
      " x_14 (Lambda)                  (None, 20)           0           ['initial_value_81[0][0]',       \n",
      "                                                                  'x_13[0][0]',                   \n",
      "                                                                  'y_13[0][0]',                   \n",
      "                                                                  'zbn_13[0][0]',                 \n",
      "                                                                  'rbn_13[0][0]',                 \n",
      "                                                                  'lambda_170[0][0]',             \n",
      "                                                                  'lambda_171[0][0]']             \n",
      "                                                                                                  \n",
      " y_14 (Lambda)                  (None, 20)           0           ['initial_value_81[0][0]',       \n",
      "                                                                  'x_13[0][0]',                   \n",
      "                                                                  'y_13[0][0]',                   \n",
      "                                                                  'zbn_13[0][0]',                 \n",
      "                                                                  'rbn_13[0][0]',                 \n",
      "                                                                  'lambda_170[0][0]',             \n",
      "                                                                  'lambda_171[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_180 (Concatenate)  (None, 40)           0           ['x_14[0][0]',                   \n",
      "                                                                  'y_14[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate_181 (Concatenate)  (None, 40)           0           ['x_14[0][0]',                   \n",
      "                                                                  'y_14[0][0]']                   \n",
      "                                                                                                  \n",
      " z1_14 (Dense)                  (None, 5)            205         ['concatenate_180[0][0]']        \n",
      "                                                                                                  \n",
      " r1_14 (Dense)                  (None, 5)            205         ['concatenate_181[0][0]']        \n",
      "                                                                                                  \n",
      " z2_14 (Dense)                  (None, 400)          2400        ['z1_14[0][0]']                  \n",
      "                                                                                                  \n",
      " r2_14 (Dense)                  (None, 400)          2400        ['r1_14[0][0]']                  \n",
      "                                                                                                  \n",
      " initial_value_82 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_14 (Reshape)                (None, 20, 20)       0           ['z2_14[0][0]']                  \n",
      "                                                                                                  \n",
      " rr_14 (Reshape)                (None, 20, 20)       0           ['r2_14[0][0]']                  \n",
      "                                                                                                  \n",
      " zbn_14 (BatchNormalization)    (None, 20, 20)       80          ['zr_14[0][0]']                  \n",
      "                                                                                                  \n",
      " rbn_14 (BatchNormalization)    (None, 20, 20)       80          ['rr_14[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_172 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_82[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_173 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_82[0][0]']       \n",
      "                                                                                                  \n",
      " x_15 (Lambda)                  (None, 20)           0           ['initial_value_82[0][0]',       \n",
      "                                                                  'x_14[0][0]',                   \n",
      "                                                                  'y_14[0][0]',                   \n",
      "                                                                  'zbn_14[0][0]',                 \n",
      "                                                                  'rbn_14[0][0]',                 \n",
      "                                                                  'lambda_172[0][0]',             \n",
      "                                                                  'lambda_173[0][0]']             \n",
      "                                                                                                  \n",
      " y_15 (Lambda)                  (None, 20)           0           ['initial_value_82[0][0]',       \n",
      "                                                                  'x_14[0][0]',                   \n",
      "                                                                  'y_14[0][0]',                   \n",
      "                                                                  'zbn_14[0][0]',                 \n",
      "                                                                  'rbn_14[0][0]',                 \n",
      "                                                                  'lambda_172[0][0]',             \n",
      "                                                                  'lambda_173[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_182 (Concatenate)  (None, 40)           0           ['x_15[0][0]',                   \n",
      "                                                                  'y_15[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate_183 (Concatenate)  (None, 40)           0           ['x_15[0][0]',                   \n",
      "                                                                  'y_15[0][0]']                   \n",
      "                                                                                                  \n",
      " z1_15 (Dense)                  (None, 5)            205         ['concatenate_182[0][0]']        \n",
      "                                                                                                  \n",
      " r1_15 (Dense)                  (None, 5)            205         ['concatenate_183[0][0]']        \n",
      "                                                                                                  \n",
      " z2_15 (Dense)                  (None, 400)          2400        ['z1_15[0][0]']                  \n",
      "                                                                                                  \n",
      " r2_15 (Dense)                  (None, 400)          2400        ['r1_15[0][0]']                  \n",
      "                                                                                                  \n",
      " initial_value_83 (InitialValue  ()                  1           ['input_26[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " zr_15 (Reshape)                (None, 20, 20)       0           ['z2_15[0][0]']                  \n",
      "                                                                                                  \n",
      " rr_15 (Reshape)                (None, 20, 20)       0           ['r2_15[0][0]']                  \n",
      "                                                                                                  \n",
      " zbn_15 (BatchNormalization)    (None, 20, 20)       80          ['zr_15[0][0]']                  \n",
      "                                                                                                  \n",
      " rbn_15 (BatchNormalization)    (None, 20, 20)       80          ['rr_15[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda_174 (Lambda)            (None, 20)           0           ['input_26[0][0]',               \n",
      "                                                                  'initial_value_83[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_175 (Lambda)            (None, 20)           0           ['input_27[0][0]',               \n",
      "                                                                  'initial_value_83[0][0]']       \n",
      "                                                                                                  \n",
      " x_16 (Lambda)                  (None, 20)           0           ['initial_value_83[0][0]',       \n",
      "                                                                  'x_15[0][0]',                   \n",
      "                                                                  'y_15[0][0]',                   \n",
      "                                                                  'zbn_15[0][0]',                 \n",
      "                                                                  'rbn_15[0][0]',                 \n",
      "                                                                  'lambda_174[0][0]',             \n",
      "                                                                  'lambda_175[0][0]']             \n",
      "                                                                                                  \n",
      " y_16 (Lambda)                  (None, 20)           0           ['initial_value_83[0][0]',       \n",
      "                                                                  'x_15[0][0]',                   \n",
      "                                                                  'y_15[0][0]',                   \n",
      "                                                                  'zbn_15[0][0]',                 \n",
      "                                                                  'rbn_15[0][0]',                 \n",
      "                                                                  'lambda_174[0][0]',             \n",
      "                                                                  'lambda_175[0][0]']             \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda_176 (Lambda)            (None, 20)           0           ['x_16[0][0]',                   \n",
      "                                                                  'y_16[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 89,016\n",
      "Trainable params: 86,180\n",
      "Non-trainable params: 2,836\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_loss.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant(np.full((n_paths, n_x_dimensions), 1.), dtype=tf.float32)\n",
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([intensity for _ in range(n_jump_factors)])))\n",
    "target = tf.zeros((n_paths, n_y_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "# define callbacks\n",
    "y0_callback = Y0Callback(os.path.join(output_dir, \"y0.csv\"))\n",
    "callbacks += [y0_callback]\n",
    "# callbacks += [BatchSave(os.path.join(output_dir, \"weights\"))]\n",
    "callbacks += [ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)]\n",
    "callbacks += [tf.keras.callbacks.TerminateOnNaN()]\n",
    "callbacks += [tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=30)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # print(\"\\n\", y_pred.numpy().flatten(), \"\\r\")\n",
    "    return(tf.keras.metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "adam = Adam(learning_rate=1e-2) \n",
    "model_loss.compile(loss=mse_loss, optimizer=adam, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (x_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 20) dtype=float32, numpy=\n",
      "array([[      0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (y_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 20) dtype=float32, numpy=\n",
      "array([[      0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      " 31/512 [>.............................] - ETA: 3:10 - loss: 29.4018\n",
      "[[0.023839 -0.041401 0.051767 -0.0076249 0.045396 0.039266 -0.030491 0.057391 0.0033224\n",
      "  0.0045069 0.021346 0.017703 0.021211 0.060787 -0.015696 -0.020725 0.0012186 -0.039529\n",
      "  0.041517 0.0026338]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 3:01 - loss: 23.4067\n",
      "[[0.016901 -0.055894 0.086208 -0.0081993 0.055573 0.039834 -0.019379  0.1242 0.012979\n",
      "  -0.0054554 0.033295 0.020948 0.045371 0.086569 0.025869 0.0074482 0.022867 -0.037271\n",
      "  0.042883 0.032536]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:46 - loss: 18.6072\n",
      "[[0.020469 -0.059366 0.09141 -0.024107 0.063572 0.037875 -0.015918 0.16447 0.0038752\n",
      "  -0.0047982 0.04225 0.047036 0.073534 0.10479 0.022526 0.019528 0.027231 -0.028594\n",
      "  0.029038 0.014614]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:33 - loss: 15.1974\n",
      "[[0.022748 -0.048189 0.086871 -0.050577 0.056366 0.050852 0.0022741 0.20157 -0.0046864\n",
      "  -0.0042237 0.040655 0.054674 0.095891 0.13139 0.025456 0.024478 0.029695 -0.037421\n",
      "  0.026429 -0.02039]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:20 - loss: 12.6912\n",
      "[[0.01795 -0.0073894 0.091357 -0.023837 0.055215 0.053984 0.040318 0.21542 -0.0024366\n",
      "  0.0062779 0.041749 0.053755 0.088186 0.14527 0.035747 0.013826 0.026213 -0.018404\n",
      "  0.036741 -0.053026]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:07 - loss: 10.8156\n",
      "[[0.039091 0.0092003 0.071577 -0.0046976 0.073047 0.058064 0.047356 0.16366 0.013067\n",
      "  0.020429 0.042722 0.052313 0.089473 0.14452 0.045577 0.0091019 0.021347 0.0038858\n",
      "  0.052671 -0.052851]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:54 - loss: 9.3775\n",
      "[[0.04531 0.01851 0.064427 0.01522 0.089918 0.050229 0.059009 0.13452 0.038309 0.031449\n",
      "  0.051056 0.051092 0.09356 0.12442 0.060944 0.004082 0.029344 0.0083695 0.075689\n",
      "  -0.053709]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:41 - loss: 8.2555\n",
      "[[0.048456 0.041844 0.065877 0.023591 0.097872 0.063521 0.056875 0.12883 0.047679\n",
      "  0.038907 0.049158 0.057947 0.10385 0.12076 0.068154 0.011408 0.041109 0.012655\n",
      "  0.076845 -0.050365]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:28 - loss: 7.3638\n",
      "[[0.059729 0.055174 0.06838 0.035017 0.098223 0.066777 0.062901 0.12928 0.058152\n",
      "  0.048945 0.056213 0.06236 0.11186 0.12126 0.069119 0.021601 0.050529 0.017117 0.079735\n",
      "  -0.040098]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:15 - loss: 6.6422\n",
      "[[0.065352 0.063288 0.076094 0.04014 0.10754 0.069146 0.064698  0.1362 0.065448 0.055167\n",
      "  0.063174 0.067971 0.11999 0.12784 0.070199 0.029705 0.059465 0.018268 0.082335\n",
      "  -0.026442]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:03 - loss: 6.0495\n",
      "[[0.070845 0.067454 0.081687 0.049525 0.11073 0.075062 0.075317 0.14234 0.074748 0.06193\n",
      "  0.070581 0.072174 0.12417 0.13449 0.075613 0.04216 0.064972 0.027197 0.087446\n",
      "  -0.006195]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 50s - loss: 5.5527\n",
      "[[0.074161 0.075096 0.090884 0.057003 0.11951 0.077815 0.079888 0.14779 0.081748\n",
      "  0.067015 0.077441 0.078657 0.12826 0.14006 0.081102 0.049666 0.067371 0.035658\n",
      "  0.095026 0.0070701]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 38s - loss: 5.1314\n",
      "[[0.081287 0.083517 0.099301 0.06289 0.12846 0.085942 0.086349 0.15854 0.090862 0.076472\n",
      "  0.084504 0.086469 0.13829 0.15016 0.085061 0.054839 0.073842 0.041839 0.10443 0.013051]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 4.7706\n",
      "[[0.088008 0.094401 0.10614 0.073089 0.13856 0.092869 0.093128 0.16637 0.09884 0.085556\n",
      "  0.089758 0.09342 0.14627 0.15638 0.091723 0.066698 0.083715 0.051489 0.11418 0.020001]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 4.4576\n",
      "[[0.091828 0.10318 0.11568 0.080632 0.14798  0.0995 0.10332 0.17464 0.10614 0.092942\n",
      "  0.10055 0.10266 0.15382 0.16495 0.097364 0.080993   0.088 0.056315 0.12316 0.024111]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 4.1835\n",
      "[[0.096998 0.11151 0.12701 0.091925 0.15873 0.10863 0.10642 0.18544 0.11736 0.10153\n",
      "  0.10331 0.10834 0.16265 0.17559 0.10476 0.083253 0.094901 0.063638 0.13275 0.026472]]\n",
      "\n",
      "512/512 [==============================] - 202s 391ms/step - loss: 4.1754\n",
      "Epoch 2/20\n",
      " 31/512 [>.............................] - ETA: 3:13 - loss: 0.0880\n",
      "[[0.10357 0.12333 0.13843 0.10166 0.17245 0.11714   0.116 0.19697 0.12845 0.11156\n",
      "  0.10979 0.11705 0.17328 0.18714 0.11277 0.088579 0.10475 0.072907 0.14431 0.035406]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:58 - loss: 0.0836\n",
      "[[0.11155 0.13343 0.14828 0.11098   0.183 0.12696 0.12143 0.20643 0.13801 0.12072\n",
      "  0.11728 0.12595 0.18173 0.19589 0.12054 0.096414  0.1123 0.080045  0.1537 0.044019]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:45 - loss: 0.0813\n",
      "[[0.11615 0.14453 0.15944 0.12104 0.19546 0.13511 0.13381 0.21791 0.14769 0.12976\n",
      "  0.12557 0.13517 0.19097  0.2064  0.1297 0.10579 0.12089 0.090072 0.16332 0.051467]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:32 - loss: 0.0801\n",
      "[[0.12308 0.15515 0.17049 0.12973 0.20728 0.14808 0.13878 0.22821 0.15875 0.13953\n",
      "  0.13222 0.14297 0.20123 0.21727 0.13697 0.11337 0.12808 0.096653 0.17466 0.056739]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:19 - loss: 0.0800\n",
      "[[0.13044 0.16445 0.18257 0.14252 0.22036 0.15626 0.15287 0.24046 0.17061 0.15021\n",
      "  0.13857 0.15279 0.21212 0.22892 0.14667 0.12172 0.13876 0.10991 0.18573 0.063485]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:08 - loss: 0.0815\n",
      "[[0.13938 0.17617 0.19493 0.15595 0.23267 0.16727 0.16275 0.25274 0.18257 0.16198\n",
      "  0.14741 0.16224 0.22356 0.24078 0.15905 0.13066 0.14962 0.12084 0.19826 0.075911]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:55 - loss: 0.0819\n",
      "[[0.14967 0.18626 0.20401  0.1657 0.24167 0.17683  0.1722 0.26033 0.19151 0.17035\n",
      "  0.16115 0.17126 0.23137 0.24837 0.16893   0.138 0.16205 0.12707 0.20898 0.087382]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:42 - loss: 0.0819\n",
      "[[0.15476 0.19856 0.21791 0.17821 0.25619 0.18949 0.18469 0.27419  0.2045 0.18272\n",
      "  0.16919 0.18181 0.24404 0.26263 0.17813 0.14779 0.17164 0.13983 0.22168 0.094838]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:29 - loss: 0.0816\n",
      "[[0.16338 0.20988  0.2302  0.1912 0.26891 0.20486 0.19258 0.28584 0.21578 0.19397\n",
      "  0.18012 0.19036 0.25446 0.27478 0.19156 0.15706 0.17909 0.14783 0.23206 0.10205]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:16 - loss: 0.0816\n",
      "[[0.17168 0.22009 0.24323 0.20245  0.2817 0.21375 0.20301 0.29778 0.22792 0.20596\n",
      "  0.18799 0.20195 0.26628 0.28675 0.20518 0.16643 0.19165 0.15709 0.24448 0.10753]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:04 - loss: 0.0823\n",
      "[[ 0.1819 0.23237 0.25568 0.21491 0.29381 0.22936 0.21537 0.30902 0.23981 0.21753\n",
      "  0.20047 0.21156 0.27706 0.29833 0.21445 0.17703 0.20427 0.17089 0.25743  0.1198]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 51s - loss: 0.0823\n",
      "[[0.19205 0.23992 0.26833  0.2242 0.30717 0.23655 0.22973  0.3213 0.25208 0.22856\n",
      "  0.22169 0.22085 0.28806 0.31044 0.22244 0.18296 0.22379 0.18197 0.26864 0.13247]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 38s - loss: 0.0823\n",
      "[[0.20588 0.24753 0.27919 0.23627 0.31688 0.25131 0.23998  0.3303 0.26221  0.2389\n",
      "  0.23419 0.23224 0.29734 0.31993 0.23392 0.20158 0.23325 0.19693 0.28039 0.14685]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0827\n",
      "[[0.21809  0.2706 0.29255 0.24804  0.3306 0.25937 0.25153 0.34358 0.27445 0.25083\n",
      "  0.24656 0.24816 0.30936 0.33306 0.24186 0.21561 0.24678 0.20379 0.29314 0.16117]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0826\n",
      "[[0.22466 0.27135 0.30942 0.26252 0.34809 0.27908 0.26628 0.36097 0.29093 0.26617\n",
      "  0.26082 0.25396 0.32628 0.34999 0.25655 0.22253 0.26172 0.21738  0.3094 0.16984]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0823\n",
      "[[0.23214 0.27947 0.32243 0.28188 0.36102 0.29114 0.27717 0.37312 0.30374  0.2771\n",
      "    0.274 0.26256  0.3372 0.36279 0.27184 0.23089 0.27058 0.23254 0.32101 0.17438]]\n",
      "\n",
      "512/512 [==============================] - 202s 394ms/step - loss: 0.0823\n",
      "Epoch 3/20\n",
      " 31/512 [>.............................] - ETA: 3:07 - loss: 0.0929\n",
      "[[0.24367 0.29311 0.33785 0.29303 0.37683 0.30559 0.28871 0.38822 0.31874  0.2907\n",
      "  0.28846 0.27783 0.35163   0.378 0.28379 0.24153 0.28236 0.24434 0.33644 0.18858]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:55 - loss: 0.0899\n",
      "[[0.26675 0.31349 0.35025 0.30321 0.38896 0.31523 0.30028 0.39955 0.33103 0.30275\n",
      "  0.30378 0.29209 0.36345 0.38951 0.29182 0.26208 0.29749 0.25592 0.34528 0.20317]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:42 - loss: 0.0898\n",
      "[[ 0.2849 0.33492 0.36121 0.31199 0.39888 0.32108  0.3109 0.40811 0.34062 0.31283\n",
      "  0.32002 0.31357   0.372 0.39919 0.30277 0.28488 0.31651 0.27045 0.35484 0.22904]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:29 - loss: 0.0882\n",
      "[[0.30022 0.35187 0.37482  0.3231 0.41296 0.33757 0.31951 0.42181 0.35437 0.32597\n",
      "   0.3347 0.32974 0.38567 0.41276 0.31195 0.30443 0.33171  0.2823  0.3678 0.24467]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:17 - loss: 0.0883\n",
      "[[0.31784 0.37073 0.38844 0.34148 0.42599 0.35125 0.33992 0.43526 0.36862 0.33912\n",
      "  0.35073 0.34833 0.39915 0.42554 0.32751 0.32499 0.34815  0.3004 0.38209 0.26465]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:05 - loss: 0.0904\n",
      "[[0.33344 0.39085 0.40058 0.36464 0.44068 0.36792 0.36178 0.45012 0.38392 0.35498\n",
      "  0.35728 0.36852 0.41465    0.44 0.34583 0.34717 0.36234 0.32194 0.40054 0.28706]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:53 - loss: 0.0911\n",
      "[[0.34193 0.40782 0.40714 0.38236 0.45354 0.38353 0.37739   0.462 0.39698 0.36593\n",
      "  0.37539  0.3842 0.42649 0.45198 0.36774 0.36456 0.36789 0.33854 0.41643 0.30433]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:40 - loss: 0.0915\n",
      "[[0.36079 0.42941 0.42535 0.39196 0.47024 0.39851 0.38684  0.4785 0.41409 0.38272\n",
      "  0.39227 0.40493 0.44356 0.46872 0.37942 0.38711 0.38824 0.35158 0.42786 0.31457]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:28 - loss: 0.0910\n",
      "[[0.37637 0.44755 0.43158 0.40613 0.48627 0.40836 0.40316 0.49348 0.42893 0.39824\n",
      "  0.40345 0.42135 0.45832 0.48445 0.39688 0.40782 0.39825 0.36654 0.44256 0.32871]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:16 - loss: 0.0902\n",
      "[[ 0.3909 0.46464 0.44363 0.42199 0.49876 0.42463 0.41279 0.50625 0.44298 0.41264\n",
      "  0.42181 0.43824 0.47184  0.4972 0.41342 0.42645 0.41578 0.37693  0.4586 0.34703]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:03 - loss: 0.0899\n",
      "[[0.41399 0.48184  0.4641 0.43433 0.51382 0.44864 0.42911 0.52042 0.45823 0.42738\n",
      "  0.43966 0.45539 0.48614 0.51131 0.42717 0.44505 0.43771 0.39484 0.47307 0.37077]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 51s - loss: 0.0894\n",
      "[[0.43236 0.50085 0.48005 0.44719 0.53063 0.46408 0.44597 0.53658 0.47456 0.44191\n",
      "   0.4575 0.47326 0.50134 0.52745 0.44384 0.46507 0.45534 0.41125 0.48859 0.38872]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 38s - loss: 0.0891\n",
      "[[0.44974 0.51501 0.49507 0.46383 0.54268 0.48114 0.46252 0.54817 0.48728 0.45564\n",
      "   0.4729 0.48809  0.5136 0.53942 0.45972 0.48196 0.47127 0.43097 0.50374 0.40783]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0891\n",
      "[[0.46958 0.53521 0.51305 0.47744  0.5604 0.49734  0.4766 0.56619 0.50409 0.47213\n",
      "  0.49092 0.50675   0.531 0.55685 0.47447 0.50226 0.49072 0.44427 0.52008 0.42626]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 13s - loss: 0.0887\n",
      "[[0.49027 0.55416 0.53198 0.49388 0.57714 0.51964 0.49366 0.58275 0.52135 0.48889\n",
      "  0.51017 0.52607 0.54827 0.57411 0.49154 0.52249 0.51013    0.46 0.53686 0.45094]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0882\n",
      "[[0.50777 0.57087 0.54822 0.51486 0.59255  0.5343 0.51076 0.59824 0.53728 0.50236\n",
      "  0.52711 0.54338 0.56255 0.58984 0.50784 0.54038 0.52572   0.485 0.55135 0.46412]]\n",
      "\n",
      "512/512 [==============================] - 202s 394ms/step - loss: 0.0882\n",
      "Epoch 4/20\n",
      " 31/512 [>.............................] - ETA: 3:08 - loss: 0.1041\n",
      "[[0.52793 0.58935 0.56559 0.53329   0.609 0.55356 0.52545 0.61452 0.55446 0.51928\n",
      "  0.54579  0.5622 0.57968 0.60556 0.52771 0.56055  0.5447 0.50616 0.56898 0.48873]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:55 - loss: 0.0971\n",
      "[[0.54865 0.60881 0.58444 0.54812 0.62733 0.57217 0.54289  0.6324 0.57238 0.53694\n",
      "  0.56584 0.58172 0.59772 0.62383 0.54291 0.58119 0.56495 0.52003 0.58587 0.51012]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:43 - loss: 0.0948\n",
      "[[ 0.5682 0.62736 0.60236   0.562 0.64489 0.58642 0.56181 0.64946 0.58896 0.55432\n",
      "  0.58381 0.59929 0.61442 0.64099 0.56176 0.60069 0.58399 0.53955 0.60072 0.52926]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:31 - loss: 0.0912\n",
      "[[0.58732 0.64477  0.6194 0.57703 0.66121   0.605 0.57522 0.66565 0.60571 0.57086\n",
      "  0.60162 0.61655 0.63103 0.65738  0.5742 0.61929 0.60193 0.55755 0.61626 0.54814]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:20 - loss: 0.0891\n",
      "[[0.60821 0.66416 0.63855 0.59834 0.67874 0.62492 0.59504  0.6841 0.62487 0.58936\n",
      "  0.62155 0.63731 0.64974 0.67492 0.59729 0.63926 0.62236 0.57915 0.63484 0.56939]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:08 - loss: 0.0900\n",
      "[[0.62885 0.68348 0.65725 0.62197 0.69652 0.64682 0.61716  0.7021 0.64347 0.60843\n",
      "  0.64072 0.65752 0.66821 0.69266  0.6178 0.65944 0.64267 0.60092  0.6567 0.59064]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:55 - loss: 0.0918\n",
      "[[ 0.6472 0.70094 0.67511 0.64057 0.71274 0.66368 0.63791 0.71752 0.66063 0.62587\n",
      "  0.65883 0.67493  0.6849 0.70878 0.63712 0.67749 0.66122 0.62187 0.67693 0.61183]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:42 - loss: 0.0919\n",
      "[[0.66705  0.7202 0.69378 0.66525  0.7311 0.68781 0.65687 0.73513 0.67936 0.64382\n",
      "  0.67867 0.69369 0.70262 0.72754 0.65468  0.6973 0.67932 0.63965 0.69088 0.63701]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:29 - loss: 0.0909\n",
      "[[0.68693 0.73855 0.71203 0.68078 0.74858 0.69949 0.67098 0.75236 0.69687  0.6628\n",
      "  0.69718 0.71146 0.72014 0.74501 0.67932 0.71661 0.69862 0.66113 0.71007 0.65126]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:16 - loss: 0.0894\n",
      "[[0.70642 0.75583 0.73012 0.70184 0.76491  0.7237 0.69016 0.76904 0.71476  0.6812\n",
      "  0.71542 0.72954 0.73748 0.76183 0.69517 0.73491 0.71708  0.6833 0.73198 0.67416]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:03 - loss: 0.0884\n",
      "[[0.72806 0.77775 0.75133 0.72298 0.78632 0.74514 0.71103  0.7908 0.73548   0.702\n",
      "  0.73647 0.75173 0.75862 0.78304 0.71522 0.75676 0.73959 0.70266 0.74913 0.69527]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 51s - loss: 0.0873\n",
      "[[0.74833 0.79767 0.77095   0.741 0.80572 0.76405 0.73119 0.81011 0.75504 0.72083\n",
      "  0.75673 0.77208 0.77767 0.80221 0.73311 0.77704 0.75918  0.7218 0.77048 0.71408]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 38s - loss: 0.0871\n",
      "[[0.76725 0.81397 0.78868 0.76032 0.82212 0.78135  0.7468 0.82618 0.77188 0.73911\n",
      "  0.77452  0.7891 0.79407 0.81859 0.75153 0.79499 0.77755 0.74374   0.787 0.73189]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0870\n",
      "[[ 0.7883 0.83583 0.80937  0.7804 0.84282 0.79869 0.76798 0.84707 0.79236 0.75955\n",
      "  0.79505 0.80998 0.81507 0.83923   0.771   0.816 0.79872 0.75952 0.80786 0.75203]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 13s - loss: 0.0864\n",
      "[[0.80998 0.85607 0.83116 0.80171 0.86306 0.82107 0.78805 0.86718 0.81322 0.77998\n",
      "  0.81678 0.83109 0.83566 0.86007 0.79202 0.83714 0.81952 0.78209 0.82481 0.77679]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0853\n",
      "[[0.82871 0.87459 0.84981 0.82465  0.8811 0.83804 0.80792 0.88553 0.83281 0.79794\n",
      "  0.83573 0.85095 0.85378 0.87859 0.81239 0.85622 0.83747 0.79714 0.84335 0.79599]]\n",
      "\n",
      "512/512 [==============================] - 201s 393ms/step - loss: 0.0853\n",
      "Epoch 5/20\n",
      " 31/512 [>.............................] - ETA: 3:13 - loss: 0.0892\n",
      "[[0.84978  0.8942 0.86902 0.84704 0.90051 0.86206 0.82765 0.90484 0.85298 0.81802\n",
      "  0.85622 0.87107 0.87373 0.89717 0.83261 0.87699 0.85783 0.82244 0.86813 0.81792]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:56 - loss: 0.0808\n",
      "[[0.87105 0.91454 0.89001 0.86353 0.92094 0.88144 0.85013 0.92503 0.87413 0.83907\n",
      "  0.87794 0.89224 0.89442 0.91801 0.85087 0.89788 0.87936 0.84066 0.88207  0.8462]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:43 - loss: 0.0777\n",
      "[[0.89178 0.93421 0.90999 0.88488 0.94084  0.8972  0.8721 0.94435 0.89308 0.85929\n",
      "   0.8976  0.9109 0.91345 0.93735 0.87126 0.91818 0.89984 0.85874 0.90187 0.86573]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:30 - loss: 0.0760\n",
      "[[0.91196  0.9532 0.92916   0.901 0.95934  0.9215 0.88879 0.96298 0.91248   0.879\n",
      "  0.91704 0.93001 0.93304  0.9563 0.88649 0.93739 0.91954  0.8795 0.91936 0.88363]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:18 - loss: 0.0743\n",
      "[[0.93459 0.97475 0.95098 0.92659 0.98014 0.94461 0.91515  0.9847 0.93502 0.90135\n",
      "  0.93941 0.95327 0.95526 0.97692  0.9123 0.95879 0.94222 0.90228 0.94227 0.90616]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:05 - loss: 0.0737\n",
      "[[0.95669 0.99587 0.97184 0.94756  1.0008 0.96112 0.93779  1.0056 0.95642 0.92373\n",
      "  0.96074 0.97518 0.97664 0.99755 0.93293 0.98054 0.96436   0.922 0.96493 0.92858]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:52 - loss: 0.0761\n",
      "[[ 0.9753  1.0138 0.99097 0.97461  1.0187 0.98181 0.96423  1.0221 0.97519 0.94267\n",
      "  0.98014 0.99293  0.9945  1.0157 0.95863 0.99853 0.98254 0.95329 0.98907 0.95019]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:40 - loss: 0.0766\n",
      "[[0.99737  1.0357  1.0124 0.99178  1.0394   1.002 0.98459  1.0426 0.99717 0.96547\n",
      "    1.002  1.0149  1.0163   1.037 0.97691  1.0206  1.0046 0.97234  1.0075 0.97094]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:28 - loss: 0.0759\n",
      "[[  1.018  1.0555  1.0324  1.0096  1.0591  1.0194  1.0024  1.0622  1.0169 0.98626\n",
      "   1.0218  1.0339  1.0357  1.0565 0.99486  1.0405  1.0248 0.98847  1.0291 0.98952]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:15 - loss: 0.0743\n",
      "[[  1.038  1.0735  1.0519   1.033  1.0774  1.0437  1.0185  1.0804  1.0369  1.0067\n",
      "   1.0413   1.053  1.0547  1.0755  1.0192  1.0593  1.0439  1.0123  1.0472  1.0102]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:03 - loss: 0.0732\n",
      "[[ 1.0597   1.095  1.0733  1.0554  1.0993   1.068  1.0433  1.1024   1.058  1.0286\n",
      "   1.0627  1.0746  1.0762  1.0967  1.0405   1.081  1.0662   1.035  1.0703  1.0347]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 50s - loss: 0.0721\n",
      "[[ 1.0819  1.1174  1.0952  1.0733  1.1214  1.0877  1.0593  1.1245  1.0801    1.05\n",
      "   1.0852  1.0977  1.0982  1.1183   1.059  1.1032  1.0881  1.0551  1.0927  1.0537]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 37s - loss: 0.0716\n",
      "[[ 1.1002  1.1335  1.1124  1.0942  1.1379  1.1065  1.0825  1.1402  1.0977   1.069\n",
      "   1.1028  1.1142  1.1146  1.1347  1.0795  1.1204  1.1058  1.0752  1.1104  1.0765]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0710\n",
      "[[ 1.1222  1.1564  1.1347  1.1152    1.16  1.1241  1.1049  1.1628    1.12  1.0917\n",
      "   1.1249  1.1368  1.1375   1.157  1.1015  1.1424  1.1282  1.0925  1.1311  1.0936]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0703\n",
      "[[ 1.1447  1.1777  1.1575  1.1329  1.1814  1.1476  1.1241  1.1844  1.1423  1.1138\n",
      "   1.1475   1.159  1.1596  1.1788  1.1196  1.1647  1.1501  1.1133  1.1474  1.1189]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0695\n",
      "[[ 1.1619  1.1944  1.1749  1.1601  1.1986  1.1682  1.1512  1.2014  1.1609  1.1311\n",
      "   1.1655   1.177  1.1765  1.1965  1.1456  1.1821  1.1663  1.1418  1.1732  1.1412]]\n",
      "\n",
      "512/512 [==============================] - 200s 391ms/step - loss: 0.0694\n",
      "Epoch 6/20\n",
      " 31/512 [>.............................] - ETA: 3:04 - loss: 0.0740\n",
      "[[ 1.1841  1.2157  1.1956  1.1862  1.2192  1.1927  1.1785  1.2219  1.1824  1.1537\n",
      "   1.1869  1.1986  1.1985  1.2166  1.1714  1.2036  1.1887  1.1696   1.197  1.1661]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:51 - loss: 0.0673\n",
      "[[ 1.2055  1.2367  1.2175  1.2089  1.2408  1.2098  1.1962  1.2435  1.2049  1.1755\n",
      "   1.2096  1.2204  1.2198  1.2388  1.1898  1.2252  1.2101  1.1889  1.2216  1.1906]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:40 - loss: 0.0639\n",
      "[[ 1.2269  1.2566   1.238  1.2244  1.2612  1.2311   1.216  1.2633  1.2244   1.197\n",
      "   1.2297   1.239  1.2396  1.2585  1.2094  1.2457  1.2309  1.2057  1.2372  1.2086]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:27 - loss: 0.0622\n",
      "[[ 1.2483  1.2778  1.2594  1.2419  1.2818  1.2481  1.2291  1.2843  1.2459  1.2187\n",
      "   1.2511  1.2605  1.2614  1.2796  1.2272  1.2666  1.2525   1.227  1.2555  1.2291]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:15 - loss: 0.0605\n",
      "[[ 1.2717  1.3005  1.2823  1.2662  1.3039  1.2766  1.2562  1.3073  1.2697  1.2424\n",
      "   1.2744   1.285  1.2851  1.3015  1.2524  1.2891  1.2762  1.2484  1.2836  1.2523]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:02 - loss: 0.0594\n",
      "[[ 1.2935  1.3212   1.303  1.2875  1.3245  1.2935  1.2787  1.3279  1.2909  1.2651\n",
      "   1.2957  1.3062  1.3063   1.322  1.2733  1.3106  1.2979  1.2696     1.3  1.2769]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:50 - loss: 0.0608\n",
      "[[ 1.3149  1.3425   1.325  1.3096  1.3456  1.3159   1.301  1.3479  1.3126  1.2869\n",
      "   1.3176   1.327  1.3274  1.3434  1.2953  1.3312   1.319  1.2906  1.3251  1.2964]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:38 - loss: 0.0600\n",
      "[[ 1.3346  1.3616  1.3443  1.3294  1.3641  1.3352  1.3241  1.3659  1.3324  1.3079\n",
      "   1.3373  1.3459  1.3465  1.3628  1.3171  1.3509  1.3382  1.3115  1.3418  1.3164]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:26 - loss: 0.0592\n",
      "[[ 1.3554  1.3822  1.3649  1.3471  1.3847  1.3531  1.3419  1.3868   1.353  1.3292\n",
      "   1.3576  1.3658  1.3668   1.383  1.3344  1.3715  1.3593  1.3317  1.3623  1.3331]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:14 - loss: 0.0576\n",
      "[[ 1.3749     1.4  1.3842  1.3679  1.4025  1.3788  1.3637  1.4047  1.3731  1.3499\n",
      "   1.3765  1.3852  1.3857  1.4014  1.3565  1.3899  1.3784  1.3504  1.3791  1.3526]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:02 - loss: 0.0565\n",
      "[[ 1.3958  1.4204  1.4052  1.3911  1.4233  1.4023  1.3897  1.4255  1.3933  1.3717\n",
      "    1.397  1.4058  1.4065  1.4216  1.3799  1.4109  1.4002  1.3768  1.4023  1.3785]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 49s - loss: 0.0558\n",
      "[[ 1.4163  1.4409  1.4256  1.4151  1.4438  1.4246  1.4143   1.446  1.4139  1.3916\n",
      "   1.4183  1.4273   1.427  1.4415  1.4042  1.4314  1.4204  1.4025  1.4255  1.4009]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 37s - loss: 0.0553\n",
      "[[ 1.4341  1.4573  1.4424  1.4361    1.46  1.4432   1.435  1.4614  1.4315  1.4108\n",
      "   1.4356   1.444  1.4437  1.4574  1.4248  1.4481  1.4381  1.4253  1.4449  1.4211]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0547\n",
      "[[ 1.4549  1.4787  1.4637  1.4605  1.4809  1.4664    1.46  1.4829  1.4529  1.4328\n",
      "   1.4567  1.4652  1.4654  1.4786  1.4498  1.4691  1.4592   1.451   1.468  1.4453]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0543\n",
      "[[ 1.4754   1.498  1.4847   1.483  1.5004  1.4888  1.4824  1.5025  1.4734  1.4534\n",
      "   1.4773  1.4856  1.4855  1.4986  1.4729  1.4892  1.4792  1.4751  1.4898  1.4674]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0537\n",
      "[[ 1.4945  1.5171  1.5037  1.5046  1.5195  1.5091  1.5041  1.5219  1.4934  1.4727\n",
      "   1.4967  1.5058  1.5048   1.518  1.4937  1.5088  1.4981  1.4972  1.5109   1.488]]\n",
      "\n",
      "512/512 [==============================] - 199s 388ms/step - loss: 0.0537\n",
      "Epoch 7/20\n",
      " 31/512 [>.............................] - ETA: 3:07 - loss: 0.0535\n",
      "[[ 1.5171  1.5393  1.5251  1.5285  1.5412  1.5316  1.5276  1.5432  1.5153  1.4955\n",
      "   1.5187  1.5276  1.5273  1.5391   1.518  1.5308   1.521  1.5213  1.5334  1.5118]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:53 - loss: 0.0504\n",
      "[[ 1.5369  1.5584  1.5455  1.5493   1.561  1.5528  1.5482   1.563  1.5363  1.5161\n",
      "   1.5397  1.5478   1.547  1.5597  1.5397  1.5507  1.5408  1.5439  1.5546  1.5334]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:38 - loss: 0.0477\n",
      "[[ 1.5576  1.5777  1.5655  1.5691   1.581  1.5731   1.569  1.5826  1.5552  1.5367\n",
      "   1.5592  1.5657  1.5663  1.5791  1.5599  1.5706  1.5609  1.5652  1.5752  1.5542]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:26 - loss: 0.0466\n",
      "[[ 1.5775  1.5977  1.5855  1.5903     1.6  1.5935  1.5902  1.6017  1.5753  1.5573\n",
      "   1.5794  1.5861  1.5865  1.5983  1.5812  1.5903  1.5809  1.5862  1.5952  1.5752]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:14 - loss: 0.0451\n",
      "[[ 1.6009  1.6206  1.6083  1.6149  1.6226  1.6169  1.6137  1.6253  1.5988  1.5807\n",
      "   1.6021  1.6105  1.6105  1.6209  1.6059  1.6129  1.6047  1.6103  1.6184  1.5993]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:03 - loss: 0.0442\n",
      "[[  1.622  1.6411  1.6285  1.6359   1.643  1.6372  1.6352  1.6455  1.6196  1.6026\n",
      "    1.623  1.6309  1.6312  1.6411  1.6271  1.6339  1.6257  1.6316  1.6394  1.6208]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:51 - loss: 0.0443\n",
      "[[ 1.6429  1.6617    1.65  1.6566  1.6636  1.6579  1.6557  1.6651  1.6407  1.6239\n",
      "   1.6442  1.6514  1.6516  1.6622   1.648  1.6543  1.6462   1.653  1.6604   1.642]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:39 - loss: 0.0439\n",
      "[[  1.662  1.6804  1.6689  1.6744  1.6819   1.677   1.675  1.6832  1.6597  1.6435\n",
      "   1.6635  1.6694  1.6703  1.6809   1.667  1.6732  1.6646  1.6723  1.6796  1.6609]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:27 - loss: 0.0438\n",
      "[[ 1.6807  1.6989  1.6879  1.6934  1.7008  1.6965  1.6939  1.7024  1.6785  1.6627\n",
      "   1.6821  1.6877  1.6888  1.6993  1.6864  1.6919  1.6836   1.691  1.6978  1.6801]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:15 - loss: 0.0428\n",
      "[[    1.7  1.7169   1.706  1.7122  1.7183  1.7143  1.7123  1.7202  1.6983   1.683\n",
      "   1.7003  1.7072  1.7077  1.7171  1.7058  1.7098  1.7028  1.7096   1.716  1.6996]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:03 - loss: 0.0423\n",
      "[[ 1.7207  1.7376  1.7269  1.7322  1.7394  1.7357  1.7334   1.741  1.7188  1.7037\n",
      "   1.7211   1.727   1.728  1.7384  1.7259  1.7307  1.7234  1.7308  1.7371  1.7202]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 50s - loss: 0.0418\n",
      "[[ 1.7407  1.7577  1.7472  1.7525  1.7599  1.7557  1.7537  1.7615   1.739  1.7232\n",
      "   1.7419  1.7484  1.7481  1.7582  1.7461  1.7511  1.7435  1.7517  1.7576  1.7405]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 38s - loss: 0.0413\n",
      "[[ 1.7576  1.7737  1.7635  1.7691  1.7757  1.7714  1.7697  1.7764  1.7561  1.7416\n",
      "   1.7589  1.7646  1.7643  1.7735  1.7625  1.7674  1.7605  1.7681  1.7734  1.7578]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0408\n",
      "[[ 1.7777   1.794  1.7838  1.7895  1.7956  1.7913  1.7904  1.7968  1.7765  1.7625\n",
      "   1.7791  1.7847   1.785  1.7938  1.7832  1.7875   1.781  1.7881  1.7932  1.7785]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0406\n",
      "[[ 1.7969  1.8127  1.8041  1.8087  1.8146  1.8114  1.8094  1.8161   1.796  1.7816\n",
      "   1.7986   1.804  1.8043  1.8137  1.8034  1.8065  1.7997  1.8077  1.8124  1.7979]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0403\n",
      "[[ 1.8163  1.8323  1.8228   1.829  1.8337  1.8307  1.8291  1.8356   1.816   1.801\n",
      "    1.818  1.8249  1.8239  1.8325  1.8226  1.8262   1.819  1.8267   1.832  1.8174]]\n",
      "\n",
      "512/512 [==============================] - 202s 394ms/step - loss: 0.0403\n",
      "Epoch 8/20\n",
      " 31/512 [>.............................] - ETA: 3:13 - loss: 0.0386\n",
      "[[ 1.8373  1.8525  1.8426  1.8494  1.8536  1.8501  1.8489   1.855  1.8359  1.8224\n",
      "   1.8385  1.8447  1.8443  1.8518  1.8431  1.8467  1.8401  1.8465  1.8515  1.8379]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 3:02 - loss: 0.0374\n",
      "[[ 1.8547   1.869  1.8605  1.8662  1.8708  1.8679  1.8656  1.8721  1.8543  1.8406\n",
      "   1.8569  1.8622  1.8615  1.8699  1.8608  1.8639  1.8576  1.8644  1.8691   1.856]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:46 - loss: 0.0357\n",
      "[[ 1.8742  1.8874  1.8793   1.884  1.8898  1.8865  1.8845  1.8907   1.872  1.8599\n",
      "   1.8752  1.8792  1.8796  1.8885  1.8784  1.8828  1.8765  1.8834  1.8882  1.8745]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:32 - loss: 0.0342\n",
      "[[  1.892  1.9053  1.8973  1.9024  1.9067  1.9043  1.9027  1.9078  1.8901  1.8785\n",
      "   1.8936  1.8974  1.8978  1.9057  1.8968  1.9006  1.8943  1.9009  1.9054  1.8927]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:19 - loss: 0.0329\n",
      "[[ 1.9119  1.9248   1.917  1.9227  1.9259  1.9234   1.922  1.9279  1.9102  1.8985\n",
      "   1.9127   1.918  1.9182  1.9249  1.9172  1.9198  1.9145  1.9205  1.9245  1.9127]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:06 - loss: 0.0318\n",
      "[[  1.931  1.9435  1.9351  1.9409  1.9442  1.9414  1.9408  1.9457  1.9291  1.9185\n",
      "   1.9317  1.9362  1.9369  1.9432  1.9357  1.9389  1.9334   1.939  1.9433  1.9316]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:54 - loss: 0.0314\n",
      "[[ 1.9485  1.9605  1.9531  1.9579  1.9615  1.9588  1.9576  1.9623   1.947  1.9361\n",
      "   1.9495  1.9538  1.9542  1.9608  1.9531  1.9558  1.9506  1.9564  1.9603  1.9489]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:41 - loss: 0.0312\n",
      "[[ 1.9668  1.9786  1.9709  1.9753  1.9793  1.9771  1.9758  1.9801  1.9652  1.9545\n",
      "   1.9676  1.9717  1.9722  1.9785  1.9709  1.9739  1.9685  1.9743  1.9784  1.9668]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:28 - loss: 0.0310\n",
      "[[ 1.9821  1.9934  1.9865  1.9902  1.9945  1.9923  1.9907  1.9953  1.9802  1.9702\n",
      "   1.9827  1.9859  1.9873  1.9934  1.9865   1.989   1.984  1.9892  1.9929  1.9821]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:15 - loss: 0.0307\n",
      "[[ 1.9985  2.0088  2.0016  2.0064  2.0096  2.0075  2.0062  2.0107  1.9973  1.9872\n",
      "   1.9984   2.003  2.0036  2.0085  2.0027  2.0043  2.0004  2.0047  2.0084  1.9985]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:03 - loss: 0.0304\n",
      "[[  2.016  2.0268  2.0205  2.0235  2.0279  2.0263  2.0248  2.0288  2.0154  2.0053\n",
      "   2.0168  2.0203   2.021  2.0274  2.0202  2.0226  2.0176  2.0233  2.0271  2.0165]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 50s - loss: 0.0299\n",
      "[[ 2.0344  2.0453  2.0389  2.0425  2.0471  2.0441  2.0436  2.0481  2.0334  2.0231\n",
      "   2.0356  2.0398  2.0395  2.0458  2.0384  2.0414  2.0362  2.0424  2.0456  2.0348]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 38s - loss: 0.0295\n",
      "[[ 2.0487  2.0593  2.0534  2.0565  2.0607   2.058  2.0572  2.0609  2.0483  2.0387\n",
      "   2.0504  2.0539  2.0534   2.059   2.052  2.0555  2.0509  2.0563  2.0596  2.0496]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0291\n",
      "[[ 2.0655  2.0762  2.0703  2.0736  2.0772  2.0749  2.0741  2.0778  2.0656  2.0562\n",
      "   2.0672  2.0708  2.0708  2.0759    2.07  2.0719  2.0678  2.0728  2.0758  2.0668]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 13s - loss: 0.0288\n",
      "[[ 2.0814  2.0913  2.0864  2.0889  2.0925  2.0908  2.0896  2.0934  2.0813  2.0721\n",
      "   2.0828   2.086  2.0864  2.0923  2.0862  2.0873   2.083  2.0886  2.0914  2.0826]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0284\n",
      "[[ 2.0989  2.1084  2.1022  2.1064  2.1089   2.107  2.1066    2.11   2.098  2.0896\n",
      "   2.0994  2.1039  2.1032  2.1081  2.1023  2.1048  2.1003  2.1046  2.1081  2.0994]]\n",
      "\n",
      "512/512 [==============================] - 203s 397ms/step - loss: 0.0285\n",
      "Epoch 9/20\n",
      " 31/512 [>.............................] - ETA: 3:23 - loss: 0.0268\n",
      "[[ 2.1159  2.1256  2.1197  2.1235  2.1265  2.1242  2.1234  2.1269  2.1155  2.1069\n",
      "    2.117  2.1207  2.1204  2.1251  2.1201   2.122  2.1179  2.1219  2.1251  2.1169]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 3:08 - loss: 0.0328\n",
      "[[ 2.1288  2.1375  2.1327  2.1355  2.1384   2.137  2.1354  2.1387   2.129  2.1204\n",
      "   2.1305  2.1329  2.1331  2.1378  2.1332  2.1342  2.1305  2.1347  2.1377  2.1306]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:57 - loss: 0.0328\n",
      "[[ 2.1442  2.1515  2.1465  2.1492  2.1524  2.1508  2.1499   2.153  2.1419  2.1352\n",
      "   2.1443  2.1464  2.1469  2.1519   2.146  2.1486  2.1452   2.149  2.1523  2.1442]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:44 - loss: 0.0335\n",
      "[[ 2.1581  2.1655  2.1608  2.1633  2.1661   2.165  2.1638  2.1665  2.1566  2.1498\n",
      "    2.159  2.1607  2.1612  2.1657  2.1606  2.1627  2.1594  2.1626  2.1658  2.1588]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:30 - loss: 0.0337\n",
      "[[ 2.1732  2.1805  2.1759  2.1786  2.1809  2.1796  2.1789  2.1818   2.172  2.1654\n",
      "   2.1737   2.176  2.1767  2.1805   2.176  2.1777  2.1749  2.1779  2.1807   2.174]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:16 - loss: 0.0329\n",
      "[[ 2.1875  2.1948  2.1893  2.1929  2.1943  2.1936  2.1932  2.1951  2.1867  2.1807\n",
      "   2.1879   2.191  2.1909  2.1937  2.1903  2.1918   2.189  2.1919  2.1946  2.1881]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 2:01 - loss: 0.0320\n",
      "[[ 2.2017  2.2085  2.2045   2.207  2.2094  2.2079  2.2072  2.2096  2.2008  2.1947\n",
      "   2.2025   2.205   2.205  2.2087  2.2043  2.2059  2.2028  2.2063  2.2087   2.202]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:47 - loss: 0.0309\n",
      "[[ 2.2158  2.2226  2.2182  2.2203  2.2227  2.2218   2.221  2.2229  2.2151  2.2091\n",
      "   2.2166  2.2185  2.2189  2.2222  2.2178    2.22  2.2168  2.2199  2.2227  2.2158]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:33 - loss: 0.0302\n",
      "[[ 2.2286   2.235  2.2305  2.2328  2.2352  2.2342  2.2336  2.2352  2.2271  2.2223\n",
      "   2.2286  2.2304  2.2315  2.2343  2.2306  2.2325  2.2297   2.232  2.2347  2.2283]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:19 - loss: 0.0294\n",
      "[[ 2.2418   2.248  2.2441  2.2463  2.2487  2.2477  2.2464  2.2494  2.2416  2.2352\n",
      "   2.2422  2.2449  2.2452  2.2484  2.2445  2.2453  2.2431  2.2458  2.2481  2.2419]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:06 - loss: 0.0289\n",
      "[[ 2.2555  2.2617  2.2576  2.2594  2.2621  2.2616  2.2607  2.2627  2.2549  2.2493\n",
      "   2.2557   2.258  2.2585  2.2616  2.2578   2.259  2.2564  2.2595   2.262  2.2557]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 53s - loss: 0.0287\n",
      "[[ 2.2704  2.2762  2.2725  2.2751  2.2774  2.2756  2.2755  2.2779  2.2695  2.2638\n",
      "   2.2705  2.2736  2.2733  2.2765  2.2725  2.2742  2.2715  2.2744  2.2763  2.2705]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 39s - loss: 0.0293\n",
      "[[ 2.2802  2.2857  2.2828  2.2843  2.2865  2.2854  2.2848  2.2865  2.2795  2.2749\n",
      "   2.2811   2.283  2.2824  2.2856  2.2818  2.2841  2.2812  2.2839  2.2861  2.2806]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 26s - loss: 0.0298\n",
      "[[ 2.2937  2.2998  2.2968  2.2984  2.3001  2.2994  2.2988  2.3008  2.2943  2.2886\n",
      "   2.2949  2.2976   2.297  2.2996   2.297  2.2971  2.2948  2.2979  2.2995  2.2952]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 13s - loss: 0.0297\n",
      "[[ 2.3068  2.3116  2.3086  2.3096   2.312  2.3108   2.311  2.3123  2.3055  2.3022\n",
      "   2.3069  2.3079  2.3086  2.3117  2.3087  2.3095  2.3069  2.3099  2.3115  2.3073]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0300\n",
      "[[ 2.3183  2.3227  2.3199  2.3215  2.3232  2.3213  2.3225  2.3231  2.3173  2.3136\n",
      "   2.3178  2.3198  2.3199  2.3226  2.3188  2.3215  2.3183  2.3208  2.3226  2.3181]]\n",
      "\n",
      "512/512 [==============================] - 210s 409ms/step - loss: 0.0300\n",
      "Epoch 10/20\n",
      " 31/512 [>.............................] - ETA: 3:05 - loss: 0.0485\n",
      "[[ 2.3293  2.3345  2.3317  2.3336  2.3352  2.3333   2.334  2.3354  2.3299  2.3254\n",
      "   2.3297  2.3324  2.3315  2.3345  2.3316  2.3326  2.3302  2.3322   2.334  2.3305]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:52 - loss: 0.0592\n",
      "[[ 2.3363  2.3405  2.3382  2.3392  2.3404  2.3403  2.3398  2.3405  2.3371  2.3326\n",
      "   2.3374   2.338  2.3385  2.3406  2.3388  2.3384  2.3367  2.3387  2.3404  2.3375]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:40 - loss: 0.0618\n",
      "[[ 2.3439  2.3469  2.3446  2.3455   2.347  2.3469  2.3463  2.3476  2.3429  2.3397\n",
      "   2.3443   2.345  2.3447  2.3474  2.3452  2.3452  2.3445  2.3459  2.3467  2.3435]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:27 - loss: 0.0622\n",
      "[[ 2.3533  2.3562  2.3539  2.3543  2.3566  2.3561  2.3544  2.3569  2.3523  2.3484\n",
      "    2.353  2.3536  2.3538  2.3571   2.355  2.3541  2.3538  2.3547  2.3559  2.3532]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:15 - loss: 0.0614\n",
      "[[ 2.3639  2.3674  2.3649  2.3653  2.3673  2.3669  2.3657  2.3681  2.3632  2.3597\n",
      "   2.3645  2.3647  2.3647  2.3675  2.3656  2.3657  2.3651  2.3662  2.3666  2.3641]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:02 - loss: 0.0608\n",
      "[[ 2.3732  2.3768  2.3732  2.3759   2.376  2.3763  2.3759  2.3771  2.3733  2.3701\n",
      "   2.3737  2.3766  2.3747  2.3756  2.3747   2.375  2.3742  2.3753  2.3759  2.3729]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:50 - loss: 0.0587\n",
      "[[ 2.3829  2.3861  2.3846  2.3854  2.3872  2.3859  2.3857  2.3873  2.3825  2.3797\n",
      "   2.3839   2.385  2.3845   2.387  2.3839  2.3846  2.3832  2.3856  2.3864  2.3823]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:39 - loss: 0.0554\n",
      "[[ 2.3916  2.3951  2.3929  2.3936  2.3947  2.3951  2.3944  2.3949  2.3919   2.389\n",
      "   2.3923  2.3935  2.3938  2.3951  2.3928  2.3936  2.3922  2.3936  2.3953   2.391]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:27 - loss: 0.0522\n",
      "[[ 2.4012   2.404  2.4016  2.4029  2.4039  2.4044  2.4037  2.4038  2.4002  2.3988\n",
      "   2.4013  2.4019  2.4028  2.4037  2.4021  2.4033  2.4019  2.4026  2.4041  2.4002]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:14 - loss: 0.0489\n",
      "[[   2.41  2.4126  2.4111   2.412  2.4133  2.4133  2.4122  2.4136  2.4097  2.4071\n",
      "   2.4106  2.4116  2.4119  2.4136  2.4113  2.4115  2.4107  2.4121   2.413  2.4096]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:02 - loss: 0.0459\n",
      "[[ 2.4188  2.4211  2.4194  2.4202  2.4215  2.4218  2.4209  2.4219  2.4181   2.416\n",
      "   2.4188  2.4196  2.4202   2.421  2.4194  2.4202  2.4196  2.4203  2.4217  2.4183]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 50s - loss: 0.0432\n",
      "[[ 2.4286  2.4309  2.4295  2.4309  2.4315  2.4311  2.4308  2.4319  2.4282  2.4259\n",
      "   2.4289  2.4299    2.43  2.4312  2.4295  2.4303  2.4292  2.4303  2.4313  2.4284]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 37s - loss: 0.0411\n",
      "[[ 2.4358  2.4384  2.4374  2.4381  2.4389  2.4386  2.4382  2.4388  2.4359  2.4337\n",
      "   2.4368  2.4374  2.4372  2.4385   2.437  2.4379  2.4363  2.4378  2.4389  2.4362]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0392\n",
      "[[ 2.4456  2.4484  2.4471  2.4479  2.4486  2.4484  2.4482  2.4491  2.4463  2.4432\n",
      "   2.4463  2.4476  2.4476  2.4486  2.4476   2.447  2.4461  2.4477  2.4485  2.4464]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0374\n",
      "[[  2.455  2.4574   2.456  2.4566  2.4575  2.4571  2.4573  2.4577  2.4546  2.4531\n",
      "   2.4553  2.4556  2.4563  2.4574  2.4563  2.4565   2.455  2.4565  2.4574  2.4554]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0359\n",
      "[[ 2.4639  2.4662  2.4649  2.4656  2.4664  2.4656  2.4662  2.4666  2.4637  2.4618\n",
      "    2.464  2.4648   2.465  2.4662  2.4644  2.4655  2.4641  2.4656  2.4662   2.464]]\n",
      "\n",
      "512/512 [==============================] - 201s 392ms/step - loss: 0.0359\n",
      "Epoch 11/20\n",
      " 31/512 [>.............................] - ETA: 3:11 - loss: 0.0198\n",
      "[[ 2.4712  2.4734  2.4721  2.4729  2.4735  2.4728  2.4732  2.4737  2.4711  2.4697\n",
      "   2.4713  2.4723   2.472  2.4733  2.4721  2.4725  2.4716  2.4723   2.473  2.4715]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:56 - loss: 0.0230\n",
      "[[ 2.4772   2.479  2.4777  2.4783  2.4789  2.4788  2.4788  2.4789  2.4772  2.4757\n",
      "   2.4777  2.4774   2.478   2.479  2.4782   2.478  2.4773  2.4782  2.4789  2.4776]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:43 - loss: 0.0235\n",
      "[[ 2.4822  2.4836   2.482  2.4829  2.4833  2.4833  2.4832  2.4835  2.4814  2.4804\n",
      "   2.4821  2.4827  2.4823  2.4832  2.4829  2.4824  2.4821  2.4827   2.483  2.4818]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:30 - loss: 0.0237\n",
      "[[ 2.4895   2.491  2.4897  2.4897  2.4912  2.4909  2.4901  2.4911   2.489  2.4872\n",
      "   2.4896  2.4892  2.4896  2.4914  2.4904  2.4899  2.4897  2.4903  2.4908  2.4898]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:18 - loss: 0.0235\n",
      "[[ 2.5001  2.5021  2.5004  2.5012  2.5018  2.5016  2.5013  2.5024  2.4996  2.4976\n",
      "   2.5004  2.5007  2.5004  2.5019  2.5013  2.5012  2.5005  2.5012  2.5012  2.5002]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:05 - loss: 0.0264\n",
      "[[ 2.5077  2.5094  2.5073  2.5095  2.5089  2.5091  2.5095  2.5093  2.5075  2.5062\n",
      "   2.5076  2.5094  2.5082  2.5086  2.5083  2.5088  2.5081  2.5082  2.5089  2.5075]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:53 - loss: 0.0284\n",
      "[[ 2.5161  2.5175  2.5167   2.517  2.5179  2.5176  2.5173  2.5179  2.5159  2.5136\n",
      "   2.5162  2.5173  2.5163  2.5182  2.5162  2.5173  2.5161  2.5172  2.5177  2.5159]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:40 - loss: 0.0290\n",
      "[[ 2.5247  2.5269  2.5259  2.5264  2.5264  2.5274  2.5264  2.5268  2.5252  2.5229\n",
      "   2.5247  2.5263  2.5255  2.5265  2.5261  2.5258  2.5248  2.5255  2.5266  2.5252]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:27 - loss: 0.0295\n",
      "[[ 2.5296  2.5307  2.5293  2.5305  2.5303   2.531  2.5303  2.5303  2.5286  2.5284\n",
      "   2.5287  2.5296  2.5299  2.5296  2.5297  2.5303  2.5296  2.5296  2.5305  2.5288]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:15 - loss: 0.0297\n",
      "[[ 2.5354  2.5367  2.5355  2.5362  2.5371   2.537  2.5361  2.5372   2.535  2.5345\n",
      "   2.5358  2.5356  2.5364  2.5368   2.536  2.5364  2.5361  2.5359  2.5367  2.5353]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:02 - loss: 0.0295\n",
      "[[   2.54  2.5414  2.5404  2.5413  2.5416  2.5424  2.5409  2.5416  2.5403  2.5389\n",
      "   2.5411  2.5406  2.5414  2.5411  2.5406  2.5412  2.5409   2.541   2.542  2.5401]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 49s - loss: 0.0294\n",
      "[[  2.546  2.5472  2.5462   2.548  2.5477  2.5474  2.5471  2.5477  2.5458  2.5452\n",
      "   2.5461  2.5472  2.5471   2.547  2.5471  2.5474  2.5467  2.5465  2.5472   2.546]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 37s - loss: 0.0291\n",
      "[[ 2.5506  2.5521  2.5511  2.5517  2.5519  2.5522  2.5517  2.5516  2.5511    2.55\n",
      "   2.5508  2.5515  2.5515  2.5514  2.5513  2.5518  2.5513  2.5509  2.5525  2.5507]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0288\n",
      "[[ 2.5566  2.5583  2.5569  2.5581  2.5578  2.5581  2.5578  2.5585  2.5575  2.5555\n",
      "   2.5571  2.5583  2.5581  2.5574  2.5582  2.5574  2.5573   2.557  2.5579  2.5578]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0282\n",
      "[[ 2.5624   2.564  2.5625  2.5632  2.5634  2.5632  2.5634  2.5637  2.5626  2.5619\n",
      "   2.5627  2.5634  2.5634  2.5629  2.5634  2.5633  2.5624  2.5627  2.5634  2.5634]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0280\n",
      "[[ 2.5675  2.5685   2.568  2.5681  2.5686  2.5678  2.5685  2.5686  2.5675  2.5666\n",
      "   2.5674  2.5678  2.5683  2.5684  2.5674  2.5686  2.5672  2.5678  2.5683  2.5677]]\n",
      "\n",
      "512/512 [==============================] - 199s 388ms/step - loss: 0.0280\n",
      "Epoch 12/20\n",
      " 31/512 [>.............................] - ETA: 3:05 - loss: 0.0271\n",
      "[[ 2.5716  2.5723  2.5721  2.5722  2.5726  2.5722  2.5728  2.5726  2.5717  2.5711\n",
      "   2.5718  2.5723  2.5723  2.5725  2.5719  2.5724  2.5715  2.5718  2.5719  2.5721]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:52 - loss: 0.0268\n",
      "[[ 2.5764  2.5766  2.5766  2.5767  2.5773  2.5772  2.5769   2.577   2.576  2.5755\n",
      "   2.5768  2.5763  2.5769  2.5773   2.577  2.5767  2.5765  2.5767  2.5768  2.5768]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:39 - loss: 0.0268\n",
      "[[ 2.5815  2.5821  2.5817  2.5822  2.5825  2.5826  2.5824  2.5824   2.582  2.5805\n",
      "   2.5821   2.582  2.5817  2.5825  2.5818   2.582  2.5815  2.5819  2.5823  2.5817]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:28 - loss: 0.0279\n",
      "[[ 2.5879  2.5891  2.5883  2.5885  2.5895   2.589   2.589  2.5898  2.5884   2.586\n",
      "   2.5889   2.588  2.5887  2.5898  2.5893   2.588  2.5876  2.5887   2.589  2.5884]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:16 - loss: 0.0302\n",
      "[[ 2.5915  2.5923  2.5922  2.5922  2.5923  2.5922  2.5921  2.5929  2.5917  2.5903\n",
      "   2.5923  2.5915  2.5922  2.5929  2.5928   2.592   2.592  2.5926  2.5923  2.5922]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:03 - loss: 0.0321\n",
      "[[ 2.5967   2.597   2.596  2.5963  2.5963  2.5963  2.5966  2.5969  2.5956   2.596\n",
      "   2.5962  2.5962  2.5968   2.597  2.5955  2.5972  2.5967  2.5966  2.5966  2.5961]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:50 - loss: 0.0353\n",
      "[[ 2.5943   2.594  2.5938  2.5953  2.5941  2.5952  2.5941  2.5941  2.5945   2.593\n",
      "   2.5944  2.5949  2.5953  2.5944  2.5938  2.5949  2.5947  2.5943   2.594  2.5931]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:38 - loss: 0.0440\n",
      "[[    2.6  2.5995  2.5999  2.6004  2.6006  2.6005  2.6002  2.6009  2.6004     2.6\n",
      "   2.5991  2.6013  2.6003  2.6003  2.5999  2.6005  2.6005  2.6004  2.6006  2.5999]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:26 - loss: 0.0499\n",
      "[[ 2.5982  2.5973  2.5985  2.5977  2.5978   2.597  2.5974   2.599  2.5982  2.5965\n",
      "   2.5972  2.5972  2.5976  2.5972  2.5971  2.5974   2.597  2.5983  2.5983  2.5976]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:14 - loss: 0.0546\n",
      "[[ 2.6045  2.6046  2.6028  2.6047  2.6039   2.605  2.6044  2.6054  2.6038  2.6068\n",
      "   2.6052   2.605  2.6045  2.6032  2.6031  2.6054  2.6044  2.6029  2.6058   2.604]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:01 - loss: 0.0562\n",
      "[[ 2.6037  2.6044  2.6043  2.6036  2.6042  2.6061   2.606  2.6053  2.6045  2.6028\n",
      "   2.6051  2.6049   2.606  2.6051  2.6037  2.6051  2.6043  2.6037   2.607  2.6045]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 49s - loss: 0.0564\n",
      "[[  2.607  2.6064  2.6077  2.6068  2.6076  2.6084  2.6079   2.608  2.6076  2.6053\n",
      "   2.6088  2.6074  2.6088  2.6075  2.6077  2.6073   2.608  2.6057  2.6089  2.6068]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 37s - loss: 0.0562\n",
      "[[  2.611  2.6123  2.6113  2.6106  2.6118  2.6127  2.6125  2.6119  2.6121  2.6101\n",
      "   2.6127  2.6125  2.6131  2.6116  2.6122  2.6118  2.6125  2.6098  2.6134   2.611]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0562\n",
      "[[ 2.6172  2.6193  2.6188  2.6171  2.6187  2.6191  2.6197  2.6192  2.6184  2.6165\n",
      "    2.619   2.618  2.6198  2.6192  2.6189  2.6179  2.6184  2.6178    2.62  2.6186]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0561\n",
      "[[ 2.6213  2.6225  2.6218  2.6206  2.6222  2.6219  2.6231  2.6228  2.6212  2.6207\n",
      "   2.6214  2.6208  2.6228  2.6222  2.6222  2.6216  2.6218  2.6212  2.6229  2.6221]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0556\n",
      "[[ 2.6243  2.6253  2.6242   2.624  2.6247  2.6251  2.6259  2.6263  2.6243  2.6241\n",
      "   2.6237  2.6248  2.6255  2.6251  2.6247  2.6251  2.6241   2.624   2.625  2.6245]]\n",
      "\n",
      "512/512 [==============================] - 200s 390ms/step - loss: 0.0556\n",
      "Epoch 13/20\n",
      " 31/512 [>.............................] - ETA: 3:09 - loss: 0.0544\n",
      "[[ 2.6242   2.624  2.6245  2.6241  2.6236  2.6246  2.6251  2.6247  2.6237   2.625\n",
      "   2.6241  2.6242  2.6248  2.6239   2.624  2.6247  2.6243  2.6234  2.6244   2.624]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:56 - loss: 0.0463\n",
      "[[ 2.6243  2.6238   2.625  2.6237   2.624  2.6251  2.6245  2.6238  2.6231  2.6239\n",
      "   2.6248  2.6238  2.6244  2.6243  2.6236  2.6249  2.6239  2.6243  2.6242  2.6241]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:44 - loss: 0.0392\n",
      "[[ 2.6235   2.623  2.6231  2.6233  2.6227   2.624  2.6238  2.6228  2.6228  2.6236\n",
      "   2.6234  2.6234   2.623  2.6229   2.623  2.6232   2.623  2.6226  2.6227  2.6228]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:31 - loss: 0.0348\n",
      "[[ 2.6259  2.6258   2.626  2.6259  2.6259  2.6266  2.6263  2.6257  2.6258  2.6258\n",
      "   2.6269  2.6261  2.6256   2.626  2.6262  2.6261  2.6255  2.6257  2.6259  2.6261]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:18 - loss: 0.0317\n",
      "[[ 2.6314  2.6316  2.6321  2.6318  2.6319  2.6324   2.632  2.6318  2.6315  2.6312\n",
      "   2.6328  2.6319  2.6315  2.6319  2.6319  2.6319  2.6314  2.6321  2.6316  2.6317]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:05 - loss: 0.0305\n",
      "[[ 2.6363  2.6364  2.6365  2.6363  2.6366  2.6366   2.637  2.6364  2.6359  2.6361\n",
      "    2.637  2.6363  2.6362  2.6366  2.6362  2.6365  2.6362  2.6367  2.6363  2.6365]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:52 - loss: 0.0293\n",
      "[[ 2.6402  2.6404  2.6401  2.6401  2.6404  2.6405  2.6406  2.6403  2.6401  2.6401\n",
      "   2.6407  2.6405  2.6404  2.6405  2.6402  2.6406  2.6401  2.6405  2.6402  2.6402]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:39 - loss: 0.0285\n",
      "[[ 2.6442  2.6442  2.6443  2.6445  2.6444  2.6447  2.6443  2.6446   2.644  2.6437\n",
      "   2.6443  2.6447  2.6442  2.6442  2.6442  2.6443  2.6443   2.644  2.6442  2.6439]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:26 - loss: 0.0272\n",
      "[[  2.643  2.6432  2.6432  2.6433   2.643  2.6432  2.6431  2.6435  2.6426  2.6428\n",
      "   2.6433  2.6433  2.6431  2.6431   2.643   2.643  2.6429  2.6432  2.6433  2.6428]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:14 - loss: 0.0259\n",
      "[[ 2.6449  2.6451  2.6449  2.6448  2.6447  2.6451  2.6449  2.6453  2.6446  2.6452\n",
      "   2.6453  2.6448   2.645   2.645  2.6447  2.6451  2.6449  2.6447   2.645  2.6447]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:02 - loss: 0.0245\n",
      "[[ 2.6474  2.6477  2.6477  2.6477  2.6475   2.648  2.6475  2.6477  2.6474  2.6472\n",
      "   2.6482  2.6477  2.6478  2.6477  2.6474  2.6477  2.6473  2.6477  2.6478  2.6476]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 50s - loss: 0.0234\n",
      "[[ 2.6507  2.6509  2.6508  2.6512  2.6509  2.6509  2.6508  2.6511  2.6505  2.6506\n",
      "   2.6512  2.6511  2.6511   2.651   2.651  2.6509  2.6506  2.6508  2.6509  2.6509]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 37s - loss: 0.0222\n",
      "[[ 2.6534   2.654  2.6538  2.6541  2.6538   2.654  2.6539   2.654  2.6538  2.6534\n",
      "   2.6539  2.6541  2.6538   2.654  2.6539  2.6537  2.6534  2.6538   2.654  2.6537]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0210\n",
      "[[ 2.6551  2.6555  2.6556  2.6557  2.6555  2.6556  2.6555  2.6556  2.6555  2.6546\n",
      "   2.6555  2.6557  2.6555  2.6557  2.6556  2.6553  2.6552  2.6554  2.6556  2.6555]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0200\n",
      "[[  2.658  2.6581  2.6581  2.6584  2.6581  2.6581  2.6582  2.6581   2.658  2.6579\n",
      "   2.6581  2.6584   2.658  2.6582  2.6581  2.6581   2.658   2.658  2.6582  2.6581]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0191\n",
      "[[ 2.6601  2.6602  2.6602  2.6602    2.66  2.6602  2.6602  2.6602    2.66  2.6601\n",
      "   2.6601  2.6601    2.66  2.6602    2.66  2.6602  2.6601    2.66  2.6602  2.6601]]\n",
      "\n",
      "512/512 [==============================] - 202s 395ms/step - loss: 0.0191\n",
      "Epoch 14/20\n",
      " 31/512 [>.............................] - ETA: 3:14 - loss: 0.0091\n",
      "[[  2.662   2.662  2.6621  2.6623   2.662  2.6621  2.6622  2.6622  2.6618  2.6622\n",
      "   2.6619  2.6619  2.6619  2.6622  2.6619  2.6621  2.6619   2.662   2.662   2.662]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 3:00 - loss: 0.0091\n",
      "[[ 2.6636  2.6636  2.6636  2.6637  2.6637  2.6639  2.6638  2.6637  2.6634  2.6638\n",
      "   2.6636  2.6634  2.6635  2.6638  2.6635  2.6637  2.6636  2.6636  2.6636  2.6636]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:48 - loss: 0.0094\n",
      "[[ 2.6669  2.6671   2.667  2.6672  2.6672  2.6673  2.6672  2.6672   2.667  2.6669\n",
      "   2.6669  2.6671  2.6669  2.6672  2.6668  2.6669  2.6668   2.667   2.667  2.6667]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:34 - loss: 0.0100\n",
      "[[ 2.6705  2.6707  2.6703  2.6706  2.6708  2.6708  2.6706  2.6707  2.6707  2.6701\n",
      "   2.6705  2.6708  2.6704  2.6707  2.6705  2.6706  2.6706  2.6707  2.6706  2.6704]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:19 - loss: 0.0110\n",
      "[[ 2.6738   2.674  2.6735  2.6739  2.6739  2.6737   2.674  2.6738  2.6737  2.6734\n",
      "   2.6735  2.6738  2.6739   2.674   2.674  2.6737  2.6739   2.674  2.6738  2.6735]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:06 - loss: 0.0123\n",
      "[[ 2.6737  2.6737  2.6736  2.6737   2.674   2.674   2.674  2.6739  2.6739  2.6735\n",
      "   2.6738  2.6742  2.6739  2.6741  2.6737  2.6736  2.6736   2.674  2.6735  2.6737]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:53 - loss: 0.0130\n",
      "[[ 2.6743  2.6743  2.6742  2.6746  2.6744  2.6747  2.6742  2.6744  2.6743  2.6744\n",
      "    2.674  2.6747  2.6744  2.6743  2.6743  2.6742  2.6745  2.6744  2.6745  2.6741]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:41 - loss: 0.0155\n",
      "[[ 2.6751   2.675  2.6747  2.6752  2.6749  2.6748  2.6751  2.6747  2.6748  2.6754\n",
      "   2.6747  2.6754  2.6745  2.6745   2.675  2.6748  2.6751  2.6744   2.675   2.675]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:28 - loss: 0.0200\n",
      "[[ 2.6784  2.6778   2.678  2.6778  2.6777  2.6774  2.6781  2.6772  2.6772  2.6772\n",
      "   2.6776  2.6772  2.6777  2.6778  2.6774  2.6782  2.6779   2.678  2.6791   2.677]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:15 - loss: 0.0258\n",
      "[[ 2.6788  2.6791  2.6782  2.6798  2.6771   2.679  2.6793  2.6798  2.6788  2.6796\n",
      "   2.6786  2.6781  2.6796  2.6787   2.679  2.6795  2.6784  2.6773  2.6778   2.679]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:03 - loss: 0.0302\n",
      "[[ 2.6805   2.681  2.6797  2.6813  2.6787  2.6821  2.6819  2.6809  2.6797  2.6811\n",
      "   2.6811  2.6808  2.6811  2.6795  2.6811  2.6801  2.6816  2.6796  2.6802  2.6824]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 50s - loss: 0.0340\n",
      "[[ 2.6792  2.6786  2.6782  2.6787  2.6767   2.678  2.6791  2.6783  2.6791  2.6787\n",
      "   2.6811   2.679  2.6789  2.6784  2.6792   2.678  2.6799  2.6772  2.6767  2.6808]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 38s - loss: 0.0392\n",
      "[[ 2.6769  2.6768  2.6757  2.6763   2.675  2.6756  2.6762  2.6751  2.6768  2.6766\n",
      "   2.6767  2.6777  2.6764  2.6767  2.6773  2.6767  2.6777  2.6747  2.6755  2.6788]]\n",
      "\n",
      "447/512 [=========================>....] - ETA: 25s - loss: 0.0454\n",
      "[[ 2.6782  2.6787  2.6774  2.6777  2.6776   2.677  2.6785   2.677  2.6792  2.6766\n",
      "   2.6785  2.6775  2.6777  2.6779  2.6802  2.6785  2.6771  2.6767  2.6774  2.6792]]\n",
      "\n",
      "479/512 [===========================>..] - ETA: 12s - loss: 0.0504\n",
      "[[ 2.6825   2.682  2.6828  2.6816  2.6816  2.6811  2.6841  2.6809  2.6828  2.6827\n",
      "   2.6837    2.68  2.6819  2.6818  2.6838  2.6826  2.6831  2.6802  2.6822  2.6849]]\n",
      "\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0531\n",
      "[[ 2.6832  2.6826  2.6843   2.683  2.6817  2.6849  2.6843   2.682  2.6843  2.6833\n",
      "   2.6834  2.6811  2.6823  2.6827  2.6845  2.6849  2.6833  2.6801   2.682  2.6819]]\n",
      "\n",
      "512/512 [==============================] - 201s 393ms/step - loss: 0.0532\n",
      "Epoch 15/20\n",
      " 31/512 [>.............................] - ETA: 3:07 - loss: 0.0758\n",
      "[[   2.68    2.68  2.6825  2.6805  2.6794  2.6831  2.6812  2.6797  2.6809  2.6804\n",
      "   2.6827  2.6796  2.6804  2.6801  2.6807  2.6823  2.6819  2.6791   2.679  2.6809]]\n",
      "\n",
      " 63/512 [==>...........................] - ETA: 2:58 - loss: 0.0690\n",
      "[[ 2.6761  2.6758  2.6775  2.6757  2.6756  2.6788  2.6766   2.676   2.675  2.6763\n",
      "   2.6775  2.6752  2.6767  2.6766  2.6763   2.678  2.6777   2.676  2.6762  2.6761]]\n",
      "\n",
      " 95/512 [====>.........................] - ETA: 2:44 - loss: 0.0602\n",
      "[[  2.675  2.6743  2.6769  2.6751  2.6752  2.6774  2.6764  2.6752  2.6745   2.676\n",
      "   2.6769  2.6748  2.6756  2.6764  2.6754  2.6748  2.6764  2.6755  2.6754  2.6751]]\n",
      "\n",
      "127/512 [======>.......................] - ETA: 2:30 - loss: 0.0534\n",
      "[[ 2.6749  2.6752  2.6763   2.675  2.6757  2.6764  2.6757   2.675  2.6749  2.6753\n",
      "   2.6772  2.6753  2.6747   2.676  2.6757  2.6747  2.6764   2.675  2.6753  2.6763]]\n",
      "\n",
      "159/512 [========>.....................] - ETA: 2:17 - loss: 0.0483\n",
      "[[ 2.6774  2.6777  2.6783  2.6779  2.6784   2.679  2.6784  2.6776  2.6773  2.6778\n",
      "   2.6795  2.6776  2.6777  2.6786  2.6782  2.6774  2.6786  2.6783  2.6778  2.6783]]\n",
      "\n",
      "191/512 [==========>...................] - ETA: 2:04 - loss: 0.0446\n",
      "[[ 2.6793  2.6792  2.6794  2.6794  2.6798  2.6795  2.6797   2.679  2.6787  2.6796\n",
      "   2.6806  2.6795  2.6793  2.6798  2.6795  2.6793  2.6799  2.6795  2.6793  2.6799]]\n",
      "\n",
      "223/512 [============>.................] - ETA: 1:52 - loss: 0.0412\n",
      "[[ 2.6807  2.6805  2.6806   2.681  2.6812   2.681  2.6809  2.6804  2.6805  2.6807\n",
      "   2.6819  2.6807  2.6808  2.6809  2.6806   2.681   2.681   2.681  2.6808  2.6808]]\n",
      "\n",
      "255/512 [=============>................] - ETA: 1:39 - loss: 0.0386\n",
      "[[ 2.6837  2.6834  2.6836  2.6842   2.684   2.684  2.6839  2.6836  2.6833  2.6838\n",
      "   2.6845  2.6839  2.6836  2.6837  2.6833  2.6839  2.6838  2.6836  2.6836  2.6833]]\n",
      "\n",
      "287/512 [===============>..............] - ETA: 1:27 - loss: 0.0358\n",
      "[[ 2.6823   2.682   2.682  2.6823  2.6821  2.6823  2.6821  2.6821  2.6817   2.682\n",
      "   2.6828  2.6819   2.682   2.682  2.6819  2.6824  2.6822   2.682  2.6823  2.6818]]\n",
      "\n",
      "319/512 [=================>............] - ETA: 1:14 - loss: 0.0333\n",
      "[[ 2.6838  2.6838   2.684  2.6839  2.6839  2.6841  2.6839  2.6841  2.6839   2.684\n",
      "   2.6845  2.6837  2.6839  2.6841  2.6838  2.6842   2.684  2.6837   2.684   2.684]]\n",
      "\n",
      "351/512 [===================>..........] - ETA: 1:02 - loss: 0.0311\n",
      "[[  2.685  2.6851  2.6852   2.685   2.685  2.6854   2.685  2.6853  2.6851  2.6848\n",
      "   2.6857  2.6851  2.6851  2.6851   2.685  2.6854   2.685   2.685  2.6852  2.6853]]\n",
      "\n",
      "383/512 [=====================>........] - ETA: 49s - loss: 0.0292\n",
      "[[ 2.6869  2.6869  2.6869  2.6868  2.6869  2.6869  2.6868  2.6869  2.6869  2.6869\n",
      "   2.6872  2.6869  2.6869   2.687  2.6869   2.687  2.6868  2.6867  2.6869  2.6869]]\n",
      "\n",
      "415/512 [=======================>......] - ETA: 37s - loss: 0.0275\n",
      "[[ 2.6882  2.6884  2.6884  2.6883  2.6883  2.6882  2.6884  2.6883  2.6883  2.6884\n",
      "   2.6884  2.6886  2.6882  2.6883  2.6884  2.6883  2.6883  2.6882  2.6884  2.6883]]\n",
      "\n",
      "419/512 [=======================>......] - ETA: 35s - loss: 0.0273"
     ]
    }
   ],
   "source": [
    "history = model_loss.fit([x0, dW, dN], target, batch_size=32, initial_epoch=0, epochs=20, callbacks=callbacks, shuffle=False)\n",
    "df_loss = pd.DataFrame(history.history['loss'])\n",
    "df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = 'b', 'r', 'g', 'y', 'm', 'c'\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "ax[0].plot(history.history['loss'], c='b')\n",
    "ax[0].set_title('Loss')\n",
    "ax[1].plot(np.arange(y0_callback.y0s.shape[0]), np.exp(1) * np.ones((y0_callback.y0s.shape[0],)), c='k', alpha=0.3, linestyle='dashed')\n",
    "\n",
    "ax[1].set_title('Y0')\n",
    "for i in np.arange(y0_callback.y0s.shape[1]):\n",
    "    ax[1].plot(y0_callback.y0s[:, i], c=colors[i%len(colors)], linewidth=1)\n",
    "fig.savefig('FBSDE20.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "* Alpha has to grow with number of dimensions to avoid explosion\n",
    "* Number of samples has to grow with number of dimensions to converge to Y_0, loss will be close to zero if the number of samples is too small. What is a good number of samples?\n",
    "* Too little samples leads to plain memorizing overfitting. Could this be a theorem? Maybe reduce the size of the neural net to avoid overfitting, it's overparameterized? Can we quantify overfitting?\n",
    "* Perhaps come up with another example with closed solution that satisfies monotonicity property"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
