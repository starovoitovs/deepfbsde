{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Consider\n",
    "\n",
    "$$\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "W \\in \\R ^m, N \\in \\R^\\ell\\\\\n",
    "X\\in \\R^d, Y\\in \\R^n, Z\\in \\R^{n\\times m}, R \\in \\R ^{n\\times \\ell}\\\\\n",
    "\\gamma \\in \\R^{d \\times \\ell}, \\sigma \\in \\R^{d\\times m} \\\\\n",
    "f\\in C^2(\\R^d, E\\subset\\R^n)\\\\\n",
    "$$\n",
    "\n",
    "For the control problem with jumps, we would have the FBSDE of the form (assuming zero drift on $X$)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\sigma_t dW_t + \\gamma_t dN_t\\\\\n",
    "dY_t &= (\\dots)dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assume that $Y=f(X)$. Necessarily, by Ito's lemma (Oksendal, p.9, 1.2.8; for k-th column):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_t &= (D f(X_t))^T \\cdot \\sigma_t \\\\\n",
    "R^{(\\cdot k)}_t &= f\\left(X_t+\\gamma ^{(\\cdot k)}_t\\right) - f(X_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assuming that $D f(X) D f(X)^T$ is never singular and $f^{-1}\\in C^2(E, \\R^d)$ exists, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_t &= \\left(D f(X_t) D f(X_t) ^T \\right) ^{-1} D f(X_t) Z =: \\beta(X_t) Z_t\\\\\n",
    "\\gamma^{(\\cdot k)}_t &= f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for $\\beta(X) = \\left(D f(X) D f(X) ^T \\right) ^{-1} D f(X)$. Thus we obtain by Ito's lemma (note that $D^2 f$ is a 3d-tensor indexed by $i,j,k$ with $k\\in 1,\\dots, n$)\n",
    "\n",
    "$$\n",
    "(\\dots)dt = \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt\n",
    "$$\n",
    "\n",
    "Altogether, we can rewrite\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\beta(X_t) Z_t dW_t + \\left(f^{-1}\\left(f(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\sum_{i,j=1}^d (\\beta(X_t) Z_t(\\beta(X_t) Z_t)^T)^{(ij)} (D^2 f(X_t))^{(ij)} dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(X)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= \\delta_{ij} \\exp(X^{(i)}) =\\operatorname{diag}(\\exp(X))\\\\\n",
    "D^2 f(X)^{(ijk)} &= \\delta_{ijk} \\exp(X^{(i)})\\\\\n",
    "\\beta(X) &= \\delta_{ij}\\exp(-X^{(i)}) = \\operatorname{diag}(\\exp(-X))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\operatorname{diag}(\\exp(-X)) Z_t dW_t + \\left(\\log\\left(\\exp(X_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac 12 \\operatorname{diag} ( \\operatorname{diag}(\\exp(-X)) Z_t( \\operatorname{diag}(\\exp(-X)) Z_t)^T) \\exp(X_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(X_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1a\n",
    "\n",
    "Consider $d=\\ell=m=n$ and $f(X) = g(X) = \\exp(aX)$, elementwise. Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{-1}(X) &= \\frac 1a \\log(X)\\\\\n",
    "Df(X)^{(ij)} &= a\\delta_{ij} \\exp(aX^{(i)}) = a\\operatorname{diag}(\\exp(aX))\\\\\n",
    "D^2 f(X)^{(ijk)} &= a^2 \\delta_{ijk} \\exp(aX^{(i)})\\\\\n",
    "\\beta(X) &= \\frac 1a \\delta_{ij}\\exp(-aX^{(i)}) = \\frac 1a \\operatorname{diag}(\\exp(-aX))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the system can be rewritten (drift of dY is k-valued vector, $\\operatorname{diag}$ means extracting diagonal, or converting vector into diagonal matrix depending on the context):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t &= \\frac 1a \\operatorname{diag}(\\exp(-aX)) Z_t dW_t + \\left(\\frac 1a \\log\\left(\\exp(aX_t) + R^{(\\cdot k)}\\right) - X_t\\right) dN_t\\\\\n",
    "dY_t &= \\frac {1}{2} \\operatorname{diag} ( \\operatorname{diag}(\\exp(-aX)) Z_t( \\operatorname{diag}(\\exp(-aX)) Z_t)^T) \\exp(aX_t) dt + Z_t dW_t + R_t dN_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus the solution is given by $Y_t = \\exp(X_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer, BatchNormalization, Add\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.metrics import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gpu_utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(pick_gpu_lowest_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=11, linewidth=90, formatter=dict(float=lambda x: \"%7.5g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{timestamp}\"\n",
    "tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "output_dir = f\"_output/models/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_paths = 2 ** 12\n",
    "n_timesteps = 16\n",
    "time_horizon = 1.\n",
    "n_x_dimensions = 20\n",
    "n_y_dimensions = 20\n",
    "n_diffusion_factors = 20\n",
    "n_jump_factors = 20\n",
    "intensity = 1.\n",
    "alpha = 4.\n",
    "stddev = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = time_horizon / n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    return tf.zeros((n_x_dimensions,))\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z) / alpha\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    # floor the log argument\n",
    "    res = tf.maximum(tf.exp(alpha * x) + r, 1e-2)\n",
    "    return (tf.math.log(res) / alpha - x)\n",
    "\n",
    "def f(t, x, y, z, r):\n",
    "    res = tf.linalg.matmul(tf.linalg.diag(tf.exp(-alpha * x)), z)\n",
    "    return 0.5 * tf.einsum('ij,j', tf.linalg.matmul(res, res, transpose_b=True), tf.exp(alpha * x))\n",
    "\n",
    "def g(x):\n",
    "    return tf.exp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layers and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Y0Callback(Callback):\n",
    "    \n",
    "    def __init__(self, filepath=None):\n",
    "        super(Y0Callback, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.y0s = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y0 = self.model.get_layer('y_0').y0.numpy()\n",
    "        self.y0s += [y0[0]]\n",
    "        print(f\"{y0}\\n\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.filepath is not None:\n",
    "            pd.DataFrame(self.y0s).to_csv(self.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSave(Callback):\n",
    "    def __init__(self, directory):\n",
    "        self.batch = 1\n",
    "        self.epoch = 1\n",
    "        self.directory = directory\n",
    "        shutil.rmtree(directory, ignore_errors=True)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        self.epoch += 1\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        filename = os.path.join(self.directory, f\"weights_{self.epoch:03}_{self.batch:03}.h5\")\n",
    "        self.model.save_weights(filename)\n",
    "        self.batch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_x_dimensions, n_y_dimensions, n_diffusion_factors, n_jump_factors, n_timesteps, time_horizon):\n",
    "\n",
    "    dt = time_horizon / n_timesteps\n",
    "\n",
    "    def dX(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', s(t, x, y, z, r), dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', v(t, x, y, z, r), dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    def dY(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.einsum('ij,j', z, dW)\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.einsum('ij,j', r, dN)\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))        \n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    @tf.function\n",
    "    def hx(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return x + dX(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    @tf.function\n",
    "    def hy(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return y + dY(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    n_hidden_units = n_x_dimensions + n_diffusion_factors + n_jump_factors + 10\n",
    "\n",
    "    inputs_x0 = Input(shape=(n_x_dimensions))\n",
    "    inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "    inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "\n",
    "    # constant x0\n",
    "\n",
    "    x0 = tf.Variable([[1. for _ in range(n_x_dimensions)]], trainable=False)\n",
    "    y0 = tf.Variable([[0. for _ in range(n_y_dimensions)]], trainable=True)\n",
    "\n",
    "    x = InitialValue(x0, trainable=False, name='x_0')(inputs_dW)\n",
    "    y = InitialValue(y0, trainable=True, name='y_0')(inputs_dW)\n",
    "\n",
    "    # adjoints\n",
    "\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z1_0')(z)\n",
    "    z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='z2_0')(z)\n",
    "    z = BatchNormalization(name='zbn_0')(z)\n",
    "    z = Reshape((n_y_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r1_0')(r)\n",
    "    r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name='r2_0')(r)\n",
    "    r = BatchNormalization(name='rbn_0')(r)\n",
    "    r = Reshape((n_y_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "\n",
    "    # pre-compile lambda layers\n",
    "    \n",
    "    for i in range(n_timesteps):\n",
    "\n",
    "        step = InitialValue(tf.Variable(i, dtype=tf.float32, trainable=False))(inputs_dW)\n",
    "\n",
    "        dW = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dW, step])\n",
    "        dN = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dN, step])\n",
    "\n",
    "        x, y = (\n",
    "            Lambda(hx, name=f'x_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "            Lambda(hy, name=f'y_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "        )\n",
    "\n",
    "        # we don't train z for the last time step; keep for consistency\n",
    "        z = concatenate([x, y])\n",
    "        z = Dense(n_hidden_units, activation='relu', name=f'z1_{i+1}')(z)\n",
    "        z = Dense(n_y_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "        z = Reshape((n_y_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "        z = BatchNormalization(name=f'zbn_{i+1}')(z)\n",
    "\n",
    "        # we don't train r for the last time step; keep for consistency\n",
    "        r = concatenate([x, y])\n",
    "        r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r1_{i+1}')(r)\n",
    "        r = Dense(n_y_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=stddev), name=f'r2_{i+1}')(r)\n",
    "        r = Reshape((n_y_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "        r = BatchNormalization(name=f'rbn_{i+1}')(r)\n",
    "\n",
    "        paths += [[x, y, z, r]]\n",
    "\n",
    "    outputs_loss = Lambda(lambda r: r[1] - tf.vectorized_map(g, r[0]))([x, y])\n",
    "    \n",
    "    # remember that z and r are matrices\n",
    "    outputs_paths = tf.stack(\n",
    "        [tf.stack([p[0] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "        [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "    model_loss = Model([inputs_x0, inputs_dW, inputs_dN], outputs_loss)\n",
    "\n",
    "    # (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "    # skips the first time step\n",
    "    model_paths = Model([inputs_x0, inputs_dW, inputs_dN], outputs_paths)\n",
    "\n",
    "    return model_loss, model_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "dt = time_horizon / n_timesteps\n",
    "model_loss, model_paths = build_model(n_x_dimensions=n_x_dimensions,\n",
    "                                      n_y_dimensions=n_y_dimensions,\n",
    "                                      n_diffusion_factors=n_diffusion_factors,\n",
    "                                      n_jump_factors=n_jump_factors,\n",
    "                                      n_timesteps=n_timesteps,\n",
    "                                      time_horizon=time_horizon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant(np.full((n_paths, n_x_dimensions), 1.), dtype=tf.float32)\n",
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([intensity for _ in range(n_jump_factors)])))\n",
    "target = tf.zeros((n_paths, n_y_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "callbacks = []\n",
    "\n",
    "callbacks += [Y0Callback(os.path.join(output_dir, \"y0.csv\"))]\n",
    "# callbacks += [BatchSave(os.path.join(output_dir, \"weights\"))]\n",
    "callbacks += [ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)]\n",
    "callbacks += [tf.keras.callbacks.TerminateOnNaN()]\n",
    "callbacks += [tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=30)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # print(\"\\n\", y_pred.numpy().flatten(), \"\\r\")\n",
    "    return(tf.keras.metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "adam = Adam(learning_rate=1e-2) \n",
    "model_loss.compile(loss=mse_loss, optimizer=adam, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (x_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 20) dtype=float32, numpy=\n",
      "array([[      0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (y_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 20) dtype=float32, numpy=\n",
      "array([[      0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:5 out of the last 49367 calls to <function build_model.<locals>.hx at 0x7f77e82ad598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 49367 calls to <function build_model.<locals>.hy at 0x7f7728794d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 49368 calls to <function build_model.<locals>.hx at 0x7f77e82ad598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 49368 calls to <function build_model.<locals>.hy at 0x7f7728794d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "128/128 [==============================] - 102s 694ms/step - loss: 25.5878\n",
      "[[-0.038123 -0.07181 0.079712 -0.074439 -0.016349 -0.073602 0.042068 0.065877 0.016712\n",
      "  -0.084782 -0.061712 -0.067763 -0.040862 -0.041728 -0.075225 -0.077522 -0.042232\n",
      "  -0.072564 0.021196 -0.070141]]\n",
      "\n",
      "Epoch 2/1000\n",
      "128/128 [==============================] - 93s 732ms/step - loss: 2.8742\n",
      "[[-0.036087 -0.069752 0.081301 -0.063581 -0.015967 -0.073643 0.042892 0.076738 0.018603\n",
      "  -0.08298 -0.059069 -0.066374 -0.039749 -0.040636 -0.064479 -0.076959 -0.042124\n",
      "  -0.071222 0.022595 -0.06921]]\n",
      "\n",
      "Epoch 3/1000\n",
      " 77/128 [=================>............] - ETA: 35s - loss: 1.7915"
     ]
    }
   ],
   "source": [
    "history = model_loss.fit([x0, dW, dN], target, batch_size=32, initial_epoch=0, epochs=1000, callbacks=callbacks, shuffle=False)\n",
    "df_loss = pd.DataFrame(history.history['loss'])\n",
    "df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "* Alpha has to grow with number of dimensions to avoid explosion\n",
    "* Number of samples has to grow with number of dimensions to converge to Y_0, loss will be close to zero if the number of samples is too small. What is a good number of samples?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
